-
  how_many_teammates_are_in_your_group: 2
  name_1: Danica Bassman
  name_2: Isha Bajekal
  name_3: 
  name_4: 
  pennkey_username_1: danicab
  pennkey_username_2: ibajekal
  pennkey_username_3: 
  pennkey_username_4: 
  name_of_your_project: Influence
  give_a_one_sentence_description_of_your_project: Influence creates HITs to test whether priming could influence crowd workers' reservation prices, and if so which method would be most effective; given our results, we aimed to extrapolate implications for creating future HITs and influencing workers
  url_to_the_logo_for_your_project: https://github.com/danicabassman/nets213_project/blob/master/final_stuff/logo.png
  what_similar_projects_exist: There have been many experiments before showing the existence and  effectiveness of priming. The first to discover priming were Meyer and Schvaneveldt in the 1970s. Since then, many others have performed similar experiments in psychology and consumer behavior. To our knowledge, we are the first to test priming using crowdsourcing and not in a formal experiment setting. 
  what_type_of_project_is_it: Social science experiment with the crowd
  how_do_you_incentivize_the_crowd_to_participate: Our main method of incentivizing members of our crowd was to pay them. The jobs consisted of survey&#45;like questions with two parts. We paid workers 10 cents for completing the job. We decided this was fair because compared to other jobs on crowdflower, our job was relatively easy (the first 10 questions that were used to prime the workers were extremely easy). In order to be more cost effective, we tried to make the jobs easy to complete with clear instructions and as enjoyable as possible. The priming questions (determining if there was a dog present in the picture or selecting a word from a group and counting its number of syllables) had clear correct answers, so the survey was more like a game. Since answering questions like these is hopefully somewhat enjoyable, we thought this could be a partial incentive to participate as well. We don't think we could have incentivized users to participate solely on this enjoyment, but the fact that it was enjoyable, particularly in relation to other crowdflower jobs, meant that we could pay workers less per job since money was not their sole incentive. 
  what_does_the_crowd_provide_for_you: The crowd, after being primed (or not, in the control group), provided their reservation prices for 5 different products. 
  how_do_you_ensure_the_quality_of_the_crowd_provides__: For our experiments to show relevant results, we need to ensure that workers were actually primed. For this, we need to know that they actually took the priming questions seriously. One of the best ways to test this is to check that they get all the answers correct for the priming questions since these questions are supposed to be pretty easy and have clearly correct answers. There were two issues with this however. One was that crowdflower's interface would not let us set only the 10 priming questions as test questions. <p>The second is more subtle. It doesn't actually matter if the workers got the test questions correct&#45;&#45;to be primed, it is only important that they read and take in the data presented in the test questions. For instance in the image priming, whether a worker correctly identifies if there is a dog in the picture has no effect on if they are primed; rather, we just need them to seriously consider the picture and look at it for several seconds to be properly influenced by it. Similarly, for the word priming, it doesn't matter if they properly count the syllables in the word that it not a color, only that they attempt to and actually take the time to process and think about the priming word. <p>A good way to test whether this has happened is if they get these test questions correct, since that usually means that they would have considered the data enough to primed by it. However, they can still have been properly primed and get the question wrong&#45;&#45;what if they stare at a picture for 15 seconds, but still do not see a dog that is there, so they answer incorrectly? There are also cases where they may be answering randomly or can spot a dog by looking at the picture for 1 second, which may not be enough time to prime them. <p>Another test, in this case, could be to measure the amount of time workers spent per question. We did this my imposing a minimum time to take on the survey, but there are issues here as well. What if workers still answer the question after looking at the picture for 1 second, and then just like the survey sit without paying attention to it until enough time has passed for them to continue?<p>Still, it is often the case that answering the questions correctly indicates that enough time and effort were spent considering the problem for the worker to be primed. Ideally, we would have workers answer the priming questions first and if they did not get them all correct or answered the survey too quickly, then they would not be asked for their reservation prices for the products in part 2. While this may weed out some workers who were actually primed, the odds of workers who were not primed being able to provide reservation prices is significantly lower. 
  how_do_you_aggregate_the_results_from_the_crowd: We used the data from crowdflower to aggregate results from the crowd. We had four different groups, Image &#45; control, Image &#45; primed, Word &#45; control, Word &#45; primed and four different sets of data. In each set of data, there were five products for which each worker in that group listed an amount they were willing to pay to acquire that product. For each product in each group, we calculated the mean and standard deviation of amounts workers were willing to pay. 
  how_does_your_project_work: First, workers answer part 1 of the survey. They are primed using either images or words. For images and words, workers are split into experimental and control groups. For images the experimental groups see more luxurious, expensive, and extravagant pictures while the control groups see more normal couterparts. For the words experimental group, the words they must count the syllables of describe more luxurious, expensive, and extravagant things, while the control group have more normal words. <p>After workers have been primed (or not, if they are in the control group), we asked them to tell us how much they were willing to pay for 5 different products. Some were luxury products (e.g. high heeled shoes) while others were more household items (e.g. a lamp). <p>Finally, we measured workers average reservation prices for given products and analyzed how responses varied depending on what groups they were in (experimental vs. control, image vs. word). 
  would_your_project_benefit_if_you_could_get_contributions_from_thousands_of_people: No
  vimeo_link: http://vimeo.com/114459012
  pennkey_username_1: 
  may_we_have_your_permission_to_feature_your_project_on_the_class_website: Yes
  team_member_1__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_2__can_we_list_your_name_listed_alongside_your_project: No
  team_member_3__can_we_list_your_name_listed_alongside_your_project: 
  team_member_4__can_we_list_your_name_listed_alongside_your_project: 
  who_are_the_members_of_your_crowd: The workers on crowdflower
  how_many_unique_participants_did_you_have: 240
  for_your_final_project_did_you_simulate_the_crowd_or_run_a_real_experiment: Real crowd
  if_the_crowd_was_simulated_how_did_you_collect_this_set_of_data: 
  if_the_crowd_was_simulated_how_would_you_change_things_to_use_a_real_crowd: 
  how_do_you_ensure_the_quality_of_the_crowd_provides: 
  what_motivation_does_the_crowd_have_for_participating_in_your_project: Pay, Enjoyment
  did_you_perform_any_analysis_comparing_different_incentives_: No
  if_you_compared_different_incentives_what_analysis_did_you_perform_: 
  if_you_have_a_graph_analyzing_incentives_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_incentives_: 
  do_your_crowd_workers_need_specialized_skills: No
  what_sort_of_skills_do_they_need: Workers did not need any specific skills (other than being able to read english). We controlled for only skilled workers on crowdflower to provide consistency. This was so the only variable factor was whether workers were primed or not, and not that by chance more skilled workers, who by correlation may also be more affluent, for instance. This way, we can claim that the differences between reservation prices is solely influenced by priming. 
  if_skills_vary_widely_what_factors_cause_one_person_to_be_better_than_another_: 
  did_you_analyze_the_skills_of_the_crowd: No
  if_you_analyzed_skills_what_analysis_did_you_perform: 
  do_you_have_a_google_graph_analyzing_skills_: No
  if_you_have_a_graph_analyzing_skills_include_the_html_for_a_google_graph_here: 
  is_the_quality_of_what_the_crowd_gives_you_a_concern: Yes
  did_you_create_a_user_interface_for_the_crowd_workers: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_crowdfacing_user_interface: https://github.com/danicabassman/nets213_project/blob/master/final_stuff/image_experimental.pdf<p>https://github.com/danicabassman/nets213_project/blob/master/final_stuff/image_control.pdf<p>https://github.com/danicabassman/nets213_project/blob/master/final_stuff/word_control.pdf<p>https://github.com/danicabassman/nets213_project/blob/master/final_stuff/word_experimental.pdf
  describe_your_crowdfacing_user_interface: We had to balance the limitations of crowdflower's template for questions with our desire to make the questionnaires as easy to complete as possible. We tried a few variations of the presentation of data relevant to the questions, but ended up settling for the ones in the screenshots. We were careful to make the instructions, etc., constant between the control and experimental questionnaires so that the only possible source of difference between the two resulting data sets would be the priming (not that one instruction set was easier to follow, could be answered more quickly, etc.). 
  did_you_analyze_the_aggregated_results_: Yes
  what_analysis_did_you_perform_on_the_aggregated_results: 
  did_you_analyze_the_quality_of_what_you_got_back: Yes
  what_analysis_did_you_perform_on_the_aggregated_results: We performed statistical T&#45;tests on our data, in order to see if there was a statistically significant difference between the average amount people are willing to spend on products after being exposed to certain stimuli.<p>For each product (shoes, watch, lamp, mug, speakers) we compared the mean values that people who were in the experimental image group (the group exposed to images that included luxury items) were willing to spend as compared to the mean values that people who were in the control image group (the group exposed to images that did not include luxury items). The t&#45;test was a good choice because we had a reasonably normal distribution for each of these products in each group. For the shoes and the speakers, we had an outlier who was will in to spend over $500,000 for each of these two items. We conducted the t&#45;test both with and without these values. We will include a link with a table of the t and corresponding p values for each of these 5 tests. <p>We then performed the t&#45;test, except this time we compared the mean values that people were willing to spend in the experimental word analysis group versus the control word analysis group. This time, there were no outliers in the data.<p>As a total, we analyzed two sets of data, and for each set, we conducted 5 t&#45;tests which utilized the mean and standard deviation values for the data. 
  what_analysis_did_you_perform_on_quality: We noticed that in our data sets for the experimental groups, there were significant outliers. While one possible explanation for this is that these particular workers had extremely high, outlying reservation prices for these particular products, but another possibility is that the workers were entering numbers randomly, not looking at the right product, or did not understand the question. We analyzed the data with and without these outliers since it was unclear if it was the result of a true outlier in the crowd for the particular product or if it was because of bad quality workers. 
  do_you_have_a_google_graph_analyzing_quality_: No
  if_you_have_a_graph_analyzing_quality_include_the_html_for_a_google_graph_here: 
  is_this_something_that_could_be_automated: No
  if_it_could_be_automated_say_how__if_it_is_difficult_or_impossible_to_automate_say_why: This could not be automated because the experiment was specifically to see the effect priming had on people. This effect would not exist on an automated machine trying to determine prices for specific products. We used the crowd in order to get a random sampling and show that this cognitive effect exists in any crowd, and therefore should be considered when using a crowd for data instead of some form of automation. 
  do_the_skills_of_individual_workers_vary_widely: No
  do_you_have_a_google_graph_analyzing_the_aggregated_results_: Yes
  if_you_have_a_graph_analyzing_the_aggregated_results_include_the_html_for_a_google_graph_here: https://github.com/danicabassman/nets213_project/blob/master/image_experiments.html<p>https://github.com/danicabassman/nets213_project/blob/master/word_experiments.html<p>https://github.com/danicabassman/nets213_project/blob/master/image_and_experimental.html<p>https://github.com/danicabassman/nets213_project/blob/master/analysis_nets213.pptx
  did_you_create_a_user_interface_for_the_end_users_to_see_the_aggregated_results: No
  if_yes_please_give_the_url_to_a_screenshot_of_the_user_interface_for_the_end_user: 
  describe_what_your_end_user_sees_in_this_interface: 
  what_is_the_scale_of_the_problem_that_you_are_trying_to_solve_: The experiments solve the problem of whether or not priming is a factor for crowds (of any size). While we only need a small sample size (ours was 120 per method of priming, so 240 total), this implications of the results are applicable across any crowd. 
  if_it_would_benefit_from_a_huge_crowd_how_would_it_benefit_: 
  what_challenges_would_scaling_to_a_large_crowd_introduce: While scaling to a large crowd would strengthen the assertion that priming exists across all crowds, it also adds the risk of introducing more confounding variables. Still, as we scale up, we should reduce the noise in the data. Since our data was not that noisy, however, there would be diminishing returns to scaling up, and it would be more cost for no added gains. 
  did_you_perform_an_analysis_about_how_to_scale_up_your_project: No
  what_analysis_did_you_perform_on_the_scaling_up: 
  do_you_have_a_google_graph_analyzing_scaling_: No
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  url_to_the_flow_diagram_for_your_project: N/A
  if_the_crowd_was_real_how_did_you_recruit_participants: We used crowdflower to recruit our participants. We incentivized them to participate with money and by making the job easy/enjoyable. 
  did_your_project_work: Our project showed general trends that we expected. Unfortunately, our results were not statistically significant, but we did see higher average reservation prices for workers that were primed. This may have been a result of either too small (only 60 participants) or that we did not use enough priming data (workers were only primed on 10 questions, not all of which were priming words or images). It also may had had something to do with the products we asked about, since priming for luxury appears to have been less effective on non&#45;luxury items when workers were primed with images. 
  do_you_have_a_google_graph_analyzing_your_project_: No
  if_you_have_a_graph_analyzing_your_project_include_the_html_for_a_google_graph_here: 
  caption_for_your_google_graph_(project_analysis): 
  what_were_the_biggest_challenges_that_you_had_to_deal_with: Our biggest issue was related to quality control. If each worker answered the questions perfectly, we could easily have deduced whether priming was effective on this crowd and extrapolated these results to all crowds. However, there is no way to tell if they were answered perfectly, because perfectly does not imply each answer was correct, but that the method used to answer the questions was in a certain specific way. As discussed in quality control, this is very hard to control for, and it became more difficult given the constraints of implementation on crowdflower.<p>While there were certain issues in the workers' processes for answering questions we could measure, like time taken to answer a question and were the priming questions answered correctly, there are other factors we cannot test, for instance, how much effort or attention is the worker giving to the priming question. Even attempting to control for this would have negative consequences since priming does not work if the workers know they are being primed. While we tried to make the instructions as clear as possible, it is still possible that some workers were, for example, entering a reservation price for the wrong product, or were entering how much they thought the product cost rather than how much they would be willing to pay for it. These are also difficult to control for. <p>Finally, as with any crowd, there could be many confounding variables providing alternate explanations for our results not related to priming. While we did our best to eliminate this risk by having large crowds, we were also limited by cost. 
  where_there_major_changes_between_what_you_originally_proposed_and_your_final_product: No
  if_so_what_changed_between_your_original_plan_and_your_final_product: 
  what_are_some_limitations_of_your_product: The sources of error were discussed in quality control and the challenges we dealt with. 
  did_your_results_deviate_from_what_you_would_expect_from_previous_work_or_what_you_learned_in_the_class: No
  if_your_results_deviated_why_might_that_be: Our results followed the general trend we expected, but the trend was not as strong as we expected it to (because it was not statistically significant). 
  caption_for_your_graph_(scaling_up): 
  caption_for_your_graph_(aggregation): Average total amount workers were willing to spend
  caption_for_your_graph_(quality): 
  caption_for_your_graph_(skills): 
  caption_for_your_graph_(incentives): 
  is_there_anything_else_youd_like_to_say_about_your_project: The general trends of our results suggests that priming workers beforehand can influence consumers' reservation prices. Although our results were not statistically significant, there are many other experiments that prove that this is an existing phenomenon. We decided to test this on crowd workers to show that it was an existing factor and should be considered when you want to create an unbiased HIT on crowdflower. While we assume that the reason our data did not show a statistically significant result had to do with sample size and other issues specific to our experiments, it could also be the case that for some reason, workers on crowd flower are just not as susceptible to priming as the average person. This means that perhaps we don't need to consider priming when working to create unbiased HITs. Still, the trends of our data suggest that on average workers will have higher reservation prices and this is supported by outside experiments. Our other goal of the project was to consider how this could be used in marketing, advertising, etc., to influence people's decisions. As a result of our data, we think online retailers such as amazon could try recommending luxury and more expensive products to their customers in order to prime them and make them more willing to spend more money or purchase more expensive products. Based on our data, we believe this is likely to be at least somewhat effective. 
  did_you_train_a_machine_learning_component_from_what_the_crowd_gave_you: No
  if_you_trained_a_machine_learning_component_describe_what_you_did: 
  did_you_analyze_the_quality_of_the_machine_learning_component: No
  if_you_have_a_graph_analyzing_a_machine_learning_component_include_the_html_for_a_google_graph_here: 
  caption_for_your_graph_(incentives): 
  caption_for_your_graph_(machine_learning_component): 
  what_was_the_main_focus_of_your_teams_effort: Conducting an in depth analysis of data
  provide_a_link_to_your_final_presentation_video: 
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_skills_: 
  what_incentivi: 
-
  how_many_teammates_are_in_your_group: 2
  name_1: Richard Kitain
  name_2: Emmanuel Genene
  name_3: 
  name_4: 
  pennkey_username_1: rkitain
  pennkey_username_2: egenene
  pennkey_username_3: 
  pennkey_username_4: 
  name_of_your_project: Crow de Mail
  give_a_one_sentence_description_of_your_project: Crow de Mail helps you write emails in any context.
  url_to_the_logo_for_your_project: https://github.com/egenene/NETS213Project/blob/cee44fc18ebcc7e476b89d9ca2c6c4f68e34e3d7/docs/mockups/Crow_de_Mail.png
  what_similar_projects_exist: The only somewhat similar project that exists is EmailValet. The main difference between the two is that EmailValet allows the crowd to read emails, while Crow de Mail allows the crowd write the emails for the user.
  what_type_of_project_is_it: Human computation algorithm
  how_do_you_incentivize_the_crowd_to_participate: The crowd is incentivized with pay. We pay each worker from the first crowd five cents per email that they write. We also pay each worker from the second crowd five cents per vote. To get faster and possibly more reliable results, we could pay the crowd more to further incentivize them to do a good job.
  what_does_the_crowd_provide_for_you: The crowd provides their ability to write basic emails, which vary from a few sentences to a few paragraphs.
  how_do_you_ensure_the_quality_of_the_crowd_provides__: Quality is one of the biggest concerns when it comes to Crow de Mail. Workers on Crowdflower get paid very little and often look for the fastest way they can make money. This often leads to low quality results, which is especially bad because emails need to be exquisite most of the time as they could be highly important.<p>The main method used to ensure the quality of the crowd is the round of voting that occurs. Once multiple emails are written by the first crowd, a second crowd examines the emails and votes on which they believe is the best. This filters out most, if not all, of the low quality results and allows the best email to be returned to the user.<p>Another technique to ensure quality would be to utilize Crowdflower’s built in qualifications system. Workers are each given a level from one to three, with three being the best rated workers. It would be very easy to change the HITs so that only level three workers are able to complete the tasks. Crow de Mail does not currently do this because the results have been fine without this and it would cost extra, but if the results started to dip in quality, this feature would be utilized.
  how_do_you_aggregate_the_results_from_the_crowd: The results from the crowd were aggregated on Crowdflower. After the first HIT is finished, Crowdflower generates a csv file with the results. The same occurs after the second HIT is finished. We then apply a majority vote algorithm and return the result to the user.
  how_does_your_project_work: First, a user requests an email to be written and provides a context and instructions. Next, the request is converted into a csv file and uploaded as a HIT to Crowdflower. The crowd creates five emails that are returned, parsed, and re&#45;uploaded as a new HIT to Crowdflower. A second crowd votes on which of these five emails is the best. The results are returned as a csv and a majority vote algorithm returns the email that was voted as best to the user.
  would_your_project_benefit_if_you_could_get_contributions_from_thousands_of_people: No
  vimeo_link: http://vimeo.com/114542957
  pennkey_username_1: 
  may_we_have_your_permission_to_feature_your_project_on_the_class_website: Yes
  team_member_1__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_2__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_3__can_we_list_your_name_listed_alongside_your_project: 
  team_member_4__can_we_list_your_name_listed_alongside_your_project: 
  who_are_the_members_of_your_crowd: Workers on Crowdflower
  how_many_unique_participants_did_you_have: 22
  for_your_final_project_did_you_simulate_the_crowd_or_run_a_real_experiment: Real crowd
  if_the_crowd_was_simulated_how_did_you_collect_this_set_of_data: 
  if_the_crowd_was_simulated_how_would_you_change_things_to_use_a_real_crowd: 
  how_do_you_ensure_the_quality_of_the_crowd_provides: 
  what_motivation_does_the_crowd_have_for_participating_in_your_project: Pay
  did_you_perform_any_analysis_comparing_different_incentives_: No
  if_you_compared_different_incentives_what_analysis_did_you_perform_: 
  if_you_have_a_graph_analyzing_incentives_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_incentives_: 
  do_your_crowd_workers_need_specialized_skills: No
  what_sort_of_skills_do_they_need: The crowd workers need to be able to write basic sentences in the language that the request came in. Basic grammar skills and spelling are also required. 
  if_skills_vary_widely_what_factors_cause_one_person_to_be_better_than_another_: The main factor that causes one person to be better than another is most likely the degree of education they received. A person with a college education will most likely write an email that is just as good or better than an email written by a person with only a high school degree.
  did_you_analyze_the_skills_of_the_crowd: Yes
  if_you_analyzed_skills_what_analysis_did_you_perform: We analyzed their skills by using the data that Crowdflower provided for us after they completed the HITs. The main subset of their skill that we analyzed was the total time it took for them to complete the task. We figure that the more time spent on writing the email, the better the email will turn out. Specifically, we compared the average time workers from the US spent writing the email versus the average time workers from the UK spent writing the email. We reached the conclusion that since workers from the US spent more time writing the email, workers from the US worked harder and did a better job than those in the UK. However, the sample size we tested with was very small, so to confirm this hypothesis we would need to test with many more people.
  do_you_have_a_google_graph_analyzing_skills_: Yes
  if_you_have_a_graph_analyzing_skills_include_the_html_for_a_google_graph_here: <html><p>  <head><p>    <script type=text/javascript src=https://www.google.com/jsapi></script><p>    <script type=text/javascript><p>      google.load(visualization, 1, {packages&#58;[corechart]});<p>google.setOnLoadCallback(drawChart);<p>function drawChart() {<p>  var data = google.visualization.arrayToDataTable([<p>    ['Attempt', 'US', 'GB'],<p>    ['Attempt 1',  2.42,      1.61],<p>    ['Attempt 2',  1.67,      1.13]<p>  ]);<p>  var options = {<p>    title&#58; 'Time Taken vs Country',<p>    hAxis&#58; {title&#58; 'Attempt#', titleTextStyle&#58; {color&#58; 'red'}}<p>  };<p>  var chart = new google.visualization.ColumnChart(document.getElementById('chart_div'));<p>  chart.draw(data, options);<p>}<p>    </script><p>  </head><p>  <body><p>    <div id=chart_div style=width&#58; 900px; height&#58; 500px;></div><p>  </body><p></html>
  is_the_quality_of_what_the_crowd_gives_you_a_concern: Yes
  did_you_create_a_user_interface_for_the_crowd_workers: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_crowdfacing_user_interface: https://github.com/egenene/NETS213Project/blob/master/docs/mockups/qc_screenshot.png<p>https://github.com/egenene/NETS213Project/blob/master/docs/mockups/agg_screenshot.png
  describe_your_crowdfacing_user_interface: Worker picks best email that matches instruction.<p>Worker writes email following the instructions.
  did_you_analyze_the_aggregated_results_: No
  what_analysis_did_you_perform_on_the_aggregated_results: 
  did_you_analyze_the_quality_of_what_you_got_back: No
  what_analysis_did_you_perform_on_the_aggregated_results: None
  what_analysis_did_you_perform_on_quality: 
  do_you_have_a_google_graph_analyzing_quality_: No
  if_you_have_a_graph_analyzing_quality_include_the_html_for_a_google_graph_here: 
  is_this_something_that_could_be_automated: No
  if_it_could_be_automated_say_how__if_it_is_difficult_or_impossible_to_automate_say_why: It is difficult or near impossible to automate what the crowd provides because machines are not able to think. To write emails that pertain to the context provided by the user, a machine would have to have seen an almost exact same request from a different user. A proper training set for a machine learning component would also be impossible to make because the user could request a topic that was never seen in the training set and the machine would not know what to do.
  do_the_skills_of_individual_workers_vary_widely: Yes
  do_you_have_a_google_graph_analyzing_the_aggregated_results_: No
  if_you_have_a_graph_analyzing_the_aggregated_results_include_the_html_for_a_google_graph_here: 
  did_you_create_a_user_interface_for_the_end_users_to_see_the_aggregated_results: No
  if_yes_please_give_the_url_to_a_screenshot_of_the_user_interface_for_the_end_user: 
  describe_what_your_end_user_sees_in_this_interface: 
  what_is_the_scale_of_the_problem_that_you_are_trying_to_solve_: The scale of the problem that Crow de Mail solves is very large. Almost everybody uses email to communicate these days and it is important that you write good emails.
  if_it_would_benefit_from_a_huge_crowd_how_would_it_benefit_: Although it would benefit, the benefit would be very small. This is because the increase in quality of the emails that are written decreases as the number of emails written increases.
  what_challenges_would_scaling_to_a_large_crowd_introduce: The main challenge that scaling to a large crowd would introduce is that it would be almost impossible for the second crowd to do their job. Picking a single email out of a thousand emails that were written for a single request would take too long and workers would end up picking randomly.
  did_you_perform_an_analysis_about_how_to_scale_up_your_project: No
  what_analysis_did_you_perform_on_the_scaling_up: 
  do_you_have_a_google_graph_analyzing_scaling_: No
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  url_to_the_flow_diagram_for_your_project: https://github.com/egenene/NETS213Project/blob/master/docs/flow_chart.png
  if_the_crowd_was_real_how_did_you_recruit_participants: Participants were recruited through Crowdflower. On Crowdflower, workers are paid based on the tasks they perform. We paid participants from both crowds five cents for the work they put in.
  did_your_project_work: Our project did work. Before we began the project, we decided that if Crow de Mail was able to properly produce an email from a student to a teacher regarding meeting for bad grades, we would label this project a success. After we completed the project, we posted the task ourselves and got back a high quality email that did exactly what it needed to. Now, people have to worry less about writing emails and can spend their time working on more important things.
  do_you_have_a_google_graph_analyzing_your_project_: No
  if_you_have_a_graph_analyzing_your_project_include_the_html_for_a_google_graph_here: 
  caption_for_your_google_graph_(project_analysis): 
  what_were_the_biggest_challenges_that_you_had_to_deal_with: The biggest challenge we came across was that the Crowdflower documentation did not want to cooperate. We found the documentation about a week before we needed to utilize it, but when it came time to write the code, the page that listed the documentation no longer existed. We then had to spend extra time looking for a third&#45;party documentation, which thankfully worked.
  where_there_major_changes_between_what_you_originally_proposed_and_your_final_product: No
  if_so_what_changed_between_your_original_plan_and_your_final_product: 
  what_are_some_limitations_of_your_product: The main limitations of Crow de Mail are that the cost will become quite high to pay Crowdflower workers because we use our own money. We could change the process to allow the users to pay for the jobs themselves. This would also help with quality control because if users provided us with more money, we would be able to utilize Crowdflower’s level three workers to get the best possible results.
  did_your_results_deviate_from_what_you_would_expect_from_previous_work_or_what_you_learned_in_the_class: Yes
  if_your_results_deviated_why_might_that_be: Our results deviated from what we expected because we received much better results. We assumed that most of the workers would take advantage of the Crowdflower system and write random strings of characters instead of actually writing the email. When we received an email that was well written and corresponded to the topic, we were quite surprised.
  caption_for_your_graph_(scaling_up): 
  caption_for_your_graph_(aggregation): 
  caption_for_your_graph_(quality): 
  caption_for_your_graph_(skills): Average time to finish HIT (US vs. UK)
  caption_for_your_graph_(incentives): 
  is_there_anything_else_youd_like_to_say_about_your_project: 
  did_you_train_a_machine_learning_component_from_what_the_crowd_gave_you: No
  if_you_trained_a_machine_learning_component_describe_what_you_did: 
  did_you_analyze_the_quality_of_the_machine_learning_component: No
  if_you_have_a_graph_analyzing_a_machine_learning_component_include_the_html_for_a_google_graph_here: 
  caption_for_your_graph_(incentives): 
  caption_for_your_graph_(machine_learning_component): 
  what_was_the_main_focus_of_your_teams_effort: Something in between
  provide_a_link_to_your_final_presentation_video: 
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_skills_: 
  what_incentivi: 
-
  how_many_teammates_are_in_your_group: 4
  name_1: Albert Shu
  name_2: Indu Subbaraj
  name_3: Paarth Taneja
  name_4: Caroline White
  pennkey_username_1: salbert
  pennkey_username_2: subbindu
  pennkey_username_3: paarth
  pennkey_username_4: whitecar
  name_of_your_project: Note My Time
  give_a_one_sentence_description_of_your_project: Note My Time is an application that allows students to split up the note&#45;taking process and share lecture notes.
  url_to_the_logo_for_your_project: https://github.com/indu&#45;subb/Nets213/blob/master/docs/Logo.png
  what_similar_projects_exist: Course Hero allows users to buy study aid materials or hire tutors for classes. StudyBlue allows users to share notes and flashcards with other users. Koofers provides access to notes and old exams. GradeGuru provides a study platform where students can share and find class&#45;specific study notes. The main difference between these companies and ours is that ours crowd sources the work in a unique manner, increasing the quality of the notes and guaranteeing the existence of notes for a given class. It is also free for anyone who contributes to the note taking process.
  what_type_of_project_is_it: A tool for crowdsourcing
  how_do_you_incentivize_the_crowd_to_participate: Users have the incentive to participate because they can get access to all the lecture notes in a class and only have to take notes for a few time slots. Instead of taking notes every class for the entire 80 minutes, users will take notes for 30 minutes. Participants directly benefit from the work they are doing and the work of other classmates. We will emphasize that our quality control measures will ensure that the notes generated are good quality.<p><p>In order to incentivize a real crowd, we would emphasize the aspect of being able to get the most out of class without having to worry about taking notes the entire time. Also, given our analysis (discussed below), in which respondents indicated that they are most motivated to share notes with friends, we could also encourage people to sign up in groups with their friends and other people that they know within their class.<p>
  what_does_the_crowd_provide_for_you: The crowd provides the notes for each class (provided in portions of each class period).<p>
  how_do_you_ensure_the_quality_of_the_crowd_provides__: Participants note&#45;taking ability will be rated by their peers. Every user is rated on a scale of 1 &#45; 5 by their peers. If someone is taking clearly subpar notes, his peers will give him a low rating. After a certain volume of low ratings, the participant will be kicked out of the group. <p>An internal counter is kept on the total number of ratings given a set of notes and the total number of ratings given to a user so that the average can be updated for every new rating. Once the average rating of the user has been updated, it is checked against a benchmark (2.5) and if below (and at least 4 ratings have been given to the user), the user’s homepage displays a warning message &#45; “Your peers have rated your previous notes as substandard. Raise your standards immediately or you will be kicked out of the class. For examples of good notes, please check out the note&#45;taking guideline page”. If the rating drops beneath a 2, the user is removed permanently. <p>When a user initially joins his/her rating will start at 3. If the user does not upload notes when it is his/her turn to do so, we will rely on the crowd (the other users) to give the user poor ratings (1’s) in order to reflect his/her poor performance. In addition, quality is improved by having two users take notes for each time slot.<p>
  how_do_you_aggregate_the_results_from_the_crowd: An 80 minute class will have three time slots&#58; 0 to 30, 25 to 55, 50 to 80 (the five minute overlap helps avoid the loss of data). Each slot will have 2 people taking notes (both notes will be given). <p>
  how_does_your_project_work: First, participants will enroll on our website. Students in the class will be randomly assigned to 30 minute time slots for which they will be responsible for taking notes (e.g. 11&#58;00 &#45; 11&#58;30). These time slots will overlap by 5 minutes to ensure no information is lost (start of class to 30 minutes, 25 to 55, 50 to 80). There will be two students taking notes in each time slot. During the class, each participant is free to listen, note&#45;free and care&#45;free, until it is his/her time to take notes. At the end of the class, each participant will upload his/her notes to the website. Notes will be uploaded in a specific format (Lecture#_Part#_username) so that other users can easily identify the notes. Users can rate another user based upon his/her notes. If a user rates a person, then that person’s rating average is updated and a check is done automatically to see if the user should be given a warning or removed (see description of quality control algorithm for more details). The users see their own ratings when they log in. <p>
  would_your_project_benefit_if_you_could_get_contributions_from_thousands_of_people: No
  vimeo_link: https://vimeo.com/114549212
  pennkey_username_1: 
  may_we_have_your_permission_to_feature_your_project_on_the_class_website: Yes
  team_member_1__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_2__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_3__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_4__can_we_list_your_name_listed_alongside_your_project: Yes
  who_are_the_members_of_your_crowd: Students taking classes at Penn
  how_many_unique_participants_did_you_have: 4
  for_your_final_project_did_you_simulate_the_crowd_or_run_a_real_experiment: Simulated crowd
  if_the_crowd_was_simulated_how_did_you_collect_this_set_of_data: We took notes ourselves during class, putting two people on each half&#45;hour shift and then alternating shifts in order to simulate the conditions of the actual note&#45;taking environment. We then used these notes in conjunction with made&#45;up usernames to implement our website.
  if_the_crowd_was_simulated_how_would_you_change_things_to_use_a_real_crowd: If the crowd was real we would have to make sure enough users sign up so that all the slots in a class are taken. Also, right now our website only allows for a single class. For a real crowd, we would also include folders for multiple classes so that the students could upload their notes to their respective classes. In addition, if we wanted to have a real crowd we would have to set up a server for our website instead of just providing the users all of the code and asking them to run the program locally. Currently, our website has a accessible database server (dynamodb on amazon), however the website itself is only hosted locally.
  how_do_you_ensure_the_quality_of_the_crowd_provides: 
  what_motivation_does_the_crowd_have_for_participating_in_your_project: Pay, Altruism, Implicit work, Reputation
  did_you_perform_any_analysis_comparing_different_incentives_: Yes
  if_you_compared_different_incentives_what_analysis_did_you_perform_: We issued a survey asking people what their incentives for note&#45;sharing would be, with options for sharing only with friends, sharing in return for some sort of compensation, and sharing with no strings attached (don’t mind sharing their notes). We found that large majority of students are only comfortable with sharing notes with their friends, with the second most popular opinion being having no problem with sharing notes. Not many respondents indicated that the presence of compensation would be a primary motivator in sharing notes. Given these results, we concluded that one of the best ways to incentivize a real crowd is to emphasize the bonds that classmates have with each other and to encourage people to sign up for the site in groups with their friends that they share classes with in order to better utilize this tendency to share notes with friends.<p>
  if_you_have_a_graph_analyzing_incentives_include_the_html_for_a_google_graph_here: <p><script type=text/javascript src=https://www.google.com/jsapi></script><p><script type=text/javascript><p>  google.load(visualization, '1.1', {packages&#58;['corechart']});<p>  google.setOnLoadCallback(drawChart);<p>  function drawChart() {<p>    var oldData = google.visualization.arrayToDataTable([<p>      ['Incentives for note&#45;sharing', 'Number of Students'],<p>      ['Friends', 20], ['Compensation', 2],<p>      ['No strings attached', 13]]);<p>    var options = { pieSliceText&#58; 'none' };<p>    var chartBefore = new google.visualization.PieChart(document.getElementById('piechart_before'));<p>    chartBefore.draw(oldData, options);<p>  }<p></script><p><span id='piechart_before' style='width&#58; 450px; display&#58; inline&#45;block'></span><p><span id='piechart_after' style='width&#58; 450px; display&#58; inline&#45;block'></span><p><br><p><span id='piechart_diff' style='width&#58; 450px; position&#58; absolute; left&#58; 250px'></span><p>
  do_you_have_a_google_graph_analyzing_incentives_: 
  do_your_crowd_workers_need_specialized_skills: Yes
  what_sort_of_skills_do_they_need: The workers need to be able to take comprehensive yet succinct notes that capture the main points of the lecture as well as any relevant details.
  if_skills_vary_widely_what_factors_cause_one_person_to_be_better_than_another_: People who are attentive listeners and relatively quick typers, and who are able to summarize points in an easy&#45;to&#45;read format, make the best note&#45;takers. We are hoping, however, that breaking up the lecture periods into multiple time&#45;slots will increase the attentiveness, and therefore skill level, of each participant since they are required to pay attention for a shorter timespan.<p>
  did_you_analyze_the_skills_of_the_crowd: Yes
  if_you_analyzed_skills_what_analysis_did_you_perform: We asked students over what period of time they are able to take notes diligently before they start missing information. We then took the most commonly responded time period as the amount of time each Note My Timer would have to take notes during his/her shift. From the results of our analysis, we determined that 30 minute periods would be most effective period of time over which students can take notes without missing information.<p>
  do_you_have_a_google_graph_analyzing_skills_: Yes
  if_you_have_a_graph_analyzing_skills_include_the_html_for_a_google_graph_here: <html><p>  <head><p>    <script type=text/javascript src=https://www.google.com/jsapi></script><p>    <script type=text/javascript><p>    google.load(visualization, 1, {packages&#58;[corechart]});<p>        google.setOnLoadCallback(drawChart);<p>        function drawChart() {<p>        var data = google.visualization.arrayToDataTable([<p>                ['Time taking notes', 'Willing to use other notes', 'Not willing to use other notes', { role&#58; 'annotation' } ],<p>                ['Less than 30 mins', 7, 1, ''],<p>                ['30 mins', 10, 0, ''],<p>                ['60 mins', 6, 1, ''],<p>                ['Entire class', 9, 0, '']<p>    ]);<p>        var options = {<p>        width&#58; 600,<p>        height&#58; 400,<p>        legend&#58; { position&#58; 'top', maxLines&#58; 3 },<p>        bar&#58; { groupWidth&#58; '75&#37;' },<p>        hAxis&#58; {title&#58; 'Length of Note&#45;taking Period'},<p>        vAxis&#58; {title&#58; 'Number of respondents endorsing'},<p>        isStacked&#58; true,<p>        };<p>  var chart = new google.visualization.ColumnChart(document.getElementById('chart_div'));<p>  chart.draw(data, options);<p>}<p>    </script><p>  </head><p>  <body><p>    <div id=chart_div style=width&#58; 900px; height&#58; 500px;></div><p>  </body><p></html>
  is_the_quality_of_what_the_crowd_gives_you_a_concern: Yes
  did_you_create_a_user_interface_for_the_crowd_workers: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_crowdfacing_user_interface: https://github.com/indu&#45;subb/Nets213/blob/master/docs/user_interface/homepage.png<p>https://github.com/indu&#45;subb/Nets213/blob/master/docs/user_interface/homepage_lowrating.png<p>https://github.com/indu&#45;subb/Nets213/blob/master/docs/user_interface/rating_guidelines_page_1.png<p>https://github.com/indu&#45;subb/Nets213/blob/master/docs/user_interface/removed_page.png<p>https://github.com/indu&#45;subb/Nets213/blob/master/docs/user_interface/schedule_page_1.png<p>
  describe_your_crowdfacing_user_interface: Sign&#45;In Page&#58; Initial page of the website. Users are prompted to log in. If the user provides incorrect information or one of the fields is blank then the user is given an error. A link to the sign&#45;up page is provided. After a user is signed in they are redirect to the user home page. Only the sign&#45;in page and sign&#45;up page are accessible if a user has not logged in yet (all other pages will just redirect to the sign&#45;in page).<p>Sign&#45;Up Page&#58; This is the page where users sign up to the website. The user only needs to provide his/her name, make up a username, and decide on a password. If the username is already taken then the user is told to pick another username. If any of the fields are blank the user will be given an error message. The sign&#45;up page also has a link back to the sign&#45;in page incase the user remembers they have an account.<p>Homepage&#58; The homepage is the first page that the user is brought to. On the top of the page is the user’s average rating. If the rating is less than 2.5, a red message will be shown under the rating that warns the user that they may be kicked out of the website if they don’t raise their rating. Links to the rating guidelines page, course notes page, and course schedule page are provided. In addition the user can logout by clicking “logout” in the top right corner. The ability to rate other users is handled on the homepage. Users provide a username and rating and the dynamodb value is updated accordingly. If the user provides a username that doesn’t exist, a blank field, or tries to rate him/herself then an error message will appear.<p>Notes page&#58; On our notes page, our website embeds a google drive folder so that users can directly click on documents that have been added to the class. The google folder is set up so that any person with the link can add and view documents. As a result, we have provided the link to the google folder on the page to allow users to add notes. Users can go back to the homepage from this page.<p>Schedule Page&#58; On our schedule page, users are displayed their randomly assigned spot during which they need to take notes. Our website embeds a viewable google spreadsheet so that users can see what lecture and time corresponds to each slot. In addition, we have provided all the time slots for everyone else in the class so that if a user does not take notes then other classmates can give him/her a bad rating (and potentially get him kicked out of the website). Users can go back to the homepage from this page.<p>Rating Guidelines Page&#58; This page provides good note taking etiquette, rating descriptions, and examples of notes of quality 5, 3, and 1. Users can go back to the homepage from this page.<p>Removed Page&#58; If a user’s rating falls below 2 then they are kicked out of the website. When that user logs back in, they are redirected to this page which notifies the user that he/she has been removed from the website due to low note quality. The user’s session is terminated so that the user can no longer do anything on the website. 
  did_you_analyze_the_aggregated_results_: Yes
  what_analysis_did_you_perform_on_the_aggregated_results: 
  did_you_analyze_the_quality_of_what_you_got_back: No
  what_analysis_did_you_perform_on_the_aggregated_results: We sent out a survey asking people to compare the aggregated responses (two people taking turns note&#45;taking in a lecture) vs. individual responses (one person taking notes for the full lecture). After analyzing our data, we came to the conclusion that people who did have a preference between the two preferred the aggregated version, but that the majority of people did not have a preference between the two. Thus, allowing people to take notes for shorter periods of time and then aggregating them would most likely not have a detrimental effect on the quality of the notes (in fact, it appears that if anything, it would have a positive effect).
  what_analysis_did_you_perform_on_quality: 
  do_you_have_a_google_graph_analyzing_quality_: 
  if_you_have_a_graph_analyzing_quality_include_the_html_for_a_google_graph_here: 
  is_this_something_that_could_be_automated: No
  if_it_could_be_automated_say_how__if_it_is_difficult_or_impossible_to_automate_say_why: Note&#45;taking for a class would be very difficult with our current technology because even a direct transcription of the professor’s words would most likely come out pretty buggy. Once you add on the task of condensing the professor’s words and capturing the main points, as well as any notes or diagrams put up on the projector or the whiteboard, the task becomes well outside the scope of current AI and machine vision systems.<p>
  do_the_skills_of_individual_workers_vary_widely: Yes
  do_you_have_a_google_graph_analyzing_the_aggregated_results_: Yes
  if_you_have_a_graph_analyzing_the_aggregated_results_include_the_html_for_a_google_graph_here: <p><script type=text/javascript src=https://www.google.com/jsapi></script><p><script type=text/javascript><p>  google.load(visualization, '1.1', {packages&#58;['corechart']});<p>  google.setOnLoadCallback(drawChart);<p>  function drawChart() {<p>    var oldData = google.visualization.arrayToDataTable([<p>      ['Better Set of Notes', 'Number of Students'],<p>      ['Aggregated', 9], ['One note&#45;taker', 5],<p>      ['Both are about the same', 21]]);<p>    var options = { pieSliceText&#58; 'none' };<p>    var chartBefore = new google.visualization.PieChart(document.getElementById('piechart_before'));<p>    <p>    chartBefore.draw(oldData, options);<p>  }<p></script><p><span id='piechart_before' style='width&#58; 450px; display&#58; inline&#45;block'></span><p><span id='piechart_after' style='width&#58; 450px; display&#58; inline&#45;block'></span><p><br><p><span id='piechart_diff' style='width&#58; 450px; position&#58; absolute; left&#58; 250px'></span><p>
  did_you_create_a_user_interface_for_the_end_users_to_see_the_aggregated_results: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_user_interface_for_the_end_user: https://github.com/indu&#45;subb/Nets213/blob/master/docs/user_interface/notes_page.png
  describe_what_your_end_user_sees_in_this_interface: Our end users will see a screen of all the notes in their particular course stored in a Google Drive folder. From this screen, they can add their own course notes using the specified naming convention (in order to indicate which lecture and time period they took notes for). <p>
  what_is_the_scale_of_the_problem_that_you_are_trying_to_solve_: The scale of the problem that we are trying to solve is limited by the number of timeslots that a given class has in a given term. Since we would ultimately want each note&#45;taker to take notes at least twice in a semester in order for the quality control measures to kick in (i.e. for the rating system to get people to improve their note&#45;taking habits), this limits the number of people using our application to around 20&#45;something people per class.<p>
  if_it_would_benefit_from_a_huge_crowd_how_would_it_benefit_: 
  what_challenges_would_scaling_to_a_large_crowd_introduce: Scaling to a large crowd would introduce the issue of potentially having too many note&#45;takers for a single class. If this were the case, there would not be enough time slots for our quality&#45;control system to kick in, since each note&#45;taker would not take notes for enough timeslots to be adjust their note&#45;taking quality based on user feedback.<p>
  did_you_perform_an_analysis_about_how_to_scale_up_your_project: 
  what_analysis_did_you_perform_on_the_scaling_up: 
  do_you_have_a_google_graph_analyzing_scaling_: 
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  url_to_the_flow_diagram_for_your_project: https://github.com/indu&#45;subb/Nets213/blob/master/docs/FlowChart.png
  if_the_crowd_was_real_how_did_you_recruit_participants: 
  did_your_project_work: Yes, our project ended up working. We have a running and functioning version of our end goal up and running now, and using the test notes that we collected and took during class, we were able to log in as various users, upload our notes from our respective timeslots, view other people’s notes, rate other users, and view when we were scheduled to take notes for a specific class. <p>Given the results from our survey, we feel that our end product pretty closely aligns with the needs and preferences of the people we surveyed. The duration of the note&#45;taking timeslots matches that indicated by respondents in order to maintain a reasonable skill level, the aggregation method by which we combine disparate notes does not hinder people’s ability to obtain meaningful notes for each lecture (and in fact, there is some evidence to show that aggregation actually has a positive effect on note&#45;taking), and our application will especially appeal to people taking classes with their friends and people they know well (people they are most incentivized to share notes with).<p>We foresee that this project, while still in a “beta” version, will allow students to more effectively pay attention in class as they will not have to worry about actively taking notes throughout.<p>
  do_you_have_a_google_graph_analyzing_your_project_: No
  if_you_have_a_graph_analyzing_your_project_include_the_html_for_a_google_graph_here: 
  caption_for_your_google_graph_(project_analysis): 
  what_were_the_biggest_challenges_that_you_had_to_deal_with: The biggest challenges with our project had to do with the back&#45;end coding. We had great deal of trouble getting a functional database. We had 2 failed database prototypes before we finally got one that worked (in the first failed prototype we tried using MongoDB but we could not get login and logout to work and in the second failed prototype we used StormPath and node.js and could only get login and logout to work). We ended up using DynamoDB. We also had difficulty using the database to implement the rating system.<p>
  where_there_major_changes_between_what_you_originally_proposed_and_your_final_product: Yes
  if_so_what_changed_between_your_original_plan_and_your_final_product: Our original plans were quite ambitious, and given our time and resource constraints, we were forced to scale down. We did not automate the combining of notes from a single lecture &#45; our notes are not stored on our own database and instead use Google Drive. In hindsight, perhaps instead of having 2 people per time period, we could have just had 1 person per time period and then had a single Google Doc per lecture that everyone would type. (Even now, we are not sure that this streamlining would be the best approach, as we would lose some of the quality control in the process; in particular, if one person missed their timeslot, there would be no notes or back&#45;up for that period of time, and everyone would be left noteless). Additionally, we originally planned on being able to rate notes directly. In our product, you rate users based off their notes. Last, our schedule method was also slightly simplified. Initially, our scheduler was going to create multiple groups based on the number of people that signed up for the class. The number of slots per student and time per slot was going to dynamically change based on the number of students per group.<p>
  what_are_some_limitations_of_your_product: As described in our changes from our original plan, at this time our project does not automate the combining of notes, does not allow users to directly rate notes, and does not have a dynamic scheduler.<p>As mentioned in the “Scaling Up” section, having a large influx of interested note&#45;takers would pose the issue of not having enough timeslots for everyone to partake effectively (i.e. with rating systems being enforced). In this sense, maintaining strong quality control would be our biggest issue in scaling up our system. Another aspect that would have to be considered is the cost of maintaining a larger database of users and notes, especially if we one day migrate our note storage from Google Drive to our own servers.<p>
  did_your_results_deviate_from_what_you_would_expect_from_previous_work_or_what_you_learned_in_the_class: No
  if_your_results_deviated_why_might_that_be: 
  caption_for_your_graph_(scaling_up): 
  caption_for_your_graph_(aggregation): Preference for aggregated vs. individual notes
  caption_for_your_graph_(quality): 
  caption_for_your_graph_(skills): Duration of diligent note&#45;taking before missing information
  caption_for_your_graph_(incentives): Willingness to share notes by incentive type
  is_there_anything_else_youd_like_to_say_about_your_project: 
  did_you_train_a_machine_learning_component_from_what_the_crowd_gave_you: No
  if_you_trained_a_machine_learning_component_describe_what_you_did: 
  did_you_analyze_the_quality_of_the_machine_learning_component: 
  if_you_have_a_graph_analyzing_a_machine_learning_component_include_the_html_for_a_google_graph_here: 
  caption_for_your_graph_(incentives): 
  caption_for_your_graph_(machine_learning_component): 
  what_was_the_main_focus_of_your_teams_effort: Engineering a complex system
  provide_a_link_to_your_final_presentation_video: 
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_skills_: 
  what_incentivi: 
-
  how_many_teammates_are_in_your_group: 3
  name_1: Chenyang Lei
  name_2: Ben Gitles
  name_3: Abhishek Gadiraju
  name_4: 
  pennkey_username_1: chenylei
  pennkey_username_2: gitlesb
  pennkey_username_3: agad
  pennkey_username_4: 
  name_of_your_project: Crowdsourced Candidates
  give_a_one_sentence_description_of_your_project: Crowdsourced Candidates uses the crowd to generate candidates for President of the United States and asks the crowd to vote on them.
  url_to_the_logo_for_your_project: https://github.com/bengitles/final_project/blob/master/docs/logo.png
  what_similar_projects_exist: Our project falls into the general domain of crowdsourcing for a political election. Here are two most related fields&#58;<p>1) There have been projects under prediction market domain where crowds are used as a source for yielding accurate predictions on political election outcome, including the number of votes. Here is one general example&#58; http://themonkeycage.org/2012/12/19/how&#45;representative&#45;are&#45;amazon&#45;mechanical&#45;turk&#45;workers/<p>The results were astoundingly good, which ranked second after the most prestigious election predictor. <p>2) There has also been research studying the demographic background and political interest of the crowds on Amazon Mechanical Turk. Here is the research&#58; http://scholar.harvard.edu/files/dtingley/files/whoarethesepeople.pdf<p>The results showed that they are from very diverse backgrounds, though slightly more towards Democrats politically. They can actually be a fairly good representation of the general voting population.<p>
  what_type_of_project_is_it: Social science experiment with the crowd
  how_do_you_incentivize_the_crowd_to_participate: In both parts of the project (the initial survey and the vote), the crowd is incentivized by pay. Chris paid people to answer hundreds of questions about their political views, and we pay people to make a simple judgement between hypothetical candidates. Because we didn’t design the incentive scheme for Chris’s survey, we will just talk about the voting part.<p>At first, we thought that we’d be able to pay people 1 cent per HIT. However, we soon realized that we would need to pay them more in order to complete the job in a timely manner, so we went up to 2 cents, then 3, then 4, then 5, then 7, then 10. We also varied the number of units that each individual was allowed to complete a HIT (increasing it to try to attract more workers), and we varied the maximum allowed time to complete a HIT (lowering it, with the hope that workers will then know how quick of a task it is).<p>Also, we consciously made our HIT as concise as possible. We did this because workers are pressed for time in order to make the most money possible. We didn’t want workers to avoid clicking on our HIT because of a long title or abort our HIT because it took too long. We made the HIT as visually appealing and simple as possible. We believe that this essentially acts as an incentive for workers because they can complete the HIT in as short of a time as possible, allowing them to go on and complete other HITs and make more money.<p>
  what_does_the_crowd_provide_for_you: The crowd provides us with a large, diverse set of individuals’ political opinions. After analyzing and whittling this set down to a select few, we have the crowd vote between them in order to determine which one is the most popular.<p>
  how_do_you_ensure_the_quality_of_the_crowd_provides__: We have gold standard quality control questions, for which we wrote a script to automatically filter the results based on gold standard questions. We also try to instill a sense of civic duty in the workers. Also, implicit in our voting mechanism is agreement&#45;based quality control. See below.
  how_do_you_aggregate_the_results_from_the_crowd: First, we implemented the quality control mechanism in code that was described above. Furthermore, we kept track of how many people answered Candidate A and how many people answered Candidate B. Because we randomized the order of the candidates, these value should be roughly equal, give or take by 10 or 15 votes. These two checks helped us verify that our results are not heavily biased and keeps the quality of our filtered data at a high level.<p>Next, we aggregate the “election” based on a global and pairwise scale. We keep track of the candidate that wins the most elections overall, but we also keep track of how many wins each candidate has against the other candidates. This is stored in the form of a 3x3 “confusion” matrix M, where M[i][j] indicates the number of times candidate i beat candidate j in a head&#45;to&#45;head election. Clearly, the sum of the diagonals of this matrix are 0, and the sum over one row i shows the number of total wins that candidate i has. This matrix also allows us to compare the pairwise values using the transpose operation. <p>
  how_does_your_project_work: Step 1&#58; Instead of collecting data from scratch, we are reusing the data from Chris’s political surveys conducted on 2008 and 2012, which were very comprehensive and were designed by professionals. [crowdsourced]<p>Step 2&#58; There are many political questions with an associated importance score. We sort these questions based on their average importance scores and picked out some of the questions that workers indicate that they care the most about. [automated]<p>Step 3&#58; We then consider each person in the crowd as a candidate and run a k&#45;means clustering across them, where the features are the important questions we extracted in the previous step.<p>We first run the algorithm to get one cluster, which is the center of mass and should be the most preferred one according to the median voter theorem. Then we run the algorithm to get two clusters, which is similar to the US presidential election. We finally run the algorithm on three clusters to better represent the complete data. For each of the 6 clusters above, we choose the center of the cluster as our “ideal candidates”. [automated]<p>Ultimately, we decided that for the sake of our experiment, we would only use the candidates from the three clusters.<p>Step 4&#58;  With the three ideal candidates we generated in the previous step, we designed our HIT interface to ask the crows vote on them again with their information provided. The resulting candidate with the highest votes is then our best ideal candidate. In the process we can have many reflections/comparison and ask many interesting questions. [crowdsourced]<p>
  would_your_project_benefit_if_you_could_get_contributions_from_thousands_of_people: Yes
  vimeo_link: https://vimeo.com/114542643
  pennkey_username_1: 
  may_we_have_your_permission_to_feature_your_project_on_the_class_website: Yes
  team_member_1__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_2__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_3__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_4__can_we_list_your_name_listed_alongside_your_project: 
  who_are_the_members_of_your_crowd: Workers on Mechanical Turk and CrowdFlower
  how_many_unique_participants_did_you_have: 400
  for_your_final_project_did_you_simulate_the_crowd_or_run_a_real_experiment: Real crowd
  if_the_crowd_was_simulated_how_did_you_collect_this_set_of_data: 
  if_the_crowd_was_simulated_how_would_you_change_things_to_use_a_real_crowd: 
  how_do_you_ensure_the_quality_of_the_crowd_provides: 
  what_motivation_does_the_crowd_have_for_participating_in_your_project: Pay, Civic Duty
  did_you_perform_any_analysis_comparing_different_incentives_: Yes
  if_you_compared_different_incentives_what_analysis_did_you_perform_: Our analysis of incentives came more out of necessity than our of design. Initially, we did a couple of trial runs with just 10 impressions to work out any kinks. We realized that we forgot to limit the number of times that an individual could complete the HIT (one of our sets of 10 HITs was completed entirely by one individual), and we forgot to limit the user set to just the United States. (We are analyzing American politics here, after all.)<p>These initial test runs were completed very quickly, so on our “real” job, we paid 1 cent per HIT completion, we limited the number times that an individual could complete the HIT to 1, and we limited our worker base to the US. However, we quickly realized that very few people were taking the HIT. We gradually raised the payment per HIT to 2 cents, then 3, then 4, then 5. In raising the amount paid, we had to lower the number of judgements per unit. This was not ideal because we wanted to be able to claim that we had a significantly large number of votes per unit, which would help us in our claim that we are trying to represent all of the United States.<p>Also along these lines, we were forced to raise the number of times that an individual could complete the HIT from 1 to 3 times. This also was not ideal because we wanted our voter base to be very diverse. By having the same user make several judgements, it makes the voters as a whole less diverse, and it lessens our claim that our voter base is representative of the US population.<p>
  if_you_have_a_graph_analyzing_incentives_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_incentives_: 
  do_your_crowd_workers_need_specialized_skills: No
  what_sort_of_skills_do_they_need: In both crowdsourced sections of our project, the workers need to understand the political questions or stance put in front of them. We do not consider this to be a particularly “specialized” skill. 
  if_skills_vary_widely_what_factors_cause_one_person_to_be_better_than_another_: Inherent in a democracy is the idea that everyone’s vote counts equally, no matter how well&#45;informed anyone is. So even if some workers are political buffs while others don’t know the difference between Obamacare and the Affordable Care Act (http://youtu.be/xBFW_2X4E&#45;Q), each of their contributions count just the same.<p>
  did_you_analyze_the_skills_of_the_crowd: No
  if_you_analyzed_skills_what_analysis_did_you_perform: 
  do_you_have_a_google_graph_analyzing_skills_: 
  if_you_have_a_graph_analyzing_skills_include_the_html_for_a_google_graph_here: 
  is_the_quality_of_what_the_crowd_gives_you_a_concern: Yes
  did_you_create_a_user_interface_for_the_crowd_workers: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_crowdfacing_user_interface: https://github.com/bengitles/final_project/blob/master/docs/ui_screenshot.png
  describe_your_crowdfacing_user_interface: We wanted to make the user interface as simple as possible. We quickly realized that a table format was best, but we still had to go through many different iterations. We tried having the candidates as the columns before inverting the table. We had individual rows for the explanations of what “1” or “7” meant, but we then realized that it was better just to have explanations in&#45;line with the header. Also, we initially started with having voters rank three candidates that they are presented with, but we eventually went down to two candidates to make it simpler.<p>	You can see the evolution of our UI in the docs section of our github.<p>
  did_you_analyze_the_aggregated_results_: Yes
  what_analysis_did_you_perform_on_the_aggregated_results: 
  did_you_analyze_the_quality_of_what_you_got_back: Yes
  what_analysis_did_you_perform_on_the_aggregated_results: Our results are plotted in a stacked bar graph format. Our three candidates are roughly aligned with the three major political alignments in the United States&#58; Democrat/Liberal, Independent/Neutral, Republican/Conservative. The bar graph demonstrates the total wins of each candidate along with information about how many times a specific candidate beat another candidate in a head&#45;to&#45;head election. Crowdflower ideally pits an equal number of pairs against each other in the aggregation. <p>	Another interesting feature we included in the HIT was the ability to add an optional comment describing why you voted a certain way. Most workers do not fill this section out. Therefore, by rule, the ones who do fill this section out probably feel very strongly about the comments they are writing. We proceeded to scrape each non&#45;empty answer for phrases like “liberal”, “conservative”, “gun”, “government”, “spending”, “health”, and “defense”. These frequencies are plotted on a bar graph to see if the issues people thought strongly enough to comment about correspond to Chris’s original importance score from his given dataset.<p>	Lastly, we wanted to identify any crippling biases in our voter constituency that we could simply not control. The most obvious example of this was if many voters from a specific state that is known to be Democratic or Republican voted in our election. We plotted the number of voters from each state and colored how many times voters went liberal, conservative, or neither in each state via a stacked bar graph.<p>&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;<p>We certainly did go through our Crowdflower full csv file and take a look at individual responses to make sure it aligned with what we were seeing at a global level. We saw that many people who commented sometimes preferred neither candidate because they were both too liberal or both not liberal enough. We decided not to run another job where “Neither” was an option because we were concerned about how many workers would simply opt out of voting because the candidates were not exactly suited to their preferences. Think about an actual election &#45;&#45; you’ve got two (maybe three) people to vote for. Submitting a write&#45;in is statistically the same as abstaining from voting. While abstaining from voting is certainly something that happens in real life, we were concerned it would happen at a disproportionate rate when peer pressure and party marketing campaigns were factors not at our disposal.<p>
  what_analysis_did_you_perform_on_quality: The first of our two quality control questions was&#58;<p>Which statement would Candidate A agree with more?<p>&#45;The government should provide health care for all citizens.<p>&#45;All citizens should purchase private health insurance.<p>&#45;Candidate A would be indifferent between the two statements above.<p>We included this question before asking the worker which candidate he prefers. We did this because we wanted to make sure that the worker carefully read and understood the table with the candidates’ political stances.<p>Our second quality control question was, “What sound does a kitty cat make?” This was simply a secondary safeguard in case the worker got the first question right by random chance. While there is still a 1/12 chance that a worker could have gotten by both of these questions by randomly clicking, our intention was moreso to act as a deterrent to random clickers and to force them to slow down and think when they may not have otherwise.<p>We also included an optional free response text explanation question, where the workers are asked to answer why they prefer the chosen candidate. While most candidates did not fill this out, it did give them an opportunity to provide some feedback, and it may have inspired some to think a bit more deeply about exactly why they were choosing the candidate that they did.<p>	We also tried to invoke a sense of civic duty in our HIT. The title of the HIT was, “Vote for POTUS”, and the instructions were, “Imagine these candidates running for President of the United States. Study them carefully, then indicate your preferences below.” By imagining that they are actually voting for President, workers will be inspired with a sense of civic duty to actually look over the candidates carefully and truly select their favorite.<p>	<p>	As James Surowiecki explains in The Wisdom of Crowds, any guess is the true value plus some error. When a crowd independently assembles and votes, all of the errors cancel out, and you are left with the true value. Similarly, in this case, even if people do arbitrarily choose one candidate, we can see this as random error that will ultimately be cancelled out. We made sure that there would be no bias from our end by randomizing the order in which candidates are presented to the voters.
  do_you_have_a_google_graph_analyzing_quality_: No
  if_you_have_a_graph_analyzing_quality_include_the_html_for_a_google_graph_here: 
  is_this_something_that_could_be_automated: Yes
  if_it_could_be_automated_say_how__if_it_is_difficult_or_impossible_to_automate_say_why: In general, you cannot automate individuals’ opinions, so it is impossible for the entire system to be automated. However, it is hypothetically possible to automate one of the two crowdsourced parts. We could have decided to get rid of the final stage of our project and not even asked real individuals to vote between candidates. Instead, we could have said that the median candidate &#45; the “center of mass” of everyone polled &#45; was the idea candidate, as the Median Voter Theory would state. We decided not to do this because we figured it would be more interesting to see if the Median Voter Theory holds up in real life.<p>        Alternatively, we could have automated the candidate generation part. We could have created every single possible hypothetical combination of political opinions, then asked the crowd to vote between every single one of these. This would have likely been a very tedious task for workers because we would have had no sense of which issues are most important to voters, so we would have to display all of them. This tedium would likely force us to pay more per HIT, and considering the extremely large number of possible combinations of political opinions across all issues, the cost would have been inordinate.
  do_the_skills_of_individual_workers_vary_widely: No
  do_you_have_a_google_graph_analyzing_the_aggregated_results_: No
  if_you_have_a_graph_analyzing_the_aggregated_results_include_the_html_for_a_google_graph_here: 
  did_you_create_a_user_interface_for_the_end_users_to_see_the_aggregated_results: No
  if_yes_please_give_the_url_to_a_screenshot_of_the_user_interface_for_the_end_user: 
  describe_what_your_end_user_sees_in_this_interface: 
  what_is_the_scale_of_the_problem_that_you_are_trying_to_solve_: The question of finding and selecting the best candidate to run for a position is an enormous one, both in terms of scale and importance. In the most directly applicable sense, this system could be helpful for a political party. Take, for example, the Republican party. During their primaries, millions of dollars are invested in campaigning, just to help a candidate win the Republican party. Perhaps, instead of making candidates essentially waste that money, the Republican party could crowdsource a survey within its own party members to ask for their political opinions. Then, by either finding the centroid or by holding a vote between potential candidates, the party could find its ideal candidate’s set of preferences. The GOP could then go back to its initial survey and find its party member who best matches these preferences. This would save enormous amounts of money from donors, which could then be used to help this candidate defeat the Democratic opponent in the general election.<p>	There are obviously a lot of problems with this idea, the largest of which is that people don’t purely vote based on a strict set of political preferences. Perhaps a more realistic suggestion is that, rather than this system being the sole determinant of the party’s nominee, it can be just one data point in a much larger decision making process. A candidate who claims to be “centrist” or “moderate” can have data to back it up. A donor who is looking for a likely winner can find a candidate that has a strong chance of winning. A candidate can morph his platform to match the most popular opinions.<p>Political surveys have been around for a long time, but with the scale and diversity of crowdsourcing, along with the added twist of clustering like&#45;minded voters, adds a new and interesting layer of complexity.<p>
  if_it_would_benefit_from_a_huge_crowd_how_would_it_benefit_: In collecting the original data, if we have a huge crowd, we can more accurately cluster on “ideal candidates” with more representative features. <p>In voting for the best “ideal candidate”, if we have a huge crowd, they can better represent the general public and yield more meaningful results. We can also design more complex quality control systems and remove the potential biases and noises. Some examples could be including their demographic information or conducting text analysis on their explanations. <p>
  what_challenges_would_scaling_to_a_large_crowd_introduce: The biggest challenge with a large crowd would be the cost. We were very thankful to be able to use Chris’s survey, which cost $5 per response for a total of approximately $5000. In order to recruit a larger crowd, one would have to pay more people to do the same $5 survey, or perhaps they could pay less. In order to pay less, it would be logical to cut down on the number of questions asked. This would be a challenge as well, trying to figure out exactly which questions to ask. Perhaps we could use information from the initial survey&#45;&#45;i.e. which issues are least important to voters&#45;&#45;to determine which questions to cut out.
  did_you_perform_an_analysis_about_how_to_scale_up_your_project: No
  what_analysis_did_you_perform_on_the_scaling_up: 
  do_you_have_a_google_graph_analyzing_scaling_: No
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  url_to_the_flow_diagram_for_your_project: https://github.com/bengitles/final_project/blob/master/docs/final_project_flow_diagram.png
  if_the_crowd_was_real_how_did_you_recruit_participants: Chris gave us his data from his political survey. He paid Mechanical Turk workers $5 to complete the survey.<p>For our voting, we posted HITs on CrowdFlower and paid users an amount varying from 1 to 10 cents to select a preferred candidate
  did_your_project_work: Yes, it worked.<p>We found that the liberal Democrat candidate got the most votes, followed by the moderate candidate, followed by the conservative Republican candidate. This confirms a theory called Single Peaked Preferences &#45; in any situation where you can rank candidates on a linear spectrum, there will be a single peak. If we had found that the Democrat and the Republican had gotten the most votes and the moderate got the fewest, Single Peaked Preferences would have been violated. So this is a clear demonstration of the theory holding true.
  do_you_have_a_google_graph_analyzing_your_project_: No
  if_you_have_a_graph_analyzing_your_project_include_the_html_for_a_google_graph_here: 
  caption_for_your_google_graph_(project_analysis): 
  what_were_the_biggest_challenges_that_you_had_to_deal_with: At first, we wanted to do our project about people judging candidates’ faces on their perceived competency and trustworthiness and seeing how those measures compared to the results of the real&#45;life elections. Similar experiments had been done in the past at universities by psychologists. However, we decided to abandon that plan when we realized that we had little chance of being able to re&#45;create the results of a psychology lab at a university. Also, the problem was more of a sociological one than a technical one. Eventually, we morphed the idea into what we now have as Crowdsourced Candidates.<p>Technically, finding the right features to select for our k&#45;means clustering was very difficult. During the initial phases, we chose the features based on our own intuition, not on any data. While this worked fine in our initial stages, we realized that we needed a more diverse group of features in order to bring out the true diversity in the data. Tuning and fixing these features was an ongoing process that is still not perfect, but has hard data and importance scores to back it up. Furthermore, finding ways to visualize our clustering was confusing at first. How do you visualize an n&#45;dimensional feature set for clustering? We did some research and realized that while PCA was not a perfect solution since it ignores all but the two most important features, it was important to use it and get a sense of the diversity in our data. Finally, even though we used three different k values for our clustering, in the end we realized that the medoids of the k = 1 and k = 2 clusterings were very similar to the medoids of the k = 3 clustering.<p>	Getting the HITs completed in a timely manner was difficult. Our whole time planning, we assumed that it would only cost 1 cent per unit because it is a relatively simple, quick judgement compared to the HITs that we have put out in previous homeworks. However, we failed to consider that in this case, we had to limit ourselves to just the United States. Also, we didn’t consider how workers value being able to fill out the same HIT repeatedly. Eventually, we had to raise the payments considerably, as well as the number of times that an individual could complete the HIT.<p>
  where_there_major_changes_between_what_you_originally_proposed_and_your_final_product: Yes
  if_so_what_changed_between_your_original_plan_and_your_final_product: Our original plan was to ask the crowds to make judgements on Penn Student Government candidates based on their photos. We were trying to see how people’s appearance would affect others’ perception on their competence and how that correlates with election outcomes. <p>We encountered two main problems&#58; 1) On the data side, photos are hard standardized and have many noises. The dataset is also too small; 2) the project itself is too fragile, because the positive/negative results will not yield any interesting insights besides proving/disproving an assumption.<p>To overcome the first problem, we then used political survey data from 2008 and 2012, conducted by authorities with well&#45;designed and comprehensive questions. They are all text based thus are easy to integrate and analyze. <p>With the new data, considering the median voter theorem, which states that a majority rule voting system will select the outcome most preferred by the median voter, we then run a k&#45;means clustering algorithm on the dataset trying to find the “ideal candidates”. With multiple cluster finds, we started to wondering which ideal candidate is the best, which cannot be inferred from the existing dataset. <p>We then conducted another round of crowdsourcing on the Crowdflower, asking the workers to vote on the ideal candidates we generated. <p>The whole process consists of clustering on existing data and a crowdsourced voting system on ideal candidates. <p>
  what_are_some_limitations_of_your_product: We have already talked about this extensively in other questions, namely about how our sample size is not large enough to make statistically significant conclusions. Also, there are many other factors to consider in electing a president aside from just the six factors that we presented.<p>&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;<p>From engineering perspective, as a scientific study, there are several problems with our study. First off, we did not obtain anywhere near the scale that we wanted to. Our plan was to get hundreds of impressions per pair of candidates, ideally each one coming from a unique person. We wanted to simulate a real election as closely as possible. However, due to the time and cost limitations previously mentioned, we had to compromise on these matters.<p>	Another possible source of error has to do with user incentives and quality control. Whereas in a real election voters have a true sense of civic duty and a vested interest in voting for the candidate that they truly prefer, voters in this study had no such incentive. Also, voters (at least hypothetically) try to really learn and understand the positions of candidates before voting for one. We tried to instill a sense of civic duty, and we tried to add measures that forced the workers to carefully consider the candidates presented to them. However, it is certainly possible that workers read just enough to get by and arbitrarily voted for a candidate because there were no repercussions for doing so.<p>
  did_your_results_deviate_from_what_you_would_expect_from_previous_work_or_what_you_learned_in_the_class: No
  if_your_results_deviated_why_might_that_be: 
  caption_for_your_graph_(scaling_up): 
  caption_for_your_graph_(aggregation): 
  caption_for_your_graph_(quality): 
  caption_for_your_graph_(skills): 
  caption_for_your_graph_(incentives): 
  is_there_anything_else_youd_like_to_say_about_your_project: We decided not to use Google Charts to create visuals, but we did create several charts using MatPlotLib that are uploaded to the docs folder of our github.
  did_you_train_a_machine_learning_component_from_what_the_crowd_gave_you: Yes
  if_you_trained_a_machine_learning_component_describe_what_you_did: One of the main challenges from this project was taking the ANES 2008 dataset that Professor Callison&#45;Burch provided us and extracting a set of candidates that could be representative of the entire dataset. We used the k&#45;means clustering algorithm, a common machine learning technique, to create clusters of user answers and then chose the centroid from each of these clusters. Finally, we then computed the medoid of each cluster (an actual data point that is linearly closest to the centroid in a given cluster). <p>	The main work involved with building the machine learning was choosing a feature set that represented the vast array of issues that everyday politicians and pundits debate and discuss for the benefit of the American public. Conversely, we could not choose too many features, as every feature included in our machine learning model would correspond to an extra consideration that our Crowdflower worker would have to make when judging a candidate. We decided to analyze the features in Professor Callison&#45;Burch’s dataset that were the most important to his workers, and then pick four of these features that spanned the broadest categories of issues (government, social, economical, etc.). For each issue, Professor Callison&#45;Burch’s dataset contained a question that asked for a worker’s stance on the issue and another question about how important the issue was to the worker, generally on a 1 &#45; 7 scale. We took the set of questions that touched on almost every major issue in the US and analyzed how important the workers thought these issues were in aggregate. After manually filtering for similar issues and feature diversity, we ultimately decided that we would include four different features in our clustering model&#58; stance on defense spending, stance on healthcare spending, the responsibility of the government with regards to employment, and stance on gun control. Additionally, we added features to indicate whether the worker aligned with a particular political party, a particular ideology, and whether the worker considered their ideal candidate to be liberal or not. This last feature was added as a pseudo&#45;quality control mechanism; since workers could choose not to align themselves with an ideology or political party, this question was meant to serve as a binary “yes/no” type of question. <p>	The values of the features were kept within similar ranges in order to avoid the need for feature scaling (a standard machine learning practice). The preferences with regards to the four political features were kept on a 1 &#45; 7 scale, in line with the original dataset. Generally, 1 indicated the extreme “liberal” side of the issue and 7 indicated the extreme conservative side of the issue. To align with this value choice, the other features also took on lower values for liberal tendencies. If a worker indicated they were democratic/liberal, that feature got a score of &#45;1; conversely, conservative votes would get a score of 1. Choosing not to align with a party and/or ideology received a score of 0.<p>	We performed the k&#45;means clustering algorithm with k = 1, 2 and 3. This gave us the overall “center of mass” for all workers (k = 1), the two centers of mass to reflect Democratic vs. Republican leanings (k = 2), and a tripartition to represent a possible middle ground that workers may prefer (k = 3). We then combined the medoids from these clusters into a 6&#45;candidate “ballot”. Unfortunately, we realized that the k = 1 and k = 2 medoids were roughly the same as the three k = 3 medoids. After translating the features back into human&#45;readable qualities, we put the file on Crowdflower for voting. Voting was done across all possible 6 pairwise permutations (accounting for randomized order). <p>
  did_you_analyze_the_quality_of_the_machine_learning_component: No
  if_you_have_a_graph_analyzing_a_machine_learning_component_include_the_html_for_a_google_graph_here: 
  caption_for_your_graph_(incentives): 
  caption_for_your_graph_(machine_learning_component): 
  what_was_the_main_focus_of_your_teams_effort: Conducting an in depth analysis of data
  provide_a_link_to_your_final_presentation_video: 
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_skills_: 
  what_incentivi: 
-
  how_many_teammates_are_in_your_group: 3
  name_1: Tiffany Lu
  name_2: Morgan Snyder
  name_3: Boyang Niu
  name_4: 
  pennkey_username_1: tifflu
  pennkey_username_2: mosnyder
  pennkey_username_3: niub
  pennkey_username_4: 
  name_of_your_project: Venn
  give_a_one_sentence_description_of_your_project: Venn uses human computation to bridge social circles by making personalized suggestions for platonic connections.
  url_to_the_logo_for_your_project: https://github.com/kumquatexpress/Venn/blob/master/static/images/Venn.png
  what_similar_projects_exist: Facebook’s “Suggested Friends” function<p>Women&#45;only social networking websites&#58; <p>GirlFriendCircles.com, GirlfriendSocial.com, SocialJane.com<p>All&#45;gender platonic matchmaking&#58; not4dating.com, bestfriendmatch.com
  what_type_of_project_is_it: Human computation algorithm, Social science experiment with the crowd
  how_do_you_incentivize_the_crowd_to_participate: (1) Participation allows for personalized friend suggestions (ultimate benefit to the user, and the purpose of the app)<p>(2) Fun quizlet interface <p>(3) Point system for successful matches<p>(4) Bridging social circles allows user to integrate previously separate friend groups
  what_does_the_crowd_provide_for_you: The crowd gives us information about their likes and dislikes through answering questions about themselves. They’re also responsible for giving opinions on the relationship potential between other users, whether through prior knowledge of one of the people, or by looking at and manually matching between their profile answers.
  how_do_you_ensure_the_quality_of_the_crowd_provides__: The users are incentivized to give accurate answers to their own profile questions, as doing this allows them to have a more accurate matching with other users. Failing to answer profile questions truthfully does not impact the other users of the app at all. On the other hand, questions about the relationship between a pair of people are not tracked using incentives, which is why we implemented the learning decision tree (noted in above section) that attempts to output a majority classification dependent on what it knows from previous answers. We can compare the tree’s answer and the other users’ answers for a particular relationship and use the weighted majority vote in this scenario, so any bad answers will be filtered out.
  how_do_you_aggregate_the_results_from_the_crowd: When looking at a relationship model between two users (u,v), we treat their answers to profile questions as two feature vectors. By taking the cosine similarity between these vectors and mapping the resultant value between [0, 100], we come up with a similarity number which we call the profile_value p. This makes up one half of the equation for aggregation and is entirely based on the users’ own answers. The second half comes from other users’ answers about this particular relationship, which is also a feature vector user_values with numbers [0,10]. We take the average of user_values and put this together with profile_value in a weighted average to come up with a final overall value [0, 100] that represents how well we think the two users would get along.
  how_does_your_project_work: 1. Users create an account on Venn.<p>2. Users can begin to interact with the Venn interface, which has two sets of question prompts&#58;<p>a) Users answer questions about themselves (e.g., “Would you rather A or B?” “What describes you best&#58; …,” “What are your hobbies?”)<p>b) Users are given information about two other users, and must decide if they would be a compatible friendship match<p>3. Based on the information provided by the crowd in step 2, users are given suggested friends, tailored to their personalities.<p>Unimplemented but potential future directions&#58;<p>4. Venn will redirect back to a messaging application (e.g., FB Messenger) that will allow you to reach out to your suggested friends. Users can mark suggested friendships as successful or not successful, which will feed back into the system for training purposes AND which will also allow users to check on the status of their suggested friendships.
  would_your_project_benefit_if_you_could_get_contributions_from_thousands_of_people: Yes
  vimeo_link: https://vimeo.com/114563698
  pennkey_username_1: 
  may_we_have_your_permission_to_feature_your_project_on_the_class_website: Yes
  team_member_1__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_2__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_3__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_4__can_we_list_your_name_listed_alongside_your_project: 
  who_are_the_members_of_your_crowd: Current Penn students.
  how_many_unique_participants_did_you_have: 18
  for_your_final_project_did_you_simulate_the_crowd_or_run_a_real_experiment: Real crowd
  if_the_crowd_was_simulated_how_did_you_collect_this_set_of_data: 
  if_the_crowd_was_simulated_how_would_you_change_things_to_use_a_real_crowd: 
  how_do_you_ensure_the_quality_of_the_crowd_provides: 
  what_motivation_does_the_crowd_have_for_participating_in_your_project: Altruism, Enjoyment
  did_you_perform_any_analysis_comparing_different_incentives_: No
  if_you_compared_different_incentives_what_analysis_did_you_perform_: 
  if_you_have_a_graph_analyzing_incentives_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_incentives_: 
  do_your_crowd_workers_need_specialized_skills: No
  what_sort_of_skills_do_they_need: Our users are required to answer questions of a personal and social nature. They are required to read and understand English. It also helps to have users who are familiar with using a quiz online interface. Venn’s most “skilled” workers are those who are willing to attentively examine two other users, and evaluate if they make a good match &#45;&#45; requiring both empathy and foresight.
  if_skills_vary_widely_what_factors_cause_one_person_to_be_better_than_another_: 
  did_you_analyze_the_skills_of_the_crowd: No
  if_you_analyzed_skills_what_analysis_did_you_perform: 
  do_you_have_a_google_graph_analyzing_skills_: No
  if_you_have_a_graph_analyzing_skills_include_the_html_for_a_google_graph_here: 
  is_the_quality_of_what_the_crowd_gives_you_a_concern: Yes
  did_you_create_a_user_interface_for_the_crowd_workers: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_crowdfacing_user_interface: https://github.com/kumquatexpress/Venn/blob/master/screenshots/match.png<p>https://github.com/kumquatexpress/Venn/blob/master/screenshots/personality_beach.png
  describe_your_crowdfacing_user_interface: The interface is a simple quizlet built from html and bootstrap, the left side contains the question which is either&#58;<p>1) In the form of how much do you like/dislike this image<p>2) In the form of how well do you think this pair of users would get along<p>And the right is a slider from 1 to 10 that determines the answer.
  did_you_analyze_the_aggregated_results_: Yes
  what_analysis_did_you_perform_on_the_aggregated_results: 
  did_you_analyze_the_quality_of_what_you_got_back: No
  what_analysis_did_you_perform_on_the_aggregated_results: We compared our profile_value number which is generated using cosine similarity on user answers to the estimates by users themselves over our userbase. For instance, when two users say they like the same things, is someone else who knows one of the users more likely to rate this relationship highly (and vice versa)?
  what_analysis_did_you_perform_on_quality: It was difficult to do analysis on quality of answers because the majority of our questions were based on the opinions of our users. There were no right or wrong answers and so we had to take the answers we were given at face value. Additionally the project lacked the scope necessary to accumulate a large number of users and answers, so we couldn’t track the quality of individual users in relation to the majority.
  do_you_have_a_google_graph_analyzing_quality_: No
  if_you_have_a_graph_analyzing_quality_include_the_html_for_a_google_graph_here: 
  is_this_something_that_could_be_automated: No
  if_it_could_be_automated_say_how__if_it_is_difficult_or_impossible_to_automate_say_why: The opinions that we obtain from the crowd aren’t trainable through any learning techniques because they rely on predisposed knowledge from each of the users instead of purely on information that we provide to them. We would be able to estimate how a user might respond in certain cases, but for cases where the user is friends with someone they give an opinion on, personal knowledge might influence their decision&#45;&#45;this is something we have no control over.
  do_the_skills_of_individual_workers_vary_widely: No
  do_you_have_a_google_graph_analyzing_the_aggregated_results_: Yes
  if_you_have_a_graph_analyzing_the_aggregated_results_include_the_html_for_a_google_graph_here: <html><p>  <head><p>    <script type=text/javascript src=https://www.google.com/jsapi></script><p>    <script type=text/javascript><p>      google.load(visualization, 1, {packages&#58;[corechart]});<p>      google.setOnLoadCallback(drawChart);<p>      function drawChart() {<p>        var data = google.visualization.arrayToDataTable([<p>          ['Profile Similarity', 'User Judged Opinion'],<p>          [ 98.15,  5.7],<p>          [ 87.65,  4.3],<p>          [ 89.77,  4.0],<p>          [ 92.41,  6.7],<p>          [ 99.15,  7.2],<p>          [ 97.80,  6.7],<p>          [ 77.18, 4.5],<p>          [ 85.11, 5.5],<p>          [ 94.57, 8.0],<p>          [ 89.54, 5.6],<p>          [ 86.10, 4.4],<p>          [ 97.65, 8.5],<p>          [ 95.68, 7.8],<p>          [ 93.43, 8.2],<p>          [ 78.19, 3.3],<p>          [99.15, 8.1],<p>          [ 95.14, 8.2],<p>          [ 82.32, 5.1]<p>        ]);<p>        var options = {<p>          title&#58; 'Profile Similarity vs. Crowdsourced Relationship Opinion',<p>          hAxis&#58; {title&#58; 'Profile Sim.', minValue&#58; 85, maxValue&#58; 95},<p>          vAxis&#58; {title&#58; 'Crowdsourced Opinion', minValue&#58; 3, maxValue&#58; 10},<p>          legend&#58; 'none',<p>          trendlines&#58; { 0&#58; {} }<p>        };<p>        var chart = new google.visualization.ScatterChart(document.getElementById('chart_div'));<p>        chart.draw(data, options);<p>      }<p>    </script><p>  </head><p>  <body><p>    <div id=chart_div style=width&#58; 900px; height&#58; 500px;></div><p>  </body><p></html>
  did_you_create_a_user_interface_for_the_end_users_to_see_the_aggregated_results: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_user_interface_for_the_end_user: https://github.com/kumquatexpress/Venn/blob/master/screenshots/results_page.png
  describe_what_your_end_user_sees_in_this_interface: The suggestions come up in a table form sorted in descending total score. The scores are generated through cosine similarity and a weighted average from the user's answers and from the answers of other users about particular relationships.
  what_is_the_scale_of_the_problem_that_you_are_trying_to_solve_: If we limited our scope to this problem within the Penn community, the problem affects over 10,000 students. If we expanded our scope to other colleges, our pulled in users from surrounding geographical areas, Venn could address this problem for a much larger group of people. Every can use more friends! It’s a question of whether or not they would participate in a matching service for more.
  if_it_would_benefit_from_a_huge_crowd_how_would_it_benefit_: The quality of data would improve in two ways, stemming from the two types of questions Venn users answer. First, since they answer questions about whether two users are a good match, there will be a good chance of more than 10 users evaluating whether a pair of users in a good match. As it is, we really can’t trust the results when these matches are only being evaluated by 1, sometimes 2, people. Second, an improvement comes from more users answering questions about themselves. As soon as they do, they are eligible to be matched with someone. More users means more potential matches for any given users. A wider match pool will translate to higher quality of matches (rather than picking the “best fit” match for one user out of a pool of 17, when it might not be a good fit at all), and more matches. This redundancy means Venn is likely to output some successful matches, even if some matches don’t pan out.
  what_challenges_would_scaling_to_a_large_crowd_introduce: Having more users is always a challenge, and Venn will need several add&#45;ons to make the interface work at this scale. First, we’ll need to continuously collect data on the success of our matches. To avoid the issue of presenting a user with too many matches (a result of so many users to be matched to), we’ll need to train another machine learning component to evaluate these matches, based on the success of previous matches. More users can also mean more hard&#45;to&#45;manage trolls! We’ll need to create a system for discarding data the people give to sabotage Venn. This should be easy for questions where the trolls are evaluating the match between two other users, because we can evaluate the match in parallel and compare our expected answer to theirs. There is nothing we can do for the questions they will incorrectly answer about themselves, however.
  did_you_perform_an_analysis_about_how_to_scale_up_your_project: No
  what_analysis_did_you_perform_on_the_scaling_up: 
  do_you_have_a_google_graph_analyzing_scaling_: No
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  url_to_the_flow_diagram_for_your_project: https://cloud.githubusercontent.com/assets/6036502/5081387/1603658a&#45;6e9d&#45;11e4&#45;8606&#45;475dd126b3f9.png
  if_the_crowd_was_real_how_did_you_recruit_participants: Our participants were our NETS213 classmates. They were found through Piazza, class dinner, etc.
  did_your_project_work: Our project successfully created an interface that aggregated personalized match suggestions between different users. We were able to incorporate machine learning methods and cosine similarity (document&#45;based evaluation) by representing the profiles of each user as a feature vector. Ultimately, the goal was to produce an approximation of the likelihood of two users being friends, and Venn represents this as a single number on the scale of 0 to 100, satisfying the goal of being both minimalistic and expressive.<p>Do you have a Google graph analyzing your project?<p>
  do_you_have_a_google_graph_analyzing_your_project_: Yes
  if_you_have_a_graph_analyzing_your_project_include_the_html_for_a_google_graph_here: <html><p>  <head><p>    <script type=text/javascript src=https://www.google.com/jsapi></script><p>    <script type=text/javascript><p>      google.load(visualization, 1, {packages&#58;[corechart]});<p>      google.setOnLoadCallback(drawChart);<p>      function drawChart() {<p>        var data = google.visualization.arrayToDataTable([<p>          ['Type', 'Total', 'User Average'],<p>          ['Suggestions',  247,      12],<p>          ['Profile Answered', 286,     15],<p>          ['Relationship Answered', 53, 4]<p>        ]);<p>        var options = {<p>          title&#58; 'Venn Performance',<p>          vAxis&#58; {title&#58; 'Feature',  titleTextStyle&#58; {color&#58; 'blue'}}<p>        };<p>        var chart = new google.visualization.BarChart(document.getElementById('chart_div'));<p>        chart.draw(data, options);<p>      }<p>    </script><p>  </head><p>  <body><p>    <div id=chart_div style=width&#58; 900px; height&#58; 500px;></div><p>  </body><p></html>
  caption_for_your_google_graph_(project_analysis): Venn Performance Statistics
  what_were_the_biggest_challenges_that_you_had_to_deal_with: Given our limited timespan, we had two big challenges&#58; One was getting a large enough user base such that we could aggregate enough data for a useful demonstration. The second was working towards the larger vision that we had originally had for the project. We had to scale down this vision in several respects, as described in the next question.<p>
  where_there_major_changes_between_what_you_originally_proposed_and_your_final_product: Yes
  if_so_what_changed_between_your_original_plan_and_your_final_product: Our original plan involved further Facebook integration such that Venn could pull basic profile information (education, hometown, friends, hobbies, likes) from Facebook and pre&#45;populate each user profile. In addition, we also hoped to integrate built&#45;in follow&#45;up mechanisms such that suggested matches could directly message each other through Venn and also be given suggested meet&#45;up locations, things to do, etc., and that people could track the success of their matches.
  what_are_some_limitations_of_your_product: There are definitely a number of limitations that come in hand with scaling up our application, most of which we’ve already mentioned previously. While we won’t have issues with cost, a large issue could be the creation of new seed users. Since Venn is so focused on the idea of expanding social circles, it could be very difficult for new users with no previously established social circles to successfully enter the application (e.g., someone moving to a new country and hoping to make friends there).  
  did_your_results_deviate_from_what_you_would_expect_from_previous_work_or_what_you_learned_in_the_class: No
  if_your_results_deviated_why_might_that_be: 
  caption_for_your_graph_(scaling_up): 
  caption_for_your_graph_(aggregation): Profile Similarity vs. Crowdsourced Relationship Opinion
  caption_for_your_graph_(quality): 
  caption_for_your_graph_(skills): 
  caption_for_your_graph_(incentives): 
  is_there_anything_else_youd_like_to_say_about_your_project: 
  did_you_train_a_machine_learning_component_from_what_the_crowd_gave_you: Yes
  if_you_trained_a_machine_learning_component_describe_what_you_did: We train a boosted decision tree on the difference matrix between the answers to pairs of profile questions.  Our training set is an example set of data provided by groups of people who are already friends, and our testing set is the set of users on Venn. The classifier itself uses an online method, so new data is constantly being retrained on. We can then take the output of the classifier tree and compare it against the majority vote of users for a particular relationship as a method of quality control.
  did_you_analyze_the_quality_of_the_machine_learning_component: No
  if_you_have_a_graph_analyzing_a_machine_learning_component_include_the_html_for_a_google_graph_here: 
  caption_for_your_graph_(incentives): 
  caption_for_your_graph_(machine_learning_component): 
  what_was_the_main_focus_of_your_teams_effort: Something in between
  provide_a_link_to_your_final_presentation_video: 
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_skills_: 
  what_incentivi: 
-
  how_many_teammates_are_in_your_group: 4
  name_1: Dhrupad Bhardwaj
  name_2: Sally Kong
  name_3: Jing Ran
  name_4: Amy Le
  pennkey_username_1: dhrupadb
  pennkey_username_2: kongjih
  pennkey_username_3: jran
  pennkey_username_4: pleamy
  name_of_your_project: Shoptimum
  give_a_one_sentence_description_of_your_project: Shoptimum is a crowdsourced fashion portal to get links to cheaper alternatives for celebrity clothes and accessories.
  url_to_the_logo_for_your_project: https://raw.githubusercontent.com/jran/Shoptimum/gh&#45;pages/shoptimum&#37;20logo.png
  what_similar_projects_exist: CopyCatChic &#45; It's a similar concept which provides cheaper alternatives to interior decoration and furniture. However it doesn't crowdsource results or showcase items and has a team of contributors post blogs about the items.<p>Polyvore &#45; It uses a crowdsourced platform to curate diverse products into a compatible ensemble in terms of decor, accessories or styling choices. However it doesn't cater to the particular agenda of finding more cost effective alternatives to existing fashion trends.
  what_type_of_project_is_it: A tool for crowdsourcing, A business idea that uses crowdsourcing
  how_do_you_incentivize_the_crowd_to_participate: The  ultimate objective of this application is to become a one stop fashion portal implicitly absorbing current trends by crowdsourcing which items would look best on the user. We plan to structure the incentive program as so&#58;<p>1. Each user who uses Shoptimum gets points for contributing to the application. Different actions have different amounts of points associated with them. For example, if the user is to submit images to be tagged and for which links are to be generated, that would involve between 5&#45;10 points based on the popularity of the image. If the user submits links for an image and tags it, based on the number of votes the user's submissions cumulatively receive that would involve a point score between 20 &#45; 100 points. If the user submitted a tag which ends up on the final highest rated combination (after a threshold of course), that would give the user a bump of 100 points for each item. Lastly, voting for the best alternative also gets you points based on how many people agree with you. As we don't show the vote counts, the vote is unbiased. Eg&#58; If you click the option that most people agree with, you get 30 points, else you get 15 points for contributing. <p>2. These points aim to translate into a system to rank users based on contributions and use frequency of the system. Should the application go live as a business model, we would tie up companies such as Macy's , Amazon, Forever 21 etc. and offer people extra points for listing their items as alternatives versus just any company. If you collect enough points, you would be eligible to receive vouchers or discounts at these stores thus incentivizing you to participate. 
  what_does_the_crowd_provide_for_you: The crowd is a very very integral part of our application. Literally every step relies on the crowd to be functional. Firstly, the initial images which need to be tagged and cheaper alternatives found are submitted by the crowd. We would of course step in by adding pictures should there be a lack of crowd submissions but it's still the crowd's submissions which matter first. Secondly, and most importantly, the crowd is the one who submits links for each of celebrity/model picture, finding cheaper versions of the same clothing items / accessories as found in the picture. The crowd goes through e&#45;commerce e&#45;fashion portals to find who is selling a similar product and links us to them. The crowd also provides basic tags on attributes related to each item which we in turn use for analytics on what kind of styles are currently trending. Lastly, the voting system is reliant entirely on the crowd who votes to choose which of the submitted alternatives is the best / most cost efficient / most similar to the product submitted in the picture. The application takes all this data and then populates a list of highest voted alternatives and it also provides some analytics to the kinds of input generated.
  how_do_you_ensure_the_quality_of_the_crowd_provides__: Step 3 in our process deals with QC &#58; The voting<p>The idea is that we ask the crowd for cheaper fashion alternatives, and then ensure the crowd is the one who selects which alternative is the closest to what the original is like. On the voting page, we show the original image side by side with other submitted alternatives. The idea being that people can compare in real time which of the alternatives is most fitting and then vote for that. The aggregation step collects these votes and accordingly maintains a table of the items which are the highest voted. By the law of large numbers, we can approximate that the crowd is almost always right and thus this is an effective QC method as an alternative which isn't satisfactory is likely not to get voted and thus would not show up in the final results. <p>For now we keep the process fairly democratic allowing each user to vote once and that vote would count as one vote only. The idea would eventually be that should users get experience and should the collect enough points via voting for the alternative that's always selected by the crowd then we could possibly modify the algorithm to a weighted vote system to give their vote more leverage. However this does present a care of abuse of power and it would require more research to fully determine which QC aggregation method is more effective. Regardless the crowd here does the QC for us. <p>How do we know that they are right? The final results page shows all the alternatives which were given the highest votes by the crowd and we can see that they're in face pretty close to what is worn by the individual in the original picture. A dip in usage would be a good indicator that people feel our matches are not accurate, thus telling us that the QC step has gone wrong. That said, again quoting the law of large numbers &#58; that's unlikely because on average the crowd is always right. 
  how_do_you_aggregate_the_results_from_the_crowd: Aggregation takes place at two steps in the process. Firstly, when people submit links for cheaper alternatives to items displayed in the picture, all these links are collected in a table and associated with a count which is basically the vote that particular alternative received. We keep a track of all the alternatives and randomly display 4 of them to be voted on in the next step where users can pick which alternative is the closest match with the original image. We small modification in the script could be that the highest voted alternative is always shows to make sure that if it's indeed the best match then everyone get's to decide . Next we aggregate the votes from the crowd incrementing the vote every time someone votes for a particular alternative. Based on the count this alternative shows up in the final results page as the best alternative for the item. 
  how_does_your_project_work: So our project is heavily crowd focussed. <p>Members of the crowds are allowed to post pictures of models and celebrities and descriptions about what they're wearing on a particular page.<p>After that, members of the crowd are allowed to go and look at posted pictures and then post links of those specific pieces of clothing available on commercial retail sites such as Amazon or Macy's etc.<p>On the same page, members of the crowd are allowed to post attributes such as color, material or other attributes about those pieces of clothing for analytics purposes.<p>In the third step, the crowd can go and see one of the posted pictures and compare the original piece of clothing to those cheaper alternatives suggested by the crowd. Members of the crowd can vote for their strongest match at this stage.<p>In the last stage, each posted picture is shown with a list of items which the crowd has deemed the best match.
  would_your_project_benefit_if_you_could_get_contributions_from_thousands_of_people: Yes
  vimeo_link: https://vimeo.com/114581689
  pennkey_username_1: 
  may_we_have_your_permission_to_feature_your_project_on_the_class_website: Yes
  team_member_1__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_2__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_3__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_4__can_we_list_your_name_listed_alongside_your_project: Yes
  who_are_the_members_of_your_crowd: Anyone and everyone interested in fashion !
  how_many_unique_participants_did_you_have: 10
  for_your_final_project_did_you_simulate_the_crowd_or_run_a_real_experiment: Simulated crowd
  if_the_crowd_was_simulated_how_did_you_collect_this_set_of_data: Given that it was finals week, we didn't have too many people willing to take the time out to find and contribute by submitting links and pictures. To add the data we needed we basically simulated the crowd among the project members and a few friends who were willing to help out. We each submitted pictures, added links and pictures, rated the best alternatives etc. Our code aggregated the data and populated it for us. 
  if_the_crowd_was_simulated_how_would_you_change_things_to_use_a_real_crowd: The main change we would incorporate would be the incentive program. We focussed our efforts on the actual functionality of the application. That said, the idea would be to give people incentives such as points for submitting links which are frequently viewed / submitting alternatives which were highly upvoted. These points could translate into discounts or coupons with retail websites such as Amazon or Macy's as a viable business model
  how_do_you_ensure_the_quality_of_the_crowd_provides: 
  what_motivation_does_the_crowd_have_for_participating_in_your_project: Pay, Enjoyment, Reputation
  did_you_perform_any_analysis_comparing_different_incentives_: Yes
  if_you_compared_different_incentives_what_analysis_did_you_perform_: We didn't perform an explicit analysis which proved that this was the best incentive structure, that said, most publicly available research indicates that crowdsourced work done by users personally invested in the outcome and the topic associated with the outcome tend to give far superior results than a motivated simply by monetary incentives. Relying, not just on interest and dedication alone, we extend the concept to add an implicit monetary benefit  via our point system which people should be excited to use as the whole objective of this is to get access to cheaper alternatives.
  if_you_have_a_graph_analyzing_incentives_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_incentives_: 
  do_your_crowd_workers_need_specialized_skills: No
  what_sort_of_skills_do_they_need: The users don't need any specialized skills for participating. We'd prefer they had a generally sound sense of fashion and don't upvote clearly un&#45;similar or rather unattractive alternatives. A specific skill it may benefit users to have is an understanding of materials and types of clothes. If they were good at identifying this a search query for cheaper alternatives would be much more specific and thus likely to be easier. (E.g.&#58; Searching for Burgundy knit lambswool full &#45; sleeve women's cardigan  vs  Maroon sweater women 
  if_skills_vary_widely_what_factors_cause_one_person_to_be_better_than_another_: As we keep this open to everyone, skills will vary. Of course because majority of the people on the app are fashion savvy or conscious we expect most of them to be able to be of relatively acceptable skill level. As mentioned, fashion sense and ability to identify clothing attributes would be a big plus when searching for alternatives.
  did_you_analyze_the_skills_of_the_crowd: No
  if_you_analyzed_skills_what_analysis_did_you_perform: 
  do_you_have_a_google_graph_analyzing_skills_: No
  if_you_have_a_graph_analyzing_skills_include_the_html_for_a_google_graph_here: 
  is_the_quality_of_what_the_crowd_gives_you_a_concern: Yes
  did_you_create_a_user_interface_for_the_crowd_workers: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_crowdfacing_user_interface: https://github.com/jran/Shoptimum/blob/master/ScreenShots.pdf
  describe_your_crowdfacing_user_interface: Each of the 7 screen shots has an associated caption starting from the top left going down in read wise order<p>1&#58; Home screen the user sees on reaching the application. Also has the list of options of tasks the user can perform on Shoptimum<p>2&#58; Submit Links &#58; The User can submit a link to a picture of a celebrity or model they want tags for so that they can emulate their style on a budget<p>3&#58; Getting the links &#58; Users can submit links to cheap alternatives on  e&#45;commerce websites and an associated link of the picture of the item as well <p>4&#58; Users can also submit description tags about the items &#58; eg&#58; Color<p>5&#58; Users can then vote on which of the alternatives is closest to the item in the original celebrity/ model picture. The votings are aggregated via simple majority<p>6&#58; A page to display the final ensemble of highest voted alternatives<p>7&#58; A page to view the analytics of what kinds of attributes about the products is currently trending. 
  did_you_analyze_the_aggregated_results_: Yes
  what_analysis_did_you_perform_on_the_aggregated_results: 
  did_you_analyze_the_quality_of_what_you_got_back: Yes
  what_analysis_did_you_perform_on_the_aggregated_results: One thing we did analyze is that for each of our clothing items that we were finding alternatives for, what color was generally the trend. The idea is simple but we plan to extend it to material and other attributes to that we can get an idea about at any given point of time what is in fashion and what is trending. This is displayed on a separate tab with pie charts for each clothing item to get an idea of who's wearing what and what the majority of posts say people are looking to wear. <p>Conclusions are hard to draw given that we had a simulated crowd. But it would be interesting to see what we get should our crowd increase to a large set of people and of course across seasons as well
  what_analysis_did_you_perform_on_quality: This was a fairly obvious comparison as the final step in our process involves populating results which display the original image and the alternative that people from the crowd chose as the best matching alternatives of all those submitted. We can see that in most of the cases the alternatives come quite close to the original products in the image. it's hard in fashion especially to find the exact same product unless it's very generic, but the idea is that you can find something pretty close to get the same overall fashion ensemble. 
  do_you_have_a_google_graph_analyzing_quality_: No
  if_you_have_a_graph_analyzing_quality_include_the_html_for_a_google_graph_here: 
  is_this_something_that_could_be_automated: Yes
  if_it_could_be_automated_say_how__if_it_is_difficult_or_impossible_to_automate_say_why: We considered automating a couple steps in the process, <p>1. Picture submissions &#58; We could crawl fashion magazines and find pictures of celebrities in their latest outfits to get an idea of fashion trends and have people tag links for those. However we felt that allowing people to also submit their own pictures was an important piece of the puzzle.<p>2. Getting links to cheaper alternatives &#58; This would definitely be the hardest part. It would have involved us getting instead of links, specific tags about each of the items such as color, material etc and using that data to make queries to various e&#45;fashion e&#45;commerce portals and get the search results. Then we would probably use a clustering algorithm to  try and match each picture with the specific item from the submitted image and accordingly post those results which the clustering algorithm deems similar. The crowd would then vote on the best alternatives. <p>Sadly, given the variety of styles out there and the relative complexity of image matching algorithms where the images may be differently angled, shadowed etc, it would mean a large ML component would have to be built it. It also restricted the sources for products whereas the crowd would be more versatile at finding alternatives from any possible source that can be found via a search engine. This step is definitely very difficult using ML, but not impossible. Perhaps a way to make it work would be to monitor usage and build a set of true matches and then train a clustering algorithm to use this labeled image matching data to generate a classifier better suited for the job in the future. It was definitely much simpler to have the crowd submit results.
  do_the_skills_of_individual_workers_vary_widely: Yes
  do_you_have_a_google_graph_analyzing_the_aggregated_results_: Yes
  if_you_have_a_graph_analyzing_the_aggregated_results_include_the_html_for_a_google_graph_here: http://jran.github.io/Shoptimum/trending.html
  did_you_create_a_user_interface_for_the_end_users_to_see_the_aggregated_results: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_user_interface_for_the_end_user: https://github.com/jran/Shoptimum/blob/master/ScreenShots.pdf<p>(The last screenshot shows this)
  describe_what_your_end_user_sees_in_this_interface: The user sees pie charts generated via the Google API showing which colors are currently trending on the items being tagged on Shoptimum
  what_is_the_scale_of_the_problem_that_you_are_trying_to_solve_: It's hard to judge the scale of the problem numerically. That said, there is a growing upper middle class population across the world who would greatly benefit from having more affordable alternatives to chic fashion worn by celebrities and models and they would greatly benefit from this service. It would also help people interested in fashion incubate interest and get an idea of how fashion trends are changing over time.
  if_it_would_benefit_from_a_huge_crowd_how_would_it_benefit_: A large crowd means a lot of votes and hopefully a lot of alternatives as submissions which would benefit everyone as a lot of choice would give the crowd a solid set of alternatives to choose from when looking at more affordable alternatives. It would also probably give people an idea of fashion trends across regions and countries and possibly ethnicities making for a very interesting social experiment.  
  what_challenges_would_scaling_to_a_large_crowd_introduce: The scripts the program use are fairly simple and thus highly scalable. The only problem to having a large user base would be data storage but this would only become a problem when the number of people is of the order of 10's of Millions. The biggest problem I forsee is a sort of data flow issue where a large number of people would be submitting pictures to be tagged or voting for alternatives but not submitting links to actual alternatives available. In this case the problem can be taken care of by using the user point system thus incentivising people to submit links as well as voting.
  did_you_perform_an_analysis_about_how_to_scale_up_your_project: No
  what_analysis_did_you_perform_on_the_scaling_up: 
  do_you_have_a_google_graph_analyzing_scaling_: No
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  url_to_the_flow_diagram_for_your_project: https://github.com/jran/Shoptimum/blob/master/flowchart.pdf
  if_the_crowd_was_real_how_did_you_recruit_participants: 
  did_your_project_work: I think it did yes!<p>In terms of functionality we managed to get all the core components up and running. We created a clean and seamless interface for the crowd to enter data and vote on this data to get a compiled list of search results. Additionally we also set up the structure analyze more data if needed and add features based on viability + user demand. <p>The project was able to showcase the fact that the crowd was an effective tool in giving us the results that we needed and that the concept that we were trying to achieve is in fact possible in the very form we envisioned it. We saw that for most of the pictures that were posted we found good alternatives that were fairly cost effective and given the frequency of pulls from sites such as Macy's, Amazon or Nordstrom we actually could partner with these companies in the future should the application user base get big enough. 
  do_you_have_a_google_graph_analyzing_your_project_: No
  if_you_have_a_graph_analyzing_your_project_include_the_html_for_a_google_graph_here: 
  caption_for_your_google_graph_(project_analysis): 
  what_were_the_biggest_challenges_that_you_had_to_deal_with: I think the biggest challenge was and is getting people to post alternatives. Everyone is super interested in getting information they need. They're also equally interested in voicing an opinion. That said, people aren't often willing to help out if it involves serious work. The idea will be that should this become commercial we would make the adding alternatives process easier by doing an initial query on regular websites and then having the crowd vet those as viable alternatives which would then be publicly voted on. We could also increase the incentive structure such that anyone actually submitting good credible links would receive a good amount of points which would translate into credit / gift cards / discounts on partner company websites on specific items etc.
  where_there_major_changes_between_what_you_originally_proposed_and_your_final_product: No
  if_so_what_changed_between_your_original_plan_and_your_final_product: I think the product is fairly consistent with what we proposed during the project proposal and status update. Yes the project involves a lot of crowdsourcing which is significantly more than we originally had in mind. That said we realized that it didn't make sense to have a ML algorithm slave away when the crowd would in fact by a much more useful and tactile tool to perform the same tasks. The idea is the eventually be able to gather enough data that we could consider training a machine learning algorithm to perform better image analysis for us that way again providing the crowd with options to pick from rather than forcing them to do heavy searching work themselves.
  what_are_some_limitations_of_your_product: At this point the biggest limitation is that the product relies very very heavily on the crowd. Not only that, different steps in the process are less and more tedious and the challenge will be to make sure information flow is consistent across the steps as a bottleneck or a lack of information at a particular stage would severely limit the ability of other stages of the app to give satisfactory results. The plan is to structure the incentive program to make sure that less attractive stages in the process are made more lucrative by higher rewards. Eventually we can also try and automate some of the more tedious tasks in the process so that users are more likely to contribute.
  did_your_results_deviate_from_what_you_would_expect_from_previous_work_or_what_you_learned_in_the_class: No
  if_your_results_deviated_why_might_that_be: 
  caption_for_your_graph_(scaling_up): 
  caption_for_your_graph_(aggregation): The page has 5 graphs each with it's own caption
  caption_for_your_graph_(quality): 
  caption_for_your_graph_(skills): 
  caption_for_your_graph_(incentives): 
  is_there_anything_else_youd_like_to_say_about_your_project: 
  did_you_train_a_machine_learning_component_from_what_the_crowd_gave_you: No
  if_you_trained_a_machine_learning_component_describe_what_you_did: 
  did_you_analyze_the_quality_of_the_machine_learning_component: No
  if_you_have_a_graph_analyzing_a_machine_learning_component_include_the_html_for_a_google_graph_here: 
  caption_for_your_graph_(incentives): 
  caption_for_your_graph_(machine_learning_component): 
  what_was_the_main_focus_of_your_teams_effort: Engineering a complex system
  provide_a_link_to_your_final_presentation_video: 
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_skills_: 
  what_incentivi: 
-
  how_many_teammates_are_in_your_group: 4
  name_1: Sean Sheffer
  name_2: Devesh Dayal
  name_3: Sierra Yit
  name_4: Kate Miller
  pennkey_username_1: sheffers
  pennkey_username_2: deveshd
  pennkey_username_3: sierray
  pennkey_username_4: katmill
  name_of_your_project: Critic Critic
  give_a_one_sentence_description_of_your_project: Critic Critic uses crowdsourcing to measure and analyze media bias.
  url_to_the_logo_for_your_project: https://github.com/kate16/criticcritic/blob/master/CriticCriticLogo.png
  what_similar_projects_exist: Miss Representation is a documentary that discusses how men and women are portrayed differently in politics, but it draws on a lot of anecdotal evidence, and does not discuss political parties, age, or race.<p>Satirical comedy shows &#45; notably John Stewart from the Daily Show and Last Week today with John Oliver and the Colbert report slice media coverage from various sources and identify when bias is present. 
  what_type_of_project_is_it: Social science experiment with the crowd
  how_do_you_incentivize_the_crowd_to_participate: The project had different trial runs to incentivize the crowd &#45; first we payed them 3 cents to provide a url and three adjectives. This led to a very low response rate and an average user satisfaction of 3.4/5 for pay and 3.3/5 for difficulty of the task. This led to a rate of response of 1 judgment a day.<p>Therefore &#45; we upped the pay to 10 cents for the job and the responses increased to 5 a day with increased satisfaction of pay to 4.2/5 and difficulty to 4/5. The increased responses were after the pay incentive.
  what_does_the_crowd_provide_for_you: The crowd provides a url to a website, news article, or blog and three adjectives that were used to describe the politician.
  how_do_you_ensure_the_quality_of_the_crowd_provides__: We knew from previous HIT assignments that they would be completed very fast if QC wasn't taken into account. Usuall non&#45;results (fields that were left blank, with whitespace or periods, were put in the fields from countries in Latin America or India). Therefore &#45; we made each question required, and for the url field we made the validator a url link (rejection for empty fields). For the adjectives we limited the fields to only letters. We limited the workers only to US 
  how_do_you_aggregate_the_results_from_the_crowd: We had a large list of adjectives that were generated from CSVs for all the candidates, and therefor we inputted the words fields to generate 5 wordclouds that would show the size of the words scaled by the weights of which they were repeated. 
  how_does_your_project_work: The first step in our project is to generate a list of urls that link to news articles/blogs/websites that cover the politicians. In order to reasonably generate content within the project scope timeline &#45; we selected politicians who represented specific target demographics. In this case we wanted politicians from all backgrounds &#45; white male, white female, african american male, Hispanic female and Hispanic male. Respectively, we selected candidates in high position that would generate high media coverage&#58; President Obama, Speaker of the House John Boehner, Hilary Clinton, Justice Sotomayor, and Senator Marco Rubio.<p>The crowdworkers were tasked with finding a piece of media coverage &#45; url/blog/news article, and identifying three adjectives that were used to describe the candidates. After the content was generated from the crowdworkers &#45; we analyzed the data by using the weight of the descriptors. Visually appealing for presenting the data was a Word Cloud &#45; therefore for each candidate the word clouds were generated. <p>The next step is analyzing the descriptors per the candidates &#45; by looking at which words had the highest weights to confirm/deny biases in the representation of the candidates.
  would_your_project_benefit_if_you_could_get_contributions_from_thousands_of_people: Yes
  vimeo_link: http://vimeo.com/114452242
  pennkey_username_1: 
  may_we_have_your_permission_to_feature_your_project_on_the_class_website: Yes
  team_member_1__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_2__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_3__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_4__can_we_list_your_name_listed_alongside_your_project: Yes
  who_are_the_members_of_your_crowd: Americans in the United States (for US media coverage)
  how_many_unique_participants_did_you_have: 456
  for_your_final_project_did_you_simulate_the_crowd_or_run_a_real_experiment: Real crowd
  if_the_crowd_was_simulated_how_did_you_collect_this_set_of_data: 
  if_the_crowd_was_simulated_how_would_you_change_things_to_use_a_real_crowd: 
  how_do_you_ensure_the_quality_of_the_crowd_provides: 
  what_motivation_does_the_crowd_have_for_participating_in_your_project: Pay
  did_you_perform_any_analysis_comparing_different_incentives_: Yes
  if_you_compared_different_incentives_what_analysis_did_you_perform_: First &#45; we suspected there was external motivation to answering the HITs per candidate even when the pay being the same. Since certain politicians were more popular than others despite pay being the same, we can argue there was some external motivation &#45; as shown by the graph&#58; External Motivation in giving responses. Obama, Clinton, Boehner, Sotomayor, and Rubio recieved total &#45; 37*3 + 44*3 + 37*3 + 16*3 + 18*3 = 456 responses even though the jobs were launched the same and pay was held equal even at 3 cents (later upped to 10 cents).<p>We looked at the user satisfactions for the jobs and rated ease of job &#45; across the 5 different HITS (one new job for each candidate). At 3 cents the user satisfaction ranged from 3.3&#45;3.5/5 for satisfaction and ease was 3.3/5. After upping the pay to 10 cents the rating increased to 4.2/5 for satisfaction and 4/5 for ease of the job.<p>Also, the rate of responses increased from 1 a day to average of 5 a day per the 5 launched jobs.
  if_you_have_a_graph_analyzing_incentives_include_the_html_for_a_google_graph_here: http://htmlpreview.github.io/?https://github.com/kate16/criticcritic/blob/master/NETS213&#37;20incentivization
  do_you_have_a_google_graph_analyzing_incentives_: 
  do_your_crowd_workers_need_specialized_skills: Yes
  what_sort_of_skills_do_they_need: Speak English, know enough English syntax to identify adjectives.
  if_skills_vary_widely_what_factors_cause_one_person_to_be_better_than_another_: They need to identify which words are adjectives for the candidate versus strictly descriptors. (For example &#45; for Marco Rubio wanting to gain support for the Latino Vote, the word 'Latino' is not an adjective describing Rubio, but rather the vote &#45; therefore this is not bias in his descriptor).
  did_you_analyze_the_skills_of_the_crowd: Yes
  if_you_analyzed_skills_what_analysis_did_you_perform: We opened up the links to their articles in the CSVs, and searched for the adjectives that they produced were in the article. We also looked at the rate of judgments per hour &#45; and saw if any of the responses were rejected because the input was less than 10 seconds (ie the crowdworker was not looking for adjectives used in the article). We looked at the quality of the results by looking at the CSVs and seeing if any of the users repeated adjectives (trying to game the system) and opening up the links to see if they were broken or not. We reached the conclusion that paying the workers more had the judgements per hour increase, the satisfaction and even the ease of the job increase in rating. For adjectives &#45; because of the simplicity of the task even though workers could repeat adjectives we looked at the results and there were very few repeated adjectives per user response. Those that put non&#45;legible letters were taken out of the word clouds.
  do_you_have_a_google_graph_analyzing_skills_: Yes
  if_you_have_a_graph_analyzing_skills_include_the_html_for_a_google_graph_here: http://htmlpreview.github.io/?https://github.com/kate16/criticcritic/blob/master/Graph&#37;20Analysis/skill_analysis.html
  is_the_quality_of_what_the_crowd_gives_you_a_concern: Yes
  did_you_create_a_user_interface_for_the_crowd_workers: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_crowdfacing_user_interface: https://github.com/kate16/criticcritic/blob/master/actualHITscreenshot.jpg
  describe_your_crowdfacing_user_interface: It asks for for the url &#45; and provides an article that the group decided is an adequate example article that the user may look at. Also &#45; our interface provides the workers with three adjectives that were from the example article, so there is no confusion to what the expectations of the work is. Lastly, there are three fields for adjectives 1, 2, and 3 and a field for the url.
  did_you_analyze_the_aggregated_results_: Yes
  what_analysis_did_you_perform_on_the_aggregated_results: 
  did_you_analyze_the_quality_of_what_you_got_back: Yes
  what_analysis_did_you_perform_on_the_aggregated_results: We analyzed the words by looking at the descriptors and finding the recurring themes of the word associations. Also we looked to weed out duplicate adjectives that were aggregated by all the forms of media.
  what_analysis_did_you_perform_on_quality: We looked at the ip locations of the users who answered to see if they were actually from cities in the US, and made a graph distribution to see if they were indeed in the US. We opened the links generated from the CSV files to see if they were actual articles and that they were not broken. Also in the list of CSVs we looked to see if they indeed were adjectives &#45; if there were consecutive repeats from the same user (which we did not include in the word cloud). We determined that because of the limitation to the US the results/judgements came in slower &#45; but the websites were indeed articles and urls to blogs, were actually about the candidates and the adjectives were present. Although at first we were skeptical that the crowd would produce reliable results &#45; the strict QC we implemented this time allowed for true user data we could use for our project. 
  do_you_have_a_google_graph_analyzing_quality_: Yes
  if_you_have_a_graph_analyzing_quality_include_the_html_for_a_google_graph_here: http://htmlpreview.github.io/?https://github.com/kate16/criticcritic/blob/master/Graph&#37;20Analysis/qualityworkerlocationsgraph.html
  is_this_something_that_could_be_automated: Yes
  if_it_could_be_automated_say_how__if_it_is_difficult_or_impossible_to_automate_say_why: It is difficult to generate because at first we used crawlers to generate the links but it produces a lot of broken links &#45; and we wanted an adequate sample size from all sources of media (the entire body of media coverage) instead of say, the New York times opinion section. Also &#45; to automate the selection of adjectives used we'd need to create a program that had the entirely list of English adjectives used in the human language &#45; run the string of words and produce matches to extract the adjectives. 
  do_the_skills_of_individual_workers_vary_widely: Yes
  do_you_have_a_google_graph_analyzing_the_aggregated_results_: Yes
  if_you_have_a_graph_analyzing_the_aggregated_results_include_the_html_for_a_google_graph_here: http://htmlpreview.github.io/?https://github.com/kate16/criticcritic/blob/master/Graph&#37;20Analysis/skill_analysis.html
  did_you_create_a_user_interface_for_the_end_users_to_see_the_aggregated_results: No
  if_yes_please_give_the_url_to_a_screenshot_of_the_user_interface_for_the_end_user: 
  describe_what_your_end_user_sees_in_this_interface: 
  what_is_the_scale_of_the_problem_that_you_are_trying_to_solve_: The scale of the problem is the entirety of national media coverage and attention. This would include every single news  article/ channel/ video that talks about a politician and generating a list of words that are used, weighing them and analyzing them for the overarching themes and trends to generate a portrayal. This would include hundreds of thousands of articles generated each day and thousands of video/news media generated each day.
  if_it_would_benefit_from_a_huge_crowd_how_would_it_benefit_: It would benefit from a huge crowd by creating large sample size of overall media coverage &#45; therfore workers could pull in videos from youtube, blogs, new articles from New York times, Fox, twitter handles and feeds &#45; with more crowd we have a larger pull and representation of the broader spectrum that is media. And with more adjectives and language that is used gnerated we can weigh the words used to see if indeed there is different portrayals of candidates.
  what_challenges_would_scaling_to_a_large_crowd_introduce: There would be duplicated sources and urls (which could be deduped like in one of the homeworks) but there would be a huge difficulty in ensuring that urls are not broken &#45; that they are actual articles and that the adjectives are actually in the articles portrayed. A representation in media can be any url or even link to an article therefore the verification of this aspect can again crowdsourced to ask&#58; is this url a representation of a politician, and are the adjectives given actually in the article itself. 
  did_you_perform_an_analysis_about_how_to_scale_up_your_project: No
  what_analysis_did_you_perform_on_the_scaling_up: How much would it cost to pull and analyze 10,000 articles &#45; which would be 10 cents &#45; to $1,000, per each candidate. Expanding this to only 10 politicians would be $10,000 &#45; therefore if we wanted fuller demographics &#45; a wide spectrum of say, 100 candidates this would be $100,000! This is a very expensive task and scaling up would need to be in a way that is automated. 
  do_you_have_a_google_graph_analyzing_scaling_: No
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  url_to_the_flow_diagram_for_your_project: https://github.com/kate16/criticcritic/blob/master/flowchart.pdf
  if_the_crowd_was_real_how_did_you_recruit_participants: We had to limit the target countries to only the United States because we wanted a measurement of the American media coverage. They had to speak English &#45; and as we wanted various sources we limited the amount of responses to 5 judgement and a limit of 5 ip addresses. Anyone who could identify an article on the coverage of a candidate and have literacy to identify adjectives were part of the crowd.
  did_your_project_work: From the wordclouds some of the interesting results&#58;<p>Word associations for Obama&#58; Muslim, communist, monarch<p>Word associations for Hilary Clinton&#58; Smart Inauthentic Lesbian.<p>John Boehner's word cloud did not contain any words pertaining to his emotional overtures (unlike the democratic candidates). Sonia Sotomayor and Rubio's cloud had positive word connotations in their representation. <p>Overarching trends&#58; Republicans more likely to be viewed as a unit, characterized by their positive with high weights being conservative. Democrats more characterized by strong emotions &#45; passionate, angry. Per gender&#58; men appeared to be viewed more as straightforward and honest. Women charaterized as calculated and ambitions, perhaps because seeking political power is atypical for the gender.'Cowardly' was more likely to describe men perhaps because of similarly gender pressures. All politicians were described as 'Angry' at some point. We were pleased to find while ageism does exist &#45; it applied to everyone once they reach a certain age and not targeted at certain candidates. 
  do_you_have_a_google_graph_analyzing_your_project_: No
  if_you_have_a_graph_analyzing_your_project_include_the_html_for_a_google_graph_here: 
  caption_for_your_google_graph_(project_analysis): 
  what_were_the_biggest_challenges_that_you_had_to_deal_with: The biggest challenges we had to deal with were finding the adequate source to represent the media and articles for political figures. We tried to use a web crawler at first &#45; but this generated many broken links and articles that were not strong portrayals of candidates. Also we encountered problems with scale &#45; we needed to direct the results that the crowd workers would produce to measure all media bias &#45; but for a reasonable time frame we limited it to five candidates from different backgrounds. With a higher scale &#45; we could produce results for candidates not only at the national stage and name presence, but rather per pure demographic &#45; female, hispanic, democrat of a certain age range. This would be an expanded feature. Also we wanted to search the articles for adjectives at first &#45; but realized that this song was different than gun articles that specific keywords couldn't be pulled as we are looking at the entirety of adjectives. 
  where_there_major_changes_between_what_you_originally_proposed_and_your_final_product: No
  if_so_what_changed_between_your_original_plan_and_your_final_product: The concept of using word clouds to analyze a slice of media portrayal of candidates is the same &#45; it just had to be scaled down instead of all politicians to certain politicians for the timeframe of the project and to make the directions clearer for the HITs. 
  what_are_some_limitations_of_your_product: Some limitations include measuring media portrayal in terms of adjectives used to describe the candidate. Subjectively &#45; portrayal can include a persons overall tone &#45; the manner in which they discuss the candidate and the context of the article. The adjectives could have been an excerpt from another article or negative examples where the article itself is examining media portrayal. Also our project was limited to text from articles &#45; but another dimension is videos on the nightly news from anchors or videos in youtube videos. In this case the audio would need to be transcribed or crowdworkers could pull the adjectives used after watching a video. We took a slice of media representation based on text of articles where media portrayal is a medium itself. 
  did_your_results_deviate_from_what_you_would_expect_from_previous_work_or_what_you_learned_in_the_class: No
  if_your_results_deviated_why_might_that_be: We knew that we would get non&#45;words/non adjectives from users who would game the system &#45; but after our QC measure and carefully making the HITs, although the results were not as fast they were better than the results from previous projects. Results could have deviated from the media in the spectrum of time &#45; for example during an election versus coverage of a veto &#45; this could explain why the candidates could be 'angry' (ie Obama pushing immigration reform on a still congress) versus their actual portrayal. 
  caption_for_your_graph_(scaling_up): 
  caption_for_your_graph_(aggregation): Types of adjectives used in aggregate results
  caption_for_your_graph_(quality): Locations of the Workers
  caption_for_your_graph_(skills): Types of Responses from Crowdworkers
  caption_for_your_graph_(incentives): External Motivation in responses per politician (With equal pay some politicians still received more responses than others)
  is_there_anything_else_youd_like_to_say_about_your_project: Here is the link to the github with all our project files &#45; Thank you so much for the learning experience this project was fun and one of our favorite classes at Penn.<p>https://github.com/kate16/criticcritic
  did_you_train_a_machine_learning_component_from_what_the_crowd_gave_you: No
  if_you_trained_a_machine_learning_component_describe_what_you_did: 
  did_you_analyze_the_quality_of_the_machine_learning_component: 
  if_you_have_a_graph_analyzing_a_machine_learning_component_include_the_html_for_a_google_graph_here: 
  caption_for_your_graph_(incentives): 
  caption_for_your_graph_(machine_learning_component): 
  what_was_the_main_focus_of_your_teams_effort: Conducting an in depth analysis of data
  provide_a_link_to_your_final_presentation_video: 
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_skills_: 
  what_incentivi: 
-
  how_many_teammates_are_in_your_group: 2
  name_1: Adam Baitch
  name_2: Jared Rodman
  name_3: 
  name_4: 
  pennkey_username_1: abaitch
  pennkey_username_2: jrodman
  pennkey_username_3: 
  pennkey_username_4: 
  name_of_your_project: CrowdFF
  give_a_one_sentence_description_of_your_project: CrowdFF compares users' starting lineup choices to Yahoo's projections for optimal starting lineups, and collected data to determine whether or not users' backgrounds and efforts made a difference in outperforming the algorithms.
  url_to_the_logo_for_your_project: https://github.com/jdrodman/crowdff/blob/master/docs/CrowdFF_Logo.png
  what_similar_projects_exist: There has been research into the accuracy of ESPN's Fantasy Rankings. Here it is&#58; http://regressing.deadspin.com/how&#45;accurate&#45;are&#45;espns&#45;fantasy&#45;football&#45;projections&#45;1669439884
  what_type_of_project_is_it: Social science experiment with the crowd
  how_do_you_incentivize_the_crowd_to_participate: We explained the project to people, and sparked their interest in learning about their own results. We promised anyone who asked that we would share with them how well they fared in comparison to Yahoo's projections. We followed through with this promise.
  what_does_the_crowd_provide_for_you: The crowd provides us with two things &#45; authorization to access their roster data through the Yahoo API and some basic info about their fantasy&#45;playing habits collected via a google form.
  how_do_you_ensure_the_quality_of_the_crowd_provides__: The crowd provides the data from their Fantasy teams, which by definition or our project cannot have quality less than perfect. The quality of their choices is what we aimed to measure.
  how_do_you_aggregate_the_results_from_the_crowd: For each crowdworker, the surface data pulled includesd, for each of 13 weeks&#58; <p>                &#45; who the current players on the roster were at the time, <p>                &#45; how many points each player was projected to score (this was pulled separately by parsing the html of ESPN’s weekly projection page since Yahoo does not expose its own projections) <p>                &#45; how many points each player actually scored, <p>                &#45; which subset of players the crowdworker picked as his/her starting lineup<p>        In addition, a follow&#45;up survey was filled out by each user  <p>                <p>                Given the projected points and actual points each week, players were ranked by their associated positions according to both metrics to construct a best&#45;projected starting lineup and a true&#45;optimal retrospective starting lineup.  The lineup chosen by the crowd&#45;worker and the best&#45;projected lineup are compared to the optimal lineup, such that the accuracy of a given lineup is computed as the fraction of the the optimal lineups also chosen by users and projections.  These user the projected accrues are aggregated across all 13 weeks and averaged to produce and final average user accuracy and average projection accuracy for each user (accuracies for users with multiple teams were averaged to produced a single user score).  Finally, survey data was correlated with roster data via a unique identifier. 
  how_does_your_project_work: We asked our users for their email address. Based on the email address, we gave them a unique link to sign into Yahoo's Fantasy Football API. They signed in, agreed to share their FF data with us, and sent us back a 6 digit access key that would allow us to pull their roster, lineup, projections, and post&#45;facto player score data from each week of the season. Based on this, the user was given an accuracy score and the projections they had seen over the course of the season were also given an accuracy score. The user then completed a survey in which they provided us with background information on their habits, strategies in fantasy football, and personal characteristics. <p>After we collected all of the data, we analyzed it for patterns and correlations between player habits and characteristics, and the differential between their accuracy and the accuracy of Yahoo's suggestions for them.
  would_your_project_benefit_if_you_could_get_contributions_from_thousands_of_people: Yes
  vimeo_link: https://vimeo.com/114559930
  pennkey_username_1: 
  may_we_have_your_permission_to_feature_your_project_on_the_class_website: Yes
  team_member_1__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_2__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_3__can_we_list_your_name_listed_alongside_your_project: No
  team_member_4__can_we_list_your_name_listed_alongside_your_project: No
  who_are_the_members_of_your_crowd: College Students who play Fantasy Football
  how_many_unique_participants_did_you_have: 33
  for_your_final_project_did_you_simulate_the_crowd_or_run_a_real_experiment: Real crowd
  if_the_crowd_was_simulated_how_did_you_collect_this_set_of_data: N/A
  if_the_crowd_was_simulated_how_would_you_change_things_to_use_a_real_crowd: N/A
  how_do_you_ensure_the_quality_of_the_crowd_provides: 
  what_motivation_does_the_crowd_have_for_participating_in_your_project: Altruism, Feedback on their performance
  did_you_perform_any_analysis_comparing_different_incentives_: No
  if_you_compared_different_incentives_what_analysis_did_you_perform_: N/A
  if_you_have_a_graph_analyzing_incentives_include_the_html_for_a_google_graph_here: N/A
  do_you_have_a_google_graph_analyzing_incentives_: 
  do_your_crowd_workers_need_specialized_skills: Yes
  what_sort_of_skills_do_they_need: They need to play Fantasy Football on Yahoo as opposed to another Fantasy site. That's it.
  if_skills_vary_widely_what_factors_cause_one_person_to_be_better_than_another_: We determined that certain users who spent more time setting their  Fantasy Football lineups were more successful than those who didn't, but only to a point. We believe this is because there is an element of randomness involved in the game, so the marginal benefit of spending more time on it past ~3 hours diminished drastically.
  did_you_analyze_the_skills_of_the_crowd: Yes
  if_you_analyzed_skills_what_analysis_did_you_perform: We analyzed Fantasy Football users' abilities to choose the optimal starting lineups based on their rosters.
  do_you_have_a_google_graph_analyzing_skills_: No
  if_you_have_a_graph_analyzing_skills_include_the_html_for_a_google_graph_here: N/A
  is_the_quality_of_what_the_crowd_gives_you_a_concern: No
  did_you_create_a_user_interface_for_the_crowd_workers: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_crowdfacing_user_interface: jdrodman/crowdff/blob/master/docs/yahoo_authorization.png<p>jdrodman/crowdff/blob/master/docs/yahoo_approval.png<p>jdrodman/crowdff/blob/master/docs/questionnaire.png<p>
  describe_your_crowdfacing_user_interface: The UI consists of two parts&#58; crowd authorization and a follow&#45;up questionnaire.<p>	Crowd authorization was done by sending crowd members a custom link produced by running a script locally.  This link brought users to Yahoo’s app authorization&#45;request page customized for CrowdFF (yahoo_authorization.png). Following hitting agree, users are then brought to a page which shows a code of numbers and letter (yahoo_approval.png) &#45; crowd workers are asked to send this code back to us so that we can complete the process of obtaining authorization.  Finally, crowd workers who have provided us authorization are asked to fill out a follow&#45;up survey (quationnaire.png) 
  did_you_analyze_the_aggregated_results_: Yes
  what_analysis_did_you_perform_on_the_aggregated_results: 
  did_you_analyze_the_quality_of_what_you_got_back: No
  what_analysis_did_you_perform_on_the_aggregated_results: The main question we looked at was considering the distribution of accuracy differentials (average user accuracy &#45; average projection accuracy) to determine if there is a significant difference between the average user and projection accuracies.  The histogram below shows that while most users are about evenly matched with projections, only a small fraction of individuals consistently outperform projections.   This would make sense since outperforming projections is likely do to more research or (more likely) just getting lucky.  On the other hand, it is a safer bet to just pick lineups based on projections which explains the spike in near&#45;zero differentials and the very few number of people who actually do worse than baseline projections.
  what_analysis_did_you_perform_on_quality: We were not concerned about poor quality, because our entire project was based around determining the quality of users versus Yahoo's projections.
  do_you_have_a_google_graph_analyzing_quality_: No
  if_you_have_a_graph_analyzing_quality_include_the_html_for_a_google_graph_here: N/A
  is_this_something_that_could_be_automated: Yes
  if_it_could_be_automated_say_how__if_it_is_difficult_or_impossible_to_automate_say_why: The system is already automated to an extent in that users don’t need to explicitly delineate who is on their roster week by week &#45; this can all be pulled given their authorization from the Yahoo API.  Ideally, If the system were to be completely automated, we would be able to collect the authorization and survey data together via a HIT &#45; but this requires advanced knowledge of creating a custom HIT for each crowd member (since each authorization would require posting a custom url and running a script that begins an API request session for every input).  We were limited in this respect due to time and lack of expertise and therefore needed to collect authorization data one by one, individually.    
  do_the_skills_of_individual_workers_vary_widely: Yes
  do_you_have_a_google_graph_analyzing_the_aggregated_results_: Yes
  if_you_have_a_graph_analyzing_the_aggregated_results_include_the_html_for_a_google_graph_here: <html><p>  <head><p>    <script type=text/javascript src=https://www.google.com/jsapi></script><p>    <script type=text/javascript><p>      google.load(visualization, 1.1, {packages&#58;[bar]});<p>      google.setOnLoadCallback(drawChart);<p>      function drawChart() {<p>        var data = google.visualization.arrayToDataTable([<p>          ['Percent Differential', '',<p>           { role&#58; 'annotation' } ],<p>          ['<&#45;1&#37;', 1, ''],<p>          ['&#45;1&#37; to 0&#37;', 14, ''],<p>          ['0&#37; to 1&#37;', 12, ''],<p>          ['1&#37; to 2&#37;', 4, ''],<p>          ['2&#37; to 3&#37;', 2, ''],<p>          ['3&#37; to 4&#37;', 3, ''],<p>          ['4&#37; to 5&#37;', 0, ''],<p>          ['>5&#37;', 2, ''],<p>        ]);<p>        var options = {<p>          width&#58; 900,<p>          height&#58; 400,<p>          legend&#58; { position&#58; 'top', maxLines&#58; 6 },<p>          bar&#58; { groupWidth&#58; '75&#37;' },<p>          isStacked&#58; true,<p>          series&#58; {<p>            0&#58; { axis&#58; 'numbers' }, <p>            },<p>          axes&#58; {<p>            <p>            y&#58; {<p>              numbers&#58; {label&#58; '# of Players'}, // Left y&#45;axis.<p>            }<p>          },<p>        };<p>        var chart = new google.charts.Bar(document.getElementById('columnchart_material'));<p>        chart.draw(data, options);<p>      }<p>    </script><p>  </head><p>  <body><p>    <div id=columnchart_material style=width&#58; 900px; height&#58; 500px;></div><p>  </body><p></html>
  did_you_create_a_user_interface_for_the_end_users_to_see_the_aggregated_results: No
  if_yes_please_give_the_url_to_a_screenshot_of_the_user_interface_for_the_end_user: N/A
  describe_what_your_end_user_sees_in_this_interface: N/A
  what_is_the_scale_of_the_problem_that_you_are_trying_to_solve_: Its estimated that 33 million people play fanstasy football every year, so the problem could hypothetical scale to that size if we want to make truly accurate conclusions (though obviously unlikely) &#45; it could include all players who play in any type of fantasy league (Yahoo, ESPN, CBS, NFL) 
  if_it_would_benefit_from_a_huge_crowd_how_would_it_benefit_: All the conclusions we draw are limited by our sample size of crowd workers and the lack of diversity among the crowd, so with a larger crowd we would be able to draw stronger conclusions.  In addition, if in the future we decide to construct and fantasy&#45;playing AI, having a larger base of crowd data which ti train the system would make it much more accurate.  
  what_challenges_would_scaling_to_a_large_crowd_introduce: The main challenges we would face are the same challenges that really limited out ability to collect more data.  The first is the ability to create a HIT that could contain the custom authorization information, though with some additional practice with MTurk this could probably be overcome.  The second is the necessity to pull projected points from by parsing the html of ESPN’s projection pages &#45; this greatly slowed down the runtime of our data&#45;pulling script (about 2 minutes per team) and could be a big problem given a much larger set of user data.  Finally, being able to incorporate different API and league types could take significant work.   Also, we did not have to incentivize users to provide us with data using payments (as they were all colleagues and friends), but we would need to pay the larger set of crowdworkers, introducing a sizable cost.   
  did_you_perform_an_analysis_about_how_to_scale_up_your_project: Yes
  what_analysis_did_you_perform_on_the_scaling_up: N/A
  do_you_have_a_google_graph_analyzing_scaling_: Yes
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: N/A
  url_to_the_flow_diagram_for_your_project: jdrodman/crowdff/blob/master/docs/Work&#37;20Flow&#37;20Diagram.pdf
  if_the_crowd_was_real_how_did_you_recruit_participants: We sent emails to listservs we are on as well as asked our friends peronally to participate.
  did_your_project_work: Though the crowd size was a bit small to make conclusions with 100&#37; confidence, we were able to draw some important conclusions.   In addition to our aggregation analysis, we used the the survey data to look at certain factors that might make a fantasy owner better or worse than projections  &#45; mainly the number of research tools they use outside of Yahoo, and the average time spent per week choosing their lineup.  Graphing against average user differential shows that there is little correlation between the number of tools used. The only correlation seemed to be that those who used some tools to make decisions outperformed those who didnt. On the other hand, there does seem to be some sort of direct positive relation between differential and hours spent &#45; up until 4+ hours spent, at which time performance actually decreases.  <p>This has led us to the overall convulsion that while effort (i.e. time spent researching) can help boost performance to an extent (although it certainly does not guarantee it), the payoff is still minimal.  Even a +5&#37; accuracy differential over the course of a season is only corresponds to about 5 better picks than projections in total out of more than 100 &#45; which likely does not necessarily translate into any more additional wins in a given season.  If the point spending time setting lineups is to give yourself an advantage over other players in your league, it is probably more worth your time your time to just pick according to projections.  That will hopefully make people more focused and productive, especially in the workplace.       
  do_you_have_a_google_graph_analyzing_your_project_: Yes
  if_you_have_a_graph_analyzing_your_project_include_the_html_for_a_google_graph_here: html><p>  <head><p>    <script type=text/javascript src=https://www.google.com/jsapi></script><p>    <script type=text/javascript><p>      google.load(visualization, 1.1, {packages&#58;[bar]});<p>      google.setOnLoadCallback(drawChart);<p>      function drawChart() {<p>        var data = google.visualization.arrayToDataTable([<p>          ['# Sports Information Services Used', '',<p>           { role&#58; 'annotation' } ],<p>          ['1', 0.73, ''],<p>          ['2', 5.05, ''],<p>          ['3', 0.22, ''],<p>          ['4', 1.19, ''],<p>          ['5+', 0.77, ''],<p>        ]);<p>        var options = {<p>          width&#58; 600,<p>          height&#58; 400,<p>          legend&#58; { position&#58; 'top', maxLines&#58; 6 },<p>          bar&#58; { groupWidth&#58; '75&#37;' },<p>          isStacked&#58; true,<p>          series&#58; {<p>            0&#58; { axis&#58; 'numbers' }, <p>            },<p>          axes&#58; {<p>            <p>            y&#58; {<p>              numbers&#58; {label&#58; 'Average &#37; Differential'}, // Left y&#45;axis.<p>            }<p>          },<p>        };<p>        var chart = new google.charts.Bar(document.getElementById('columnchart_material'));<p>        chart.draw(data, options);<p>      }<p>    </script><p>  </head><p>  <body><p>    <div id=columnchart_material style=width&#58; 900px; height&#58; 500px;></div><p>  </body><p></html><p><p><html><p>  <head><p>    <script type=text/javascript src=https://www.google.com/jsapi></script><p>    <script type=text/javascript><p>      google.load(visualization, 1.1, {packages&#58;[bar]});<p>      google.setOnLoadCallback(drawChart);<p>      function drawChart() {<p>        var data = google.visualization.arrayToDataTable([<p>          ['# Hours/wk Spent Setting Lineup', '',<p>           { role&#58; 'annotation' } ],<p>          ['1', 0.86, ''],<p>          ['2', 1.49, ''],<p>          ['3', 3.34, ''],<p>          ['4+', &#45;0.85, ''],<p>        ]);<p>        var options = {<p>          title&#58; Hello,<p>          width&#58; 600,<p>          height&#58; 400,<p>          bar&#58; { groupWidth&#58; '75&#37;' },<p>          isStacked&#58; true,<p>          series&#58; {<p>            0&#58; { axis&#58; 'numbers' }, <p>            },<p>          axes&#58; {<p>            <p>            y&#58; {<p>              numbers&#58; {label&#58; 'Average &#37; Differential'}, // Left y&#45;axis.<p>            }<p>          },<p>        };<p>        var chart = new google.charts.Bar(document.getElementById('columnchart_material'));<p>        chart.draw(data, options);<p>      }<p>    </script><p>  </head><p>  <body><p>    <div id=columnchart_material style=width&#58; 900px; height&#58; 500px;></div><p>  </body><p></html>
  caption_for_your_google_graph_(project_analysis): Users vs Projections Differentials based on # information sources used
  what_were_the_biggest_challenges_that_you_had_to_deal_with: One of the biggest challenges we faced was dealing with the Yahoo API.  While it helped us pull roster data atomatically from users, building a data collection system around their specific authorization mechanism and having to parse separate ESPN html pages to obtain player projections (since Yahoo decided not to expose their own projections for some reason) took significantly more engineering effort and time than we would have hoped, and limited the the number of users we could obtain data from.     
  where_there_major_changes_between_what_you_originally_proposed_and_your_final_product: Yes
  if_so_what_changed_between_your_original_plan_and_your_final_product: Our original plan changed pretty drastically, from out original plan. Initially we set out to make a housing&#45;swap app that had absolutely nothing to do with Fantasy Football. After readng that our system did not do crowdsource enough, we came across the idea establishing a service whereby users could request help from crowd workers in setting their weekly lineup.  Eventually, we realized that with fanstasy seasons finding down, we would at best obtain 1 week of data which be didn’t believe would be all that interesting.  Finally, we felt that using crowd as a source of fantasy data was a better idea that would allow us to use all 13 weeks of data and obtain more interesting results.
  what_are_some_limitations_of_your_product: As discussed above,  the main limitation of our project was that data had to be collected individually without the use of a widespread HIT.  This drastically reduced out sample data size, reducing the robustness of our results.  Even if we were able to accumulate mote users, however, the need too pull projected points by parsing individual html pages reduced the performance of our aggregation system as well &#45; ideally Yahoo would have exposed their project points via their API. 
  did_your_results_deviate_from_what_you_would_expect_from_previous_work_or_what_you_learned_in_the_class: No
  if_your_results_deviated_why_might_that_be: The results mostly did not differ from what we expected, but likely differ from the general actions of a percentage of fantasy owners when it comes to lineup setting.  We found that spending more effort to set ones lineups could potentially help with lineup accuracy, but the ultimate payout is likely minimal.  If more fantasy owners knew this, they might be more inclined to just pick lineups according to projected values in the future and save their employers lots of money!
  caption_for_your_graph_(scaling_up): N/A
  caption_for_your_graph_(aggregation): Distribution of	User’s and Yahoo’s Projection Differentials
  caption_for_your_graph_(quality): N/A
  caption_for_your_graph_(skills): N/A
  caption_for_your_graph_(incentives): N/A
  is_there_anything_else_youd_like_to_say_about_your_project: 
  did_you_train_a_machine_learning_component_from_what_the_crowd_gave_you: No
  if_you_trained_a_machine_learning_component_describe_what_you_did: Our system does not currently include training a machine learning component but we could imagine using crowd roster data in the the future to train a fantasy playing AI machine that is better than mere projections.   
  did_you_analyze_the_quality_of_the_machine_learning_component: No
  if_you_have_a_graph_analyzing_a_machine_learning_component_include_the_html_for_a_google_graph_here: N/A
  caption_for_your_graph_(incentives): 
  caption_for_your_graph_(machine_learning_component): N/A
  what_was_the_main_focus_of_your_teams_effort: Conducting an in depth analysis of data
  provide_a_link_to_your_final_presentation_video: 
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_skills_: 
  what_incentivi: 
-
  how_many_teammates_are_in_your_group: 3
  name_1: Clara Wu
  name_2: Dilip Rajan
  name_3: Dennis Sell
  name_4: 
  pennkey_username_1: scwu
  pennkey_username_2: drajan
  pennkey_username_3: selld
  pennkey_username_4: 
  name_of_your_project: RapGenii
  give_a_one_sentence_description_of_your_project: RapGenii&#58; Rap lyric creation for the masses
  url_to_the_logo_for_your_project: https://github.com/scwu/rapgenii/blob/master/docs/RapGenii_logo.png
  what_similar_projects_exist: Rap Genius (http://rap.genius.com/) does analysis of rap lyrics, but does perform rap lyric generation<p>Rap Pad (http://rappad.co/) allows aspiring rappers to create their own lyrics and share with others
  what_type_of_project_is_it: Human computation algorithm
  how_do_you_incentivize_the_crowd_to_participate: First, writing random rap lyrics is something that several of us personally enjoy. We think that users can be to quite a large extent be incentivized merely because they will enjoy it too. This is especially the case for voting on suggestions as the effort needed is minimal, and people generally like to express their opinions in this way.<p>Also, we set up a points system, which we refer to as a user's Rap God Score. 1 point is added for a suggestion, and 10 points are added for a suggestion which is then added to the rap. All in all, this system closely resembles the incentivization behind reddit and various other websites.
  what_does_the_crowd_provide_for_you: The crowd ends up providing quality rap lyrics with negligible marginal cost to us. 
  how_do_you_ensure_the_quality_of_the_crowd_provides__: We ensure quality control by having other users vote on which lines are the best. There is clearly no way to automate determining the quality of raps.<p>When a threshold of votes have been reached on any particular suggestion, we decide on the best suggestion. We do this by calculating the Wilson's score of each line using the upvotes and downvotes it has received. <p>Determines the rap lines with the best rating using a Wilson's score with 85&#37; certainty. Note that Wilson's score has many benefits over simpler scoring methods such as ratio or difference of upvotes and downvotes.
  how_do_you_aggregate_the_results_from_the_crowd: Aggregation is very simple and merely consists of collecting. Users make suggestions or vote on things, and we simply aggregate them in a list, or by adding the votes up.<p>Additionally, once a particular line suggestion gets to a particular votes threshold, we choose the best line among the suggestions and add it to the rap. We leave all of the other lines there as suggestions. The only time at which we remove lines is when a rap gets to a certain number of downvotes.
  how_does_your_project_work: Most of the work being done will be performed by the crowd. Users can create raps or contribute to raps by suggesting lines. Additionally, users can upvote or downvote raps.<p>The program will automatically remove any suggestions once they reach a low score threshold and it will choose the best line using a metric known as wilson's score when a suggestion reaches a particular number of votes.
  would_your_project_benefit_if_you_could_get_contributions_from_thousands_of_people: Yes
  vimeo_link: https://vimeo.com/114591145
  pennkey_username_1: 
  may_we_have_your_permission_to_feature_your_project_on_the_class_website: Yes
  team_member_1__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_2__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_3__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_4__can_we_list_your_name_listed_alongside_your_project: 
  who_are_the_members_of_your_crowd: internet denizens who enjoy being creative, and are looking for a way to have fun
  how_many_unique_participants_did_you_have: 65
  for_your_final_project_did_you_simulate_the_crowd_or_run_a_real_experiment: Real crowd
  if_the_crowd_was_simulated_how_did_you_collect_this_set_of_data: 
  if_the_crowd_was_simulated_how_would_you_change_things_to_use_a_real_crowd: 
  how_do_you_ensure_the_quality_of_the_crowd_provides: 
  what_motivation_does_the_crowd_have_for_participating_in_your_project: Enjoyment, Reputation
  did_you_perform_any_analysis_comparing_different_incentives_: No
  if_you_compared_different_incentives_what_analysis_did_you_perform_: 
  if_you_have_a_graph_analyzing_incentives_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_incentives_: 
  do_your_crowd_workers_need_specialized_skills: No
  what_sort_of_skills_do_they_need: They simply need a basic amount of lyrical creativity (and a facebook account, at least currently)
  if_skills_vary_widely_what_factors_cause_one_person_to_be_better_than_another_: We do not believe that skills vary greatly, though further analysis would need to be done. In any case, unskilled contributors would ideally not be able to harm the quality of the raps created, thanks to the voting system. 
  did_you_analyze_the_skills_of_the_crowd: No
  if_you_analyzed_skills_what_analysis_did_you_perform: 
  do_you_have_a_google_graph_analyzing_skills_: 
  if_you_have_a_graph_analyzing_skills_include_the_html_for_a_google_graph_here: 
  is_the_quality_of_what_the_crowd_gives_you_a_concern: Yes
  did_you_create_a_user_interface_for_the_crowd_workers: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_crowdfacing_user_interface: https://github.com/scwu/rapgenii/blob/master/docs/unfinished_rap.png
  describe_your_crowdfacing_user_interface: On the left, a user can see the rap in progress&#58; the title, the currently added lines, etc. On the right, a user can suggest new lines, or vote on the other suggestions.
  did_you_analyze_the_aggregated_results_: Yes
  what_analysis_did_you_perform_on_the_aggregated_results: 
  did_you_analyze_the_quality_of_what_you_got_back: No
  what_analysis_did_you_perform_on_the_aggregated_results: We simply looked at how contributions made on the site. Our data for line suggestions was somewhat non&#45;uniform due to a change we made, so we show the number of votes different users have made. 
  what_analysis_did_you_perform_on_quality: 
  do_you_have_a_google_graph_analyzing_quality_: 
  if_you_have_a_graph_analyzing_quality_include_the_html_for_a_google_graph_here: 
  is_this_something_that_could_be_automated: No
  if_it_could_be_automated_say_how__if_it_is_difficult_or_impossible_to_automate_say_why: It can't
  do_the_skills_of_individual_workers_vary_widely: No
  do_you_have_a_google_graph_analyzing_the_aggregated_results_: Yes
  if_you_have_a_graph_analyzing_the_aggregated_results_include_the_html_for_a_google_graph_here: <html><p>  <head><p>    <script type=text/javascript src=https://www.google.com/jsapi></script><p>    <script type=text/javascript><p>      google.load(visualization, 1, {packages&#58;[corechart]});<p>      google.setOnLoadCallback(drawChart);<p>      function drawChart() {<p>        var data = google.visualization.arrayToDataTable([<p>            ['votes'],<p>            [1], [21], [1], [7], [1], [1], [11], [4], [2], [24], [1], [2], [4], [6], [2], [9], [1], [1], [3], [18], [23], [3], [2], [1]<p>            ]);<p>        var options = {<p>          title&#58; 'Votes by Users',<p>          legend&#58; { position&#58; 'none' },<p>        };<p>        var chart = new google.visualization.Histogram(document.getElementById('chart_div'));<p>        chart.draw(data, options);<p>      }<p>    </script><p>  </head><p>  <body><p>    <div id=chart_div style=width&#58; 900px; height&#58; 500px;></div><p>  </body><p></html>
  did_you_create_a_user_interface_for_the_end_users_to_see_the_aggregated_results: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_user_interface_for_the_end_user: We show users the finished rap. <p>https://github.com/scwu/rapgenii/blob/master/docs/finished_rap.png
  describe_what_your_end_user_sees_in_this_interface: Users can also see the finished lyrics in the tap. Additionally, they can see who suggested each line and what score it received by hovering over any particular line.
  what_is_the_scale_of_the_problem_that_you_are_trying_to_solve_: The problem is of a questionable scale. The problem is mainly that people did not have this method of spending their free time before, so the scale of the problem is merely how many people would enjoy this.
  if_it_would_benefit_from_a_huge_crowd_how_would_it_benefit_: The quality and speed of production would be better with such a large crowd. The site can be boring at times because there is simply not enough traffic.
  what_challenges_would_scaling_to_a_large_crowd_introduce: Not much. We would simply need to optimize some of our backend code to be able to handle the strain. It would require no additional human effort.
  did_you_perform_an_analysis_about_how_to_scale_up_your_project: No
  what_analysis_did_you_perform_on_the_scaling_up: 
  do_you_have_a_google_graph_analyzing_scaling_: 
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  url_to_the_flow_diagram_for_your_project: https://github.com/scwu/rapgenii/blob/master/docs/flow&#37;20diagram.pdf
  if_the_crowd_was_real_how_did_you_recruit_participants: We posted about our website in various places online&#58; Facebook, Facebook groups, twitter, hacker news, etc.<p>We also asked many friends specifically.
  did_your_project_work: We believe that the project was a success overall. A few really interesting raps were created, quite a few people really enjoyed the site and kept coming back to contribute, as one can see in the charts that we have concluded. The charts show the distribution of voting counts and rap god points among those users who signed in and participated on the site.<p>Roughly 40&#37; of users who logged in voted and 30&#37; contributed.<p>
  do_you_have_a_google_graph_analyzing_your_project_: Yes
  if_you_have_a_graph_analyzing_your_project_include_the_html_for_a_google_graph_here: <html><p>  <head><p>    <script type=text/javascript src=https://www.google.com/jsapi></script><p>    <script type=text/javascript><p>      google.load(visualization, 1, {packages&#58;[corechart]});<p>      google.setOnLoadCallback(drawChart);<p>      function drawChart() {<p>        var data = google.visualization.arrayToDataTable([<p>            ['rapGodPoints'],<p>            [91], [4], [11], [4], [51], [13], [2], [16], [10], [47], [4], [2], [2], [31], [52], [41], [16], [18]<p>            ]);<p>        var options = {<p>          title&#58; 'RapGodPoints Earned',<p>          legend&#58; { position&#58; 'none' },<p>        };<p>        var chart = new google.visualization.Histogram(document.getElementById('chart_div'));<p>        chart.draw(data, options);<p>      }<p>    </script><p>  </head><p>  <body><p>    <div id=chart_div style=width&#58; 900px; height&#58; 500px;></div><p>  </body><p></html>
  caption_for_your_google_graph_(project_analysis): Rap God Points by user
  what_were_the_biggest_challenges_that_you_had_to_deal_with: A lot of work went into making a proper front end. <p>It also took us a while to decide exactly how to choose the best suggestion and when. We considered enforcing a requirement to get to a certain number of upvotes, but the upvote/downvote system along with a Wilson score seemed more appropriate. Then, the question was when do we decide on a best line? We decided to utilize and upvote&#45;downvote requirement to decide when to do this.
  where_there_major_changes_between_what_you_originally_proposed_and_your_final_product: Yes
  if_so_what_changed_between_your_original_plan_and_your_final_product: I'm not sure if this is major, but we largely replanned when lines are added on the raps. We originally planned on having this done after every period of time. However, we decided this would not bode well with irregular traffic patterns. Hence, we decided to utilize voting counts to roughly decide when to choose the best suggestion. (We still use Wilson's score to decide which line to add, however). 
  what_are_some_limitations_of_your_product: Our product is limited in that it doesn't have a clear way to generate revenue (other than the obvious method of advertising). Still, if we were to get this to scale, the money coming in from ads would hopefully be enough to handle the minor costs of infrastructure.
  did_your_results_deviate_from_what_you_would_expect_from_previous_work_or_what_you_learned_in_the_class: No
  if_your_results_deviated_why_might_that_be: 
  caption_for_your_graph_(scaling_up): 
  caption_for_your_graph_(aggregation): Votes by Users
  caption_for_your_graph_(quality): 
  caption_for_your_graph_(skills): 
  caption_for_your_graph_(incentives): 
  is_there_anything_else_youd_like_to_say_about_your_project: Additionally, the founder of RapGenius and Ben Horowitz of Andreesen&#45;Horowitz responded positively to our tweets linking to the site.
  did_you_train_a_machine_learning_component_from_what_the_crowd_gave_you: No
  if_you_trained_a_machine_learning_component_describe_what_you_did: 
  did_you_analyze_the_quality_of_the_machine_learning_component: 
  if_you_have_a_graph_analyzing_a_machine_learning_component_include_the_html_for_a_google_graph_here: 
  caption_for_your_graph_(incentives): 
  caption_for_your_graph_(machine_learning_component): 
  what_was_the_main_focus_of_your_teams_effort: Engineering a complex system
  provide_a_link_to_your_final_presentation_video: 
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_skills_: 
  what_incentivi: 
-
  how_many_teammates_are_in_your_group: 4
  name_1: Noah Shpak
  name_2: Luke Carlson
  name_3: Conner Swords
  name_4: Eli Brockett
  pennkey_username_1: noashpak
  pennkey_username_2: carjack
  pennkey_username_3: cswords
  pennkey_username_4: elibrock
  name_of_your_project: PReTweet
  give_a_one_sentence_description_of_your_project: PReTweet is an application that uses crowdsourcing to determine how audiences will respond to a potential tweet. 
  url_to_the_logo_for_your_project: https://github.com/jLukeC/PReTweet/blob/master/images/PreTweet.tif
  what_similar_projects_exist: None that we could find.
  what_type_of_project_is_it: A business idea that uses crowdsourcing
  how_do_you_incentivize_the_crowd_to_participate: To incentivize the crowd, we paid them on Crowdflower.  Also, we put in some time making the HIT easy to navigate and its instructions clear.  When we tested the HIT, the workers raked it overall 4.5/5 in terms of clarity, ease of job, and fairness. 
  what_does_the_crowd_provide_for_you: The crowd provides us with three metrics&#58; a tweet's appropriateness, humor level, and grammatical precision.  Each of these values range from one to five and the tweets average score is sent to the user.
  how_do_you_ensure_the_quality_of_the_crowd_provides__: Test questions will be included in the HIT when requests are are submitted to Crowdflower.  To ensure that an inappropriate tweet isn't labeled as appropriate and that the workers are performing honestly, we will add an offensive or inappropriate tweet to gauge worker performance&#58; if the worker doesn't answer correctly, his judgement will be ignored.
  how_do_you_aggregate_the_results_from_the_crowd: We used a simple algorithm that averages the results of the workers for each specific tweet.  These averages are the scores that we report to the user as the final step.
  how_does_your_project_work: First, a user texts a potential tweet to our Twilio number, which is parsed by our Python script and uploaded as a HIT on Crowdflower immediately.  When the HIT is completed by the crowd, our script grabs the results, aggregates them into scores, and texts them back to the user using Twilio.  
  would_your_project_benefit_if_you_could_get_contributions_from_thousands_of_people: Yes
  vimeo_link: https://vimeo.com/114585270
  pennkey_username_1: 
  may_we_have_your_permission_to_feature_your_project_on_the_class_website: Yes
  team_member_1__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_2__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_3__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_4__can_we_list_your_name_listed_alongside_your_project: Yes
  who_are_the_members_of_your_crowd: Crowdflower Workers
  how_many_unique_participants_did_you_have: 47
  for_your_final_project_did_you_simulate_the_crowd_or_run_a_real_experiment: Real crowd
  if_the_crowd_was_simulated_how_did_you_collect_this_set_of_data: N/A
  if_the_crowd_was_simulated_how_would_you_change_things_to_use_a_real_crowd: N/A
  how_do_you_ensure_the_quality_of_the_crowd_provides: 
  what_motivation_does_the_crowd_have_for_participating_in_your_project: Pay
  did_you_perform_any_analysis_comparing_different_incentives_: Yes
  if_you_compared_different_incentives_what_analysis_did_you_perform_: We compared the time it took when the workers were offered varying amounts of money for analyzing the tweets.  With 10 test HITs, our data showed that the change from 2&#45;3 cents per HIT to 6 cents per hit cut the latency period in half (from ~20 minutes to ~10).  The number of judgements, when in a range of 3&#45;5, didn't substantially affect the wait time or the crowd's opinion of a specific tweet.
  if_you_have_a_graph_analyzing_incentives_include_the_html_for_a_google_graph_here: <html><p>  <head><p>    <script type=text/javascript src=https://www.google.com/jsapi></script><p>    <script type=text/javascript><p>      google.load(visualization, 1, {packages&#58;[corechart]});<p>      google.setOnLoadCallback(drawChart);<p>      function drawChart() {<p>        var data = google.visualization.arrayToDataTable([<p>          ['Title', 'Judgments', 'Pay (Cents)', 'Time (Minutes)'],<p>          ['', 5,  6, 10],<p>          ['', 3,  6, 11],<p>          ['', 5,  6, 11],<p>          ['', 3, 4, 15],<p>          ['', 5, 4, 16],<p>          ['', 3, 2, 17],<p>          ['', 3,  2, 18],<p>          ['', 3, 3, 18],<p>          ['', 5, 2, 19],<p>          ['', 5, 3, 21]<p>        ]);<p>        var options = {<p>          title&#58; 'Company Performance',<p>          vAxis&#58; {title&#58; '',  titleTextStyle&#58; {color&#58; 'red'}}<p>        };<p>        var chart = new google.visualization.ColumnChart(document.getElementById('chart_div'));<p>        chart.draw(data, options);<p>      }<p>    </script><p>  </head><p>  <body><p>    <div id=chart_div style=width&#58; 900px; height&#58; 500px;></div><p>  </body><p></html>
  do_you_have_a_google_graph_analyzing_incentives_: 
  do_your_crowd_workers_need_specialized_skills: No
  what_sort_of_skills_do_they_need: The only requirement is that they speak English.
  if_skills_vary_widely_what_factors_cause_one_person_to_be_better_than_another_: N/A
  did_you_analyze_the_skills_of_the_crowd: No
  if_you_analyzed_skills_what_analysis_did_you_perform: N/A
  do_you_have_a_google_graph_analyzing_skills_: No
  if_you_have_a_graph_analyzing_skills_include_the_html_for_a_google_graph_here: N/A
  is_the_quality_of_what_the_crowd_gives_you_a_concern: Yes
  did_you_create_a_user_interface_for_the_crowd_workers: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_crowdfacing_user_interface: https://github.com/jLukeC/PReTweet/blob/master/images/Full&#37;20HIT.JPG
  describe_your_crowdfacing_user_interface: The crowd&#45;facing interface is the HIT we designed for our crowd workers.  Its design is simple, consisting of the tweet itself and a scale (check box) for each metric&#45;&#45;appropriateness, humor level, and grammatical accuracy.
  did_you_analyze_the_aggregated_results_: No
  what_analysis_did_you_perform_on_the_aggregated_results: 
  did_you_analyze_the_quality_of_what_you_got_back: Yes
  what_analysis_did_you_perform_on_the_aggregated_results: N/A
  what_analysis_did_you_perform_on_quality: We did a few test runs to make sure that our test questions were fair and accurate.  The test question was answered correctly every time, so we are confident that this type of quality control is effective.
  do_you_have_a_google_graph_analyzing_quality_: No
  if_you_have_a_graph_analyzing_quality_include_the_html_for_a_google_graph_here: N/A
  is_this_something_that_could_be_automated: Yes
  if_it_could_be_automated_say_how__if_it_is_difficult_or_impossible_to_automate_say_why: This could be automated with machine learning, but because appropriate can depend on current events and the subject of tweets isn't always clear, a human opinion is much more reliable.
  do_the_skills_of_individual_workers_vary_widely: No
  do_you_have_a_google_graph_analyzing_the_aggregated_results_: No
  if_you_have_a_graph_analyzing_the_aggregated_results_include_the_html_for_a_google_graph_here: N/A
  did_you_create_a_user_interface_for_the_end_users_to_see_the_aggregated_results: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_user_interface_for_the_end_user: https://github.com/jLukeC/PReTweet/blob/master/images/response&#37;202.png<p>https://github.com/jLukeC/PReTweet/blob/master/images/response&#37;201.PNG<p>
  describe_what_your_end_user_sees_in_this_interface: The user receives a text that looks like this&#58;<p>Appropriateness&#58;    x / 3<p>Humor&#58;                   x / 3<p>Grammar&#58;               x / 3
  what_is_the_scale_of_the_problem_that_you_are_trying_to_solve_: Depending on the success of our product, the scale of the problem could become very large.  Even if 20 companies signed up for this service, we would receive a semi&#45;constant stream of tweets requiring reliable analysis.  The problem we are trying to solve is universal; any company that has a Twitter account could benefit, but only companies dedicated to improving and sustaining their social media image would participate.
  if_it_would_benefit_from_a_huge_crowd_how_would_it_benefit_: More workers means faster results.  Latency can be a problem if users have time sensitive tweets they want to publish, and having more workers would decrease wait time substantially.  Since the current wait time is about 10 minutes for 6 cents per tweet, in a future development of our application, we would give the user the option to pay more for less wait time.
  what_challenges_would_scaling_to_a_large_crowd_introduce: A larger crowd would require higher pay to incite more people to participate.
  did_you_perform_an_analysis_about_how_to_scale_up_your_project: Yes
  what_analysis_did_you_perform_on_the_scaling_up: (See Incentive Analysis)
  do_you_have_a_google_graph_analyzing_scaling_: No
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: N/A
  url_to_the_flow_diagram_for_your_project: https://github.com/jLukeC/PReTweet/blob/master/README.md#project&#45;flowchart
  if_the_crowd_was_real_how_did_you_recruit_participants: We used Crowdflower's interface, paying $0.05 per 10 tweets.
  did_your_project_work: Yes! We have done many full tests and our process is working well.  The testing tweets have received feedback in 10&#45;12 minutes consistently and the workers are giving reliable results.   
  do_you_have_a_google_graph_analyzing_your_project_: No
  if_you_have_a_graph_analyzing_your_project_include_the_html_for_a_google_graph_here: N/A
  caption_for_your_google_graph_(project_analysis): N/A
  what_were_the_biggest_challenges_that_you_had_to_deal_with: Dealing with input and output was the biggest challenge for this assignment.  Conceptually, all we had to do was get the tweet from the user, put that text into a pre&#45;made HIT, upload it to Crowdflower, and then when it's done, download the data and send it back.  This was tough because we had to learn how to implement the Twilio API and post a HIT to Crowdflower from a Python script.  
  where_there_major_changes_between_what_you_originally_proposed_and_your_final_product: No
  if_so_what_changed_between_your_original_plan_and_your_final_product: N/A
  what_are_some_limitations_of_your_product: The biggest limitation is how we take in the potential tweets before they are published.  Ideally, PReTweet would be a web app that Twitter users sign in to, and their tweets would be automatically processed when sent through Twitter itself.  To get this implementation we would need more experience with web app design.  In the future&#45;&#45;possibly for PennApps&#45;&#45;we hope to accomplish this, but we wanted to make sure our minimum viable project was a success before going further.  
  did_your_results_deviate_from_what_you_would_expect_from_previous_work_or_what_you_learned_in_the_class: No
  if_your_results_deviated_why_might_that_be: N/A
  caption_for_your_graph_(scaling_up): N/A
  caption_for_your_graph_(aggregation): N/A
  caption_for_your_graph_(quality): N/A
  caption_for_your_graph_(skills): N/A
  caption_for_your_graph_(incentives): Incentive Analysis
  is_there_anything_else_youd_like_to_say_about_your_project: Thanks for your help!  We really enjoyed this class!
  did_you_train_a_machine_learning_component_from_what_the_crowd_gave_you: No
  if_you_trained_a_machine_learning_component_describe_what_you_did: N/A
  did_you_analyze_the_quality_of_the_machine_learning_component: No
  if_you_have_a_graph_analyzing_a_machine_learning_component_include_the_html_for_a_google_graph_here: N/A
  caption_for_your_graph_(incentives): 
  caption_for_your_graph_(machine_learning_component): N/A
  what_was_the_main_focus_of_your_teams_effort: Engineering a complex system
  provide_a_link_to_your_final_presentation_video: 
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_skills_: 
  what_incentivi: 
-
  how_many_teammates_are_in_your_group: 3
  name_1: Jamie Ariella Levine
  name_2: Elijah Valenciano
  name_3: Louis Petro
  name_4: 
  pennkey_username_1: Levjam
  pennkey_username_2: Elijahv
  pennkey_username_3: Petrol
  pennkey_username_4: 
  name_of_your_project: FoodFlip
  give_a_one_sentence_description_of_your_project: Food Flip makes living with food restrictions easier by crowdmembers giving suggestions for recipe swaps.
  url_to_the_logo_for_your_project: https://github.com/AriellaLev/FoodFlip/blob/master/screenshots_and_images/logo.jpg
  what_similar_projects_exist: These aren't crowdsourced projects &#45; just static websites that give advice to deal and live with food restrictions&#58;<p>http://www.kidswithfoodallergies.org/resourcespre.php?id=93&<p>http://www.eatingwithfoodallergies.com<p>A site which uses a similar question&#45;and&#45;answer community is Stack Overflow. 
  what_type_of_project_is_it: A business idea that uses crowdsourcing
  how_do_you_incentivize_the_crowd_to_participate: People with food restrictions would most likely participate in the Food Flip community. They would be incentivized because they would be users contributing to a community in which they can also benefit a lot from by finding answers to their own questions. The more people, the more ideas,  and the more help provided.  The site provides a community of those who face similar situations or have similar tastes. It will be more efficient and trustworthy than say a Yahoo Answer’s answer, yet it will be more diverse than a simple Google search of “gluten&#45;free bread” where you get a lot of the same generic answers. They were also incentivized by a point system of upvotes and downvotes, with a slight gamification factor of the asker’s final answer, where that answer is presented at the top of the screen regardless of upvotes or downvotes.
  what_does_the_crowd_provide_for_you: The crowd provides a resource that normal instructive sites would not&#58; human experiences with food substitutions, and many of them at that, providing a vast variety of food substitution options. Crowdsourcing also allows people to build off of each other’s experiences and ideas. The crowd provides quesitons for food experiences as well as answers to these questions.
  how_do_you_ensure_the_quality_of_the_crowd_provides__: In our website, we handle with quality control in many ways. We will deal with the issues chronologically, in the order that users would encounter each issue.<p>When users are signing up for accounts, there is a security question people must get right for the account creation submission to go through and execute. The question is a simple math question that all FoodFlip prospective users should be able to handle. This security question prevents spambots from signing up from our website and providing excessive unecessary traffic.<p>After a question is asked and answers are provided, logged&#45;in users can upvote and downvote answers to their hearts' contents. I, personally, would be more likely to look at an answer with 17 upvotes than an answer with 3 upvotes or 7 downvotes. Basically, people get credit where credit is deserved.<p>Additionally, the asker of the question, in addition to website admins, can select his or her favorite answer. Anyone reading the question afterwards might be wondering about the personal opinion of the original question asker, and the format of the website allows the specific best answer, as chosen by the asked, to be displayed as special.<p>Lastly, users can flag answers. They can flag answers as bad if they feel the answers to be irrelevant or disrespectful.<p>We feel that all of these methods effectively accounts for quality control.
  how_do_you_aggregate_the_results_from_the_crowd: Results were aggregated from the crowd by keeping track of the number of users, the number of questions asked, and the number of answers. We are also be keeping track of the counts of tags and categories used. These aggregation tools are provided by the DW Q&A tool of our site platform. 
  how_does_your_project_work: Our project is a business idea using question and answer forum crowdsourcing. The crowds provide both the questions and the answers and  quality control is provided via the crowd who can upvote and downvote both questions and answers. The aggregation of data (including the aggregation of quality control features) is done automatically by Wordpress. The project is a website run through Wordpress with a question and answer module. Quality control is processed by users upvoting, downvoting, and selecting the best answers, which only website staff or question asker specifically can do. The website automatically updates user interaction to everyone's screen immediately using Wordpress. As an additional feature, users also have the opportunity to create their own groups in order to improve communication among a small group of users.
  would_your_project_benefit_if_you_could_get_contributions_from_thousands_of_people: Yes
  vimeo_link: https://vimeo.com/114575200
  pennkey_username_1: 
  may_we_have_your_permission_to_feature_your_project_on_the_class_website: Yes
  team_member_1__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_2__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_3__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_4__can_we_list_your_name_listed_alongside_your_project: 
  who_are_the_members_of_your_crowd: Members of our crowd are usually people with experience cooking with food restrictions or healthier food options. Currently the crowd that we were able to obtain is made up mostly of college students who are looking for healthier options (for their own bodies). 
  how_many_unique_participants_did_you_have: 25
  for_your_final_project_did_you_simulate_the_crowd_or_run_a_real_experiment: Real crowd
  if_the_crowd_was_simulated_how_did_you_collect_this_set_of_data: 
  if_the_crowd_was_simulated_how_would_you_change_things_to_use_a_real_crowd: 
  how_do_you_ensure_the_quality_of_the_crowd_provides: 
  what_motivation_does_the_crowd_have_for_participating_in_your_project: Altruism, Reputation, A sense of community
  did_you_perform_any_analysis_comparing_different_incentives_: No
  if_you_compared_different_incentives_what_analysis_did_you_perform_: 
  if_you_have_a_graph_analyzing_incentives_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_incentives_: 
  do_your_crowd_workers_need_specialized_skills: No
  what_sort_of_skills_do_they_need: They need to be familiar with basic cooking skills, or at least be familiar withnhaving knowledge of the recipe. (Wow, this cake was really made with applesauce instead of eggs?!) The more experience they have, the better the quality.
  if_skills_vary_widely_what_factors_cause_one_person_to_be_better_than_another_: The best case scenario is that someone has cooked a given recipe with and without certain food substitutes and also tried the food each time. Sometimes people cook a recipe only with a substitute, so they have no baseline to compare to when they eat it. Sometimes people cook both with and without the substitute but don't eat the food. The minimally qualified person is someone who has eaten the food with hearsay of the ingredients. Those participants with less skills or experience than that don't have reliable data.
  did_you_analyze_the_skills_of_the_crowd: No
  if_you_analyzed_skills_what_analysis_did_you_perform: 
  do_you_have_a_google_graph_analyzing_skills_: No
  if_you_have_a_graph_analyzing_skills_include_the_html_for_a_google_graph_here: 
  is_the_quality_of_what_the_crowd_gives_you_a_concern: No
  did_you_create_a_user_interface_for_the_crowd_workers: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_crowdfacing_user_interface: https://github.com/AriellaLev/FoodFlip/blob/master/screenshots_and_images/crowdfacinguserinterace.png  But the live website is here&#58;  www.foodflip.org
  describe_your_crowdfacing_user_interface: On the website, the user interface for the question page includes a category sections, associated tags, and your written question with possible extra information. The categories include the main types of preparing food including baking and grilling. Tags can be put with relevant keywords such as “vegan” to find questions easier. The WordPress tool also provides a cool machine learning feature in which it remembers the past submitted questions. So when you start to type in a question, you can see related questions. <p>On the actual recipe questions interface of submitted questions, you can see all of the questions that have been asked as well as filter them by their different statuses. The questions can also be ranked by number of views, the number of answers, or the number of votes for the question itself.
  did_you_analyze_the_aggregated_results_: Yes
  what_analysis_did_you_perform_on_the_aggregated_results: 
  did_you_analyze_the_quality_of_what_you_got_back: No
  what_analysis_did_you_perform_on_the_aggregated_results: We analyzed the amount of tags represented by questions and we analyzed how many questions were asked per category. 
  what_analysis_did_you_perform_on_quality: 
  do_you_have_a_google_graph_analyzing_quality_: No
  if_you_have_a_graph_analyzing_quality_include_the_html_for_a_google_graph_here: 
  is_this_something_that_could_be_automated: No
  if_it_could_be_automated_say_how__if_it_is_difficult_or_impossible_to_automate_say_why: This would be nearly super impossible to automate, as far as we know as college students in 2014 without high security clearance, unless you had a artificial intelligent computer that was trained to accurately judge human taste/qualia based on the food (the chemicals themselves, texture, digestion, etc), at which point it could brute force permutations of foods to substitute. However, I still can't imagine how a computer could generate the questions themselves.
  do_the_skills_of_individual_workers_vary_widely: Yes
  do_you_have_a_google_graph_analyzing_the_aggregated_results_: Yes
  if_you_have_a_graph_analyzing_the_aggregated_results_include_the_html_for_a_google_graph_here: https://github.com/AriellaLev/FoodFlip/blob/master/data/QuestionCategories.html<p>https://github.com/AriellaLev/FoodFlip/blob/master/data/QuestionTags.html
  did_you_create_a_user_interface_for_the_end_users_to_see_the_aggregated_results: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_user_interface_for_the_end_user: https://github.com/AriellaLev/FoodFlip/blob/master/screenshots_and_images/question_interface.png
  describe_what_your_end_user_sees_in_this_interface: On the website, the user interface for the question page includes a category sections, associated tags, and your written question with possible extra information. The categories include the main types of preparing food including baking and grilling. Tags can be put with relevant keywords such as “vegan” to find questions easier. The WordPress tool also provides a feature in which it remembers the past submitted questions. So when you start to type in a question, you can see related questions. <p>
  what_is_the_scale_of_the_problem_that_you_are_trying_to_solve_: We are building this website for all English&#45;speaking food&#45;consumers with food restrictions who might benefit from this website. Right now, our website is built for college students in the social network of the website staff.  Members of our crowd are usually people with experience cooking with food restrictions or healthier food options.  
  if_it_would_benefit_from_a_huge_crowd_how_would_it_benefit_: Our project would definitely be a lot more useful if thousands of people could provide their experiences and preferences about recipes and food swaps. The more suggestions/options available, the better food&#45;restricted people might be able to eat. If thousands of people also ask more questions, people with food restrictions could be able to browse through the website to find good recipes they might want to try.
  what_challenges_would_scaling_to_a_large_crowd_introduce: Wordpress is supposed to be really great at dealing with large&#45;scale interface. As the platform matures, all of the available plugins mature, and scaling up a website isn't too hectic and complicated. If you're having an issue, there's tons of documentation on the Internet available on Google web searches. If you're asking a question about Wordpress, chances are someone else has asked the question also (and gotten the question answered by someone else on the Internet). We would have to ensure that our server has sufficient processor power and memory resources to meet these large&#45;scale demands though.
  did_you_perform_an_analysis_about_how_to_scale_up_your_project: No
  what_analysis_did_you_perform_on_the_scaling_up: 
  do_you_have_a_google_graph_analyzing_scaling_: No
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  url_to_the_flow_diagram_for_your_project: https://raw.githubusercontent.com/AriellaLev/FoodFlip/master/general_flow.jpg
  if_the_crowd_was_real_how_did_you_recruit_participants: We recruited our friends and classmates to give us data. People with food restrictions and people seeking food substitutions participated. More users would be incentivized because they would be users already in this virtual community who also want to share their advice or opinions. 
  did_your_project_work: It works as a concept, but we haven't found enough users with food&#45;restriction experience to really get the website filled with enough data to actually be helpful to the user&#45;base. We've been collecting data for about a week and we've collected many questions about food swaps, but most of our questions don't have answers or answers the askers were happy with. We think the lack of answers is because most college students don't have lots of experience with cooking. 
  do_you_have_a_google_graph_analyzing_your_project_: Yes
  if_you_have_a_graph_analyzing_your_project_include_the_html_for_a_google_graph_here: https://github.com/AriellaLev/FoodFlip/blob/master/data/QuestionCategories.html<p>https://github.com/AriellaLev/FoodFlip/blob/master/data/QuestionTags.html
  caption_for_your_google_graph_(project_analysis): Categories of Questions Asked, Tags for Questions Asked
  what_were_the_biggest_challenges_that_you_had_to_deal_with: Our biggest challenge was getting enough of a user base to be self&#45;perpetuating, which is the only point at which the business model is feasible.
  where_there_major_changes_between_what_you_originally_proposed_and_your_final_product: No
  if_so_what_changed_between_your_original_plan_and_your_final_product: 
  what_are_some_limitations_of_your_product: Similar to our challenges, our we were limited by the knowledge of our crowd to provide adequate answers, as well as a wide enough user base to provide accurate, appropriate and timely answers.
  did_your_results_deviate_from_what_you_would_expect_from_previous_work_or_what_you_learned_in_the_class: No
  if_your_results_deviated_why_might_that_be: 
  caption_for_your_graph_(scaling_up): 
  caption_for_your_graph_(aggregation): Categories of Questions Asked, Tags for Questions Asked
  caption_for_your_graph_(quality): 
  caption_for_your_graph_(skills): 
  caption_for_your_graph_(incentives): 
  is_there_anything_else_youd_like_to_say_about_your_project: 
  did_you_train_a_machine_learning_component_from_what_the_crowd_gave_you: No
  if_you_trained_a_machine_learning_component_describe_what_you_did: 
  did_you_analyze_the_quality_of_the_machine_learning_component: No
  if_you_have_a_graph_analyzing_a_machine_learning_component_include_the_html_for_a_google_graph_here: 
  caption_for_your_graph_(incentives): 
  caption_for_your_graph_(machine_learning_component): 
  what_was_the_main_focus_of_your_teams_effort: Something in between
  provide_a_link_to_your_final_presentation_video: 
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_skills_: 
  what_incentivi: 
-
  how_many_teammates_are_in_your_group: 2
  name_1: Joshua Stone
  name_2: Venkata Amarthaluru
  name_3: 
  name_4: 
  pennkey_username_1: jston
  pennkey_username_2: avenkata
  pennkey_username_3: 
  pennkey_username_4: 
  name_of_your_project: MarketChatter
  give_a_one_sentence_description_of_your_project: MarketChatter allows an analysis of the stock sentiment by examining data from social media and news articles.
  url_to_the_logo_for_your_project: https://github.com/joshuastone/nets213&#45;final&#45;project/blob/master/logo.png
  what_similar_projects_exist: There are some direct competitors including Bloomberg and StockTwits, which also attempt to solve the problem of communicating public sentiment on particular equities through news articles and tweets respectively. Indirect competitors solving related problems include Chart.ly (allows integration of stock price charts with Twitter) and Covestor (provides company specific tweet updates via email). There are even some alternative investment funds, such as social&#45;media based hedge funds, that use social networks as the basis for the investment methodology. An example of hedge fund that implements this type of investment mandate is Derwent Capital, also known as “the Twitter Hedge Fund”. MarketChatter differentiates itself from direct and indirect competitors by overlaying information from both articles and tweets. Unlike social&#45;media based hedge funds, MarketChatter provides retail investors with easy accessibility.
  what_type_of_project_is_it: A business idea that uses crowdsourcing
  how_do_you_incentivize_the_crowd_to_participate: Content Creators were incentivized with intrinsic motivation from enjoyment, altruism, and reputation. Social media allows customers to rant about their experiences providing a form of enjoyment. Talking about experiences allows individuals to help friends in their network to avoid bad opportunities and pursue good opportunities representing altruism. Journalists are naturally incentivized in producing content that generates a lot views in order to build reputation.  The other crowd consisting of Content Evaluators was incentivized with extrinsic motivation. Workers on CrowdFlower benefited from payment. Monetary compensation consisted of $0.01 per 10 tweets and $0.01 per 2 articles about a company. We received Crowdsourced sentiment labels for approximately 100 tweets and 20 articles for each of 20 companies. 
  what_does_the_crowd_provide_for_you: The content creators crowd helps provide us with the raw data necessary to conduct the sensitivity analysis. The content evaluators crowd provides us with the human computation power necessary to perform the sensitivity analysis. MarketChatter benefits from Crowdsourcing since computers struggle with assessing emotions embedded in text, whereas humans can easily perform sentiment analysis.
  how_do_you_ensure_the_quality_of_the_crowd_provides__: The quality of sentiment labels is ensured through a few controls. The first control was language spoken which restricted workers to those who speak English since all articles and Tweets are in English.<p>The next control was country of origin which restricted workers to those within the U.S to increase chance of familiarity with the U.S companies among workers.<p>Additionally, test questions were used to filter the results of workers. In order for a worker's results to be considered valid, he/she had to complete a minimum of 3 test questions with 66&#37; overall accuracy.<p>The final quality control mechanism consisted of collecting three judgments per article/Tweet. To generate the final sentiment label for each article/Tweet, a weighted majority vote was performed. First, each unique worker had a weight score generated based on the proportion of test questions that he/she answered correctly. Next, a positive score, negative score, neutral score, and irrelevant score was generated for each article/Tweet using the weights of the workers responsible for assigning labels to the respective article/Tweet. Finally, the majority weighted label was take as the final sentiment label for each article/Tweet.<p>
  how_do_you_aggregate_the_results_from_the_crowd: MarketChatter scrapes information from Tweets and news articles from Yahoo Finance, a financial news aggregator. We use Yahoo Finance ticker specific RSS feeds to get the raw data. To aggregate the responses from the crowd, we employ weighted majority vote. This leads to a better aggregation procedure than just a majority vote. The weighted majority vote will use worker scores computed from worker test questions. 
  how_does_your_project_work: 1. Customers voluntarily post information about their experiences and journalist post news online.<p>2. MarketChatter would scrape the web for data about a particular company using Twitter and ticker specific RSS feeds from Yahoo Finance. <p>3. Crowdsourcing to CrowdFlower allows for sentiment analysis to assess each data point as positive, neutral, or negative.<p>4. Finally, results of the sentiment analysis are shown in a convenient format.<p>
  would_your_project_benefit_if_you_could_get_contributions_from_thousands_of_people: Yes
  vimeo_link: https://vimeo.com/114588838
  pennkey_username_1: 
  may_we_have_your_permission_to_feature_your_project_on_the_class_website: Yes
  team_member_1__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_2__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_3__can_we_list_your_name_listed_alongside_your_project: 
  team_member_4__can_we_list_your_name_listed_alongside_your_project: 
  who_are_the_members_of_your_crowd: MarketChatter makes use of two crowds. Content Creators represent the first crowd, which is composed of people posting Tweets and journalists writing news articles. Content Evaluators represent the second crowd, which is composed of members on CrowdFlower that will help conduct the sentiment analysis. 
  how_many_unique_participants_did_you_have: 431
  for_your_final_project_did_you_simulate_the_crowd_or_run_a_real_experiment: Real crowd
  if_the_crowd_was_simulated_how_did_you_collect_this_set_of_data: 
  if_the_crowd_was_simulated_how_would_you_change_things_to_use_a_real_crowd: 
  how_do_you_ensure_the_quality_of_the_crowd_provides: 
  what_motivation_does_the_crowd_have_for_participating_in_your_project: Pay, Altruism
  did_you_perform_any_analysis_comparing_different_incentives_: Yes
  if_you_compared_different_incentives_what_analysis_did_you_perform_: We compared two different levels payment incentives for the Content Evaluators crowd and the resulting distribution of Trusted vs. Untrusted judgments as determined by Crowdflower. We found that merely increasing the payment from $0.01 to $0.02 per task (representing a $0.01 bonus) significantly improved the amount of trusted judgments for both articles and tweets.
  if_you_have_a_graph_analyzing_incentives_include_the_html_for_a_google_graph_here: <html><p>  <head><p>    <script type=text/javascript src=https://www.google.com/jsapi></script><p>    <script type=text/javascript><p>    google.load(visualization, 1, {packages&#58;[corechart]});<p>    google.setOnLoadCallback(drawChart);<p>    function drawChart() {<p>      var data = google.visualization.arrayToDataTable([<p>        ['Trustworthiness', 'Trusted', 'Untrusted', { role&#58; 'annotation' } ],<p>        ['Articles (Pre&#45;Bonus)', 38, 62,''],<p>        ['Articles (Post&#45;Bonus)', 96, 4,''],<p>        ['Tweets (Pre&#45;Bonus)', 63, 37,''],<p>        ['Tweets (Post&#45;Bonus)', 96, 4,'']<p>      ]);<p>      var options = {<p>        width&#58; 800,<p>        height&#58; 400,<p>        legend&#58; { position&#58; 'top', maxLines&#58; 3 },<p>        bar&#58; { groupWidth&#58; '75&#37;' },<p>        isStacked&#58; true,<p>        hAxis&#58; {title&#58; 'Data Source'},<p>        vAxis&#58; {title&#58; 'Cumulative &#37; of Total'}<p>      };<p>      var chart = new google.visualization.ColumnChart(document.getElementById('chart_div'));<p>      chart.draw(data, options);<p>    }<p>    </script><p>  </head><p>  <body><p>    <div id=chart_div style=width&#58; 900px; height&#58; 500px;></div><p>  </body><p></html>
  do_you_have_a_google_graph_analyzing_incentives_: 
  do_your_crowd_workers_need_specialized_skills: No
  what_sort_of_skills_do_they_need: Workers need to be able to detect the emotion and mood associated with a Tweet or article. This requires reading comprehension. Workers also need to assess whether the information conveyed in the article fundamentally affects the company’s operations. This requires critical reasoning. Overall, MarketChatter requires English speaking US&#45;based workers in order for the workers to have the proper skill set necessary for the task.
  if_skills_vary_widely_what_factors_cause_one_person_to_be_better_than_another_: For our project, no, the skills did not vary widely since we were only restricting workers based on language spoken and country of origin.
  did_you_analyze_the_skills_of_the_crowd: Yes
  if_you_analyzed_skills_what_analysis_did_you_perform: We analyzed how well workers performed on the test questions, differentiating between articles and Tweets. It was important to differentiate between articles and Tweets because we expected lazy workers to attempt to speed through the article tasks which require greater focus. As expected, more workers failed the test questions for the article job.
  do_you_have_a_google_graph_analyzing_skills_: No
  if_you_have_a_graph_analyzing_skills_include_the_html_for_a_google_graph_here: <html><p>  <head><p>    <script type=text/javascript src=https://www.google.com/jsapi></script><p>    <script type=text/javascript><p>    google.load(visualization, 1, {packages&#58;[corechart]});<p>    google.setOnLoadCallback(drawChart);<p>    function drawChart() {<p>      var data = google.visualization.arrayToDataTable([<p>        ['Correctness', 'Correctly Answered Test Questions', 'Incorrectly Answered Test Questions', { role&#58; 'annotation' } ],<p>        ['Articles', 1754, 542,''],<p>        ['Tweets', 1849, 206, '']<p>      ]);<p>      var options = {<p>        width&#58; 800,<p>        height&#58; 400,<p>        legend&#58; { position&#58; 'top', maxLines&#58; 3 },<p>        bar&#58; { groupWidth&#58; '75&#37;' },<p>        isStacked&#58; true,<p>        hAxis&#58; {title&#58; 'Data Source'},<p>        vAxis&#58; {title&#58; 'Cumulative Proportion of Total'}<p>      };<p>      var chart = new google.visualization.ColumnChart(document.getElementById('chart_div'));<p>      chart.draw(data, options);<p>    }<p>    </script><p>  </head><p>  <body><p>    <div id=chart_div style=width&#58; 900px; height&#58; 500px;></div><p>  </body><p></html>
  is_the_quality_of_what_the_crowd_gives_you_a_concern: Yes
  did_you_create_a_user_interface_for_the_crowd_workers: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_crowdfacing_user_interface: https://github.com/joshuastone/nets213&#45;final&#45;project/blob/master/crowdflower_ui.png
  describe_your_crowdfacing_user_interface: The interface is fairly basic and consists of the company symbol, a link to the Google Finance page of the company and the text contained in the Tweet or a link to the Article. Finally, it contains a  multiple choice question requesting the sentiment of the article/tweet and a checkbox to indicate if the user believes the information in the article/tweet will impact the company's operational performance.
  did_you_analyze_the_aggregated_results_: Yes
  what_analysis_did_you_perform_on_the_aggregated_results: 
  did_you_analyze_the_quality_of_what_you_got_back: Yes
  what_analysis_did_you_perform_on_the_aggregated_results: We preformed sentiment analysis across 20 different publicly traded companies. We looked at the sentiment analysis by aggregating results across all 20 of the companies to generate the sentiment across this portfolio. One can compare the sentiment analysis for a particular company with the entire aggregated portfolio to better understand whether a particular security has more favorable public sentiment than the broader market.
  what_analysis_did_you_perform_on_quality: To assess quality, we looked at the agreement among the workers for the same Tweet/Article. We did quality control through agreement and redundancy. There are multiple judgements since each Tweet/Article is labeled by three separate workers. High levels of agreement indicate a higher quality judgement. We found that 32&#37; of Tweets/Articles had unanimous agreement among all three workers, and 53&#37; had two of the three workers agree on the sentiment label. Only 15&#37; of Tweets/Articles had no agreement. Overall, this indicates MarketChatter has high quality.
  do_you_have_a_google_graph_analyzing_quality_: Yes
  if_you_have_a_graph_analyzing_quality_include_the_html_for_a_google_graph_here: <html><p>  <head><p>    <script type=text/javascript src=https://www.google.com/jsapi></script><p>    <script type=text/javascript><p>      google.load(visualization, 1, {packages&#58;[corechart]});<p>      google.setOnLoadCallback(drawChart);<p>      function drawChart() {<p>        var data = google.visualization.arrayToDataTable([<p>          ['Number of Workers in Agreement', '&#37;'],<p>          ['All',     0.32],<p>          ['Two',      0.53],<p>          ['None',  0.15]<p>        ]);<p>        var options = {<p>          title&#58; '&#37; Agreement Between Workers'<p>        };<p>        var chart = new google.visualization.PieChart(document.getElementById('piechart'));<p>        chart.draw(data, options);<p>      }<p>    </script><p>  </head><p>  <body><p>    <div id=piechart style=width&#58; 900px; height&#58; 500px;></div><p>  </body><p></html>
  is_this_something_that_could_be_automated: Yes
  if_it_could_be_automated_say_how__if_it_is_difficult_or_impossible_to_automate_say_why: Yes, it would be possible to train a machine learning model (most likely naive Bayes) to label tweets and articles as positive or negative. However, humans would naturally be better at detecting sentiment as well as identifying articles/tweets that merely mention a company without the company being the focus of the article/tweet (these are articles/tweets that we would want to ignore).
  do_the_skills_of_individual_workers_vary_widely: No
  do_you_have_a_google_graph_analyzing_the_aggregated_results_: Yes
  if_you_have_a_graph_analyzing_the_aggregated_results_include_the_html_for_a_google_graph_here: <html><p>  <head><p>    <script type=text/javascript src=https://www.google.com/jsapi></script><p>    <script type=text/javascript><p>      google.load(visualization, 1, {packages&#58;[corechart]});<p>      google.setOnLoadCallback(drawChart);<p>      function drawChart() {<p>        var data = google.visualization.arrayToDataTable([<p>          ['Sentiment', '&#37;'],<p>          ['Positive',     0.25],<p>          ['Negative',      0.10],<p>          ['Neutral',  0.65]<p>        ]);<p>        var options = {<p>          title&#58; 'Aggregate Market Sentiment'<p>        };<p>        var chart = new google.visualization.PieChart(document.getElementById('piechart'));<p>        chart.draw(data, options);<p>        var data2 = google.visualization.arrayToDataTable([<p>        ['Sentiment', 'Positive', 'Negative', 'Neutral', { role&#58; 'annotation' } ],<p>        [' GOOG ', 0.318235593349 , 0.139861005277 , 0.541903401374 ,''],<p>[' JPM ', 0.3096751808 , 0.295420829309 , 0.39490398989 ,''],<p>[' CVX ', 0.278847848846 , 0.212233568925 , 0.50891858223 ,''],<p>[' WFC ', 0.48646813948 , 0.114670489626 , 0.398861370893 ,''],<p>[' TWTR ', 0.28522406228 , 0.238545643617 , 0.476230294102 ,''],<p>[' AAPL ', 0.82560578687 , 0.0676584438709 , 0.106735769259 ,''],<p>[' BAC ', 0.444468819271 , 0.0921572144018 , 0.463373966327 ,''],<p>[' KO ', 0.404313650333 , 0.0870520114642 , 0.508634338203 ,''],<p>[' FB ', 0.397287135188 , 0.0985519509313 , 0.504160913881 ,''],<p>[' WMT ', 0.352378904735 , 0.10804804576 , 0.539573049506 ,''],<p>[' ORCL ', 0.469870444442 , 0.12939313903 , 0.400736416528 ,''],<p>[' JNJ ', 0.501076209741 , 0.0921486348624 , 0.406775155397 ,''],<p>[' T ', 0.270314142406 , 0.129061030016 , 0.600624827577 ,''],<p>[' MRK ', 0.440947531969 , 0.235863520119 , 0.323188947912 ,''],<p>[' GE ', 0.303080536313 , 0.100223623698 , 0.596695839989 ,''],<p>[' VZ ', 0.391060420542 , 0.153341167765 , 0.455598411693 ,''],<p>[' XOM ', 0.247896309343 , 0.249700192574 , 0.502403498084 ,''],<p>[' PFE ', 0.46655500696 , 0.0301178355567 , 0.503327157484 ,''],<p>[' MSFT ', 0.369145640593 , 0.189308791745 , 0.441545567662 ,''],<p>[' PG ', 0.408923039568 , 0.0719538200011 , 0.519123140431 ,'']<p>      ]);<p>      var options2 = {<p>        width&#58; 800,<p>        height&#58; 400,<p>        legend&#58; { position&#58; 'top', maxLines&#58; 3 },<p>        bar&#58; { groupWidth&#58; '75&#37;' },<p>        isStacked&#58; true,<p>        hAxis&#58; {title&#58; 'Ticker Symbol'},<p>        vAxis&#58; {title&#58; 'Cumulative &#37; of Total'}<p>      };<p>      var chart2 = new google.visualization.ColumnChart(document.getElementById('chart_div2'));<p>      chart2.draw(data2, options2);<p>      }<p>    </script><p>  </head><p>  <body><p>    <div id=piechart style=width&#58; 900px; height&#58; 500px;></div><p>  <div id=chart_div2 style=width&#58; 900px; height&#58; 500px;></div><p>  </body><p></html>
  did_you_create_a_user_interface_for_the_end_users_to_see_the_aggregated_results: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_user_interface_for_the_end_user: https://github.com/joshuastone/nets213&#45;final&#45;project/blob/master/ui.png
  describe_what_your_end_user_sees_in_this_interface: The user interface features a search bar, the company being examined, the distribution of sentiment associated with that company, the sentiment of the company compared to the market as a whole, and a list of the most recently analyzed articles and Tweets with their associated sentiment.
  what_is_the_scale_of_the_problem_that_you_are_trying_to_solve_: The problem we are trying to solve is very large in scale, as it could technically be expanded to included not only all stocks on US stock exchanges, but also all stocks across all exchanges around the world. For our project, we limited our scope to 20 large&#45;cap US stocks to remain cost&#45;effective.
  if_it_would_benefit_from_a_huge_crowd_how_would_it_benefit_: MarketChatter provides sentiment analysis on any publicly traded stock. If there is a huge content evaluators crowd, this allows for more extensive raw data allowing a more representative sample of the public’s view on the company. If there is a huge content evaluators crowd, this increases the supply of workers and reduces the cost of performing sentiment analysis. The other interesting phenomenon is that as more and more companies are searched there will be increasing benefits to scale since MarketChatter would already have sentiment labels for a lot of relevant Tweets/Articles for  companies already searched in the past. 
  what_challenges_would_scaling_to_a_large_crowd_introduce: The primary challenge with scaling to a large crowd that this project would encounter is remaining financially feasible. However, assuming that a larger crowd leads to more accurate results, we would expect for users to be more willing to pay higher fees for the service as well, so this obstacle could potentially be overcome if this project is converted into a business. Another issue with scaling to not only include a larger crowd but also a larger set of supported stocks is that of efficiency with the algorithm that downloads relevant Tweets/Articles and computes the aggregate sentiment scores.
  did_you_perform_an_analysis_about_how_to_scale_up_your_project: Yes
  what_analysis_did_you_perform_on_the_scaling_up: We performed an analysis on how the algorithm that drives our project scales as the number of companies supported increases. To our enjoyment, we found that the algorithm scales well and appears to have a linear O(n) runtime. Since the number of stocks in the world is finite, this is satisfactory for our project.
  do_you_have_a_google_graph_analyzing_scaling_: Yes
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: <p><html><p>  <head><p>    <script type=text/javascript src=https://www.google.com/jsapi></script><p>    <script type=text/javascript><p>      google.load(visualization, 1, {packages&#58;[corechart]});<p>      google.setOnLoadCallback(drawChart);<p>      function drawChart() {<p>        var data = google.visualization.arrayToDataTable([<p>        ['Trustworthiness', 'Runtime', { role&#58; 'annotation' } ],<p>        [1, 0.79,''],<p>        [2, 1.5,''],<p>        [3, 2.19,''],<p>        [4, 2.92,''],<p>        [5, 3.49,''],<p>        [6, 4.31,''],<p>        [7, 5.41,''],<p>        [8, 7.61,''],<p>        [9, 7.72,''],<p>        [10, 7.65,''],<p>        [11, 7.88,''],<p>        [12, 8.625,''],<p>        [13, 11.14,''],<p>        [14, 11.12,''],<p>        [15, 11.72,''],<p>        [16, 12.36,''],<p>        [17, 12.53,''],<p>        [18, 13.86,''],<p>        [19, 13.41,''],<p>        [20, 15.71,'']<p>      ]);<p>        var options = {<p>          title&#58; 'Runtime vs. # Ticker Symbols (Linear Fit)',<p>          hAxis&#58; {minValue&#58; 0, maxValue&#58; 15},<p>          vAxis&#58; {minValue&#58; 0, maxValue&#58; 15},<p>          chartArea&#58; {width&#58;'50&#37;'},<p>          hAxis&#58; {title&#58; 'Number of Ticker Symbols'},<p>          vAxis&#58; {title&#58; 'Runtime (seconds)'},<p>          trendlines&#58; {<p>            0&#58; {<p>              type&#58; 'linear',<p>              showR2&#58; true,<p>              visibleInLegend&#58; true<p>            }<p>          }<p>        };<p>        var chartLinear = new google.visualization.ScatterChart(document.getElementById('chartLinear'));<p>        chartLinear.draw(data, options);<p>        options.title = 'Runtime vs. # Ticker Symbols (Exponential Fit)'<p>        options.trendlines[0].type = 'exponential';<p>        options.colors = ['#6F9654'];<p>        var chartExponential = new google.visualization.ScatterChart(document.getElementById('chartExponential'));<p>        chartExponential.draw(data, options);<p>      }<p>    </script><p>  </head><p>  <body><p>    <div id=chartLinear style=height&#58; 350px; width&#58; 800px></div><p>    <div id=chartExponential style=height&#58; 350px; width&#58; 800px></div><p>  </body><p></html>
  url_to_the_flow_diagram_for_your_project: https://github.com/joshuastone/nets213&#45;final&#45;project/blob/master/flow_chart.png
  if_the_crowd_was_real_how_did_you_recruit_participants: For the first Content Creators crowd it was relatively easy to recruit participants, since they were intrinsically motivated to provide content, which we obtained as raw data through scraping the web and obtaining Tweets as well as ticker&#45;specific RSS feeds. For the second Content Evaluators crowd it was more difficult to recruit participants. We decided to use CrowdFlower workers as the members for this crowd. The recruitment was primarily done by offering monetary compensation while controlling for English speaking US based workers. 
  did_your_project_work: In short, yes, our project works. Each Tweet/Article can result in an actual indication (positive, negative, neutral) or may result in a classification as irrelevant. To assess whether we obtained Tweets/Articles relevant to the particular stock, we looked at the composition of the sentiment analysis as &#37; Relevant and &#37; Irrelevant. We had 60&#37; of Tweets/Articles that were relevant implying that MarketChatter is doing a respectable job in collecting relevant Tweets/Articles. Additionally, we qualitatively compared some of the sentiment predictions with subsequent stock performance. In particular, Apple, which had a very strong positive sentiment at the time due to iWatch projections, experienced an increase in stock price the next day. In contrast, JP Morgan and Exxon Mobile experienced decreases in stock price over the following few days due to suspicion of a banking fine and suspected continued decrease in oil prices.
  do_you_have_a_google_graph_analyzing_your_project_: Yes
  if_you_have_a_graph_analyzing_your_project_include_the_html_for_a_google_graph_here: <html><p>  <head><p>    <script type=text/javascript src=https://www.google.com/jsapi></script><p>    <script type=text/javascript><p>      google.load(visualization, 1, {packages&#58;[corechart]});<p>      google.setOnLoadCallback(drawChart);<p>      function drawChart() {<p>        var data = google.visualization.arrayToDataTable([<p>          ['Data Label', '&#37;'],<p>          ['Relevant',     0.60],<p>          ['Irrelevant',      0.40]<p>        ]);<p>        var options = {<p>          title&#58; 'Relevant vs. Irrelevant Data'<p>        };<p>        var chart = new google.visualization.PieChart(document.getElementById('piechart'));<p>        chart.draw(data, options);<p>      }<p>    </script><p>  </head><p>  <body><p>    <div id=piechart style=width&#58; 900px; height&#58; 500px;></div><p>  </body><p></html>
  caption_for_your_google_graph_(project_analysis): Relevant vs. Irrelevant Data
  what_were_the_biggest_challenges_that_you_had_to_deal_with: To our surprise, one of the biggest challenges was dealing with Crowdflower. We experienced many seemingly random errors with the website not being able to find our job after creation. Additionally, it was fairly difficult to properly motivate workers into providing trusted answers, especially for the job dealing with the sentiment analysis of articles. Another challenge that arose was finding an easily accessible source of news articles via a free API that also provided ticker symbols. We settled on Yahoo News, but even it limits a user's ability to retrieve historical articles. If we were to implement this project as a business, we would instead build a full&#45;fledged web crawler capable of pulling articles from any website and tagging ticker symbols.
  where_there_major_changes_between_what_you_originally_proposed_and_your_final_product: No
  if_so_what_changed_between_your_original_plan_and_your_final_product: 
  what_are_some_limitations_of_your_product: The potential fundamental limitation of our project is that most articles and tweets about a company might merely be reactions to official statements by a company which would certainly already be reflected in the stock price. However, as discussed previously, the project did appear to provide at least some value in predicting future stock prices. A potential improvement would be to limit to companies that have consumer products, since consumer sentiment is something that almost certainly precedes company performance.<p>An additional limitation of our implementation in particular is that we only collected data for 20 companies over a time period of approximately 2 weeks. This could easily be improved in a follow&#45;up implementation, but it is worth noting that it is a limitation of the project in its current form.
  did_your_results_deviate_from_what_you_would_expect_from_previous_work_or_what_you_learned_in_the_class: No
  if_your_results_deviated_why_might_that_be: 
  caption_for_your_graph_(scaling_up): Runtime vs. # Ticker Symbols
  caption_for_your_graph_(aggregation): Aggregate Market Sentiment and Aggregate Sentiment Across Companies
  caption_for_your_graph_(quality): Percent Agreement Between Workers
  caption_for_your_graph_(skills): Test Question Performance
  caption_for_your_graph_(incentives): Trusted vs. Untrusted Judgments Before and After Increasing Financial Incentive
  is_there_anything_else_youd_like_to_say_about_your_project: Out Aggregate Sentiment Across Companies Chart  shows the allocation of positive, negative, and neutral labels across each of the twenty companies examined. This seems to lead to market&#45;relevant conclusions. For instance, AAPL has the highest percentage positive, and the stock has also been performing really well due to improved operational and financial strategy under the guidance of activist investor Icahn. On the other hand, XOM has the lowest &#37; positive, and it has been doing relatively poorly especially in light of the recent oil price pressure. It would also be interesting to implement this project using machine learning for sentiment analysis instead of crowdsourcing and compare the results. Overall, MarketChatter has been successful in allowing perspective into public sentiment for various stocks. 
  did_you_train_a_machine_learning_component_from_what_the_crowd_gave_you: No
  if_you_trained_a_machine_learning_component_describe_what_you_did: 
  did_you_analyze_the_quality_of_the_machine_learning_component: 
  if_you_have_a_graph_analyzing_a_machine_learning_component_include_the_html_for_a_google_graph_here: 
  caption_for_your_graph_(incentives): 
  caption_for_your_graph_(machine_learning_component): 
  what_was_the_main_focus_of_your_teams_effort: Something in between
  provide_a_link_to_your_final_presentation_video: 
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_skills_: 
  what_incentivi: 
-
  how_many_teammates_are_in_your_group: 4
  name_1: Ethan Abramson
  name_2: Varun Agarwal
  name_3: Rohan Bopardikar
  name_4: Shreshth Khilani
  pennkey_username_1: etha
  pennkey_username_2: agarwalv
  pennkey_username_3: rohanb
  pennkey_username_4: shreshth
  name_of_your_project: Fud&#45;Fud
  give_a_one_sentence_description_of_your_project: Fud&#45;Fud leverages the crowd to form a food truck delivery service.
  url_to_the_logo_for_your_project: https://github.com/nflethana/fudfud/blob/master/node&#45;app/logo.png
  what_similar_projects_exist: FoodToEat is somewhat similar in that it helps food trucks process delivery orders.<p>Post mates offers delivery for restaurants and services that do not offer it traditionally, but ours is the first crowd&#45;sourced based service targeting food trucks specifically. <p>
  what_type_of_project_is_it: A business idea that uses crowdsourcing
  how_do_you_incentivize_the_crowd_to_participate: Users will either receive monetary compensation in the case of runners or the satisfaction of delivered food from their favorite food truck in the case of eaters. In terms of financial compensation, the runner and the eater will work out an appropriate dollar amount to pay when they get in touch, in addition to the cost of the food. Generally, given the relatively cheap cost of food at food trucks, we expect the compensation to be between $2&#45;$5. Eaters will definitely be incentivized to participate because for a small premium, they will not have to leave their current location to get the tasty food truck food &#45;&#45; something that will be especially valuable during the hot summer or upcoming cold winter months. 
  what_does_the_crowd_provide_for_you: They provide delivery of food from local food trucks, and the demand for such delivery.<p>
  how_do_you_ensure_the_quality_of_the_crowd_provides__: Eaters will review the runners who delivered their food after it is dropped off on a star rating scale from one to five.  If the rating is a two or below the runner will be ‘blacklisted’ for that eater, and the two will never be paired again. If a runner has a rating of 2 or below on average after multiple runs, he will automatically be blacklisted for all users on the site. <p>
  how_do_you_aggregate_the_results_from_the_crowd: We aggregated results from the crowd based on where the runner was delivering to, what trucks he was delivering from, and his estimated time of arrival. 
  how_does_your_project_work: As an eater&#58;  You go to the site and click on the location to which you would like food delivered.  From there you can select from the list which runner you would like to deliver your food.  You can then call them or select them on the website and place an order. You are then able to review your experience with this delivery person, and rate them accordingly. <p>As a runner&#58;  You post a new Fud Run to the website and wait for users to contact you with their orders.  You then pick up their food, deliver it to their location, and accept the monetary compensation, while trying to increase your rating on the site.
  would_your_project_benefit_if_you_could_get_contributions_from_thousands_of_people: Yes
  vimeo_link: http://vimeo.com/114519682
  pennkey_username_1: 
  may_we_have_your_permission_to_feature_your_project_on_the_class_website: Yes
  team_member_1__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_2__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_3__can_we_list_your_name_listed_alongside_your_project: No
  team_member_4__can_we_list_your_name_listed_alongside_your_project: Yes
  who_are_the_members_of_your_crowd: Penn students, faculty, and other local Philadelphia residents.
  how_many_unique_participants_did_you_have: 6
  for_your_final_project_did_you_simulate_the_crowd_or_run_a_real_experiment: Simulated crowd
  if_the_crowd_was_simulated_how_did_you_collect_this_set_of_data: We simulated the crowd by creating our own data and testing the integrity of site functions by simulating its use between the group members.<p>
  if_the_crowd_was_simulated_how_would_you_change_things_to_use_a_real_crowd: To change things to use a real crowd all we would have to do is get them to sign up.  The platform works as a stand alone application, and we wouldn’t need to do anything new from a technical perspective.
  how_do_you_ensure_the_quality_of_the_crowd_provides: 
  what_motivation_does_the_crowd_have_for_participating_in_your_project: Pay
  did_you_perform_any_analysis_comparing_different_incentives_: Yes
  if_you_compared_different_incentives_what_analysis_did_you_perform_: We had to think carefully about how to add an additional delivery fee.  We could have added a flat rate (like our competitor postmates that charges a $10 flat fee).  In the end, we decided that it would be best to let our users determine the delivery fee as they should be able to reach the market clearing price over the phone or venmo.<p>
  if_you_have_a_graph_analyzing_incentives_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_incentives_: 
  do_your_crowd_workers_need_specialized_skills: No
  what_sort_of_skills_do_they_need: To be hungry people, or profit seeking depending on the type of user.
  if_skills_vary_widely_what_factors_cause_one_person_to_be_better_than_another_: 
  did_you_analyze_the_skills_of_the_crowd: No
  if_you_analyzed_skills_what_analysis_did_you_perform: 
  do_you_have_a_google_graph_analyzing_skills_: No
  if_you_have_a_graph_analyzing_skills_include_the_html_for_a_google_graph_here: 
  is_the_quality_of_what_the_crowd_gives_you_a_concern: Yes
  did_you_create_a_user_interface_for_the_crowd_workers: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_crowdfacing_user_interface: https://github.com/nflethana/fudfud/blob/master/screenshot1.jpg<p>
  describe_your_crowdfacing_user_interface: The user interface is designed to be clean and easy to use.  On the left the user can see all of the available delivery locations.  On the right the user can choose to create a new food run.  The ease of use of our site was the main concern, and it drove the incorporation of menus, the venmo api, and part of bootstrap’s theme.<p>
  did_you_analyze_the_aggregated_results_: No
  what_analysis_did_you_perform_on_the_aggregated_results: 
  did_you_analyze_the_quality_of_what_you_got_back: No
  what_analysis_did_you_perform_on_the_aggregated_results: We did not analyze the aggregation, although if we had a large user base this would be beneficial to better breakdown delivery locations on Penn's campus.
  what_analysis_did_you_perform_on_quality: 
  do_you_have_a_google_graph_analyzing_quality_: No
  if_you_have_a_graph_analyzing_quality_include_the_html_for_a_google_graph_here: 
  is_this_something_that_could_be_automated: No
  if_it_could_be_automated_say_how__if_it_is_difficult_or_impossible_to_automate_say_why: There’s no way to profitably automate what delivery users do.  In addition, an automated user base for a demand for orders would make no sense.<p>
  do_the_skills_of_individual_workers_vary_widely: No
  do_you_have_a_google_graph_analyzing_the_aggregated_results_: No
  if_you_have_a_graph_analyzing_the_aggregated_results_include_the_html_for_a_google_graph_here: 
  did_you_create_a_user_interface_for_the_end_users_to_see_the_aggregated_results: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_user_interface_for_the_end_user: https://github.com/nflethana/fudfud/blob/master/aggregationScreenshot.png<p>
  describe_what_your_end_user_sees_in_this_interface: For the end user, the interface is very clean, and aggregation is done by splitting food runs into the possible delivery location first.  It then provides users with the food trucks that runners will visit.  It also provides the user with the ratings of the person doing the food run.<p>
  what_is_the_scale_of_the_problem_that_you_are_trying_to_solve_: The problem we are trying to solve is specifically tailored to the Penn and City of Philadelphia community.  The solution, however, is more broadly applicable to any restaurant that does not currently offer a delivery service.  The scale, therefore, could be potentially massive if this were to catch on nation&#45;wide.  This scale, however, could be flawlessly managed.<p>
  if_it_would_benefit_from_a_huge_crowd_how_would_it_benefit_: The user reviews would be more meaningful if there were enough people using it to provide us with good quality information.  In addition, the more people using the site would allow more runners to be present, which would allow for more matches to be made.<p>
  what_challenges_would_scaling_to_a_large_crowd_introduce: Scaling would introduce location based aggregation issues where we would need to specify the exact delivery location instead of just building on Penn’s campus.  Most of the scaling challenges aren’t actually technical, but changes which would make the site user friendly.  The databases are distributed, scalable cloud databases that can be accessed and updated from anywhere around the world.  The web server is an Elastic Beanstalk scaling, auto&#45;balanced server, which will seamlessly scale up and down with demand.  All of which can be done from the comfort of my bed and pajamas on my cell phone.<p>
  did_you_perform_an_analysis_about_how_to_scale_up_your_project: Yes
  what_analysis_did_you_perform_on_the_scaling_up: Yes, we considered the technical challenges of scale early on, and made specific design decisions on the database and web server end to account for these scaling issues.<p>
  do_you_have_a_google_graph_analyzing_scaling_: No
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  url_to_the_flow_diagram_for_your_project: https://github.com/nflethana/fudfud/blob/master/Screen&#37;20Shot&#37;202014&#45;12&#45;14&#37;20at&#37;2010.50.17&#37;20PM.png
  if_the_crowd_was_real_how_did_you_recruit_participants: 
  did_your_project_work: It did work.  We were able to build a scalable working product that is ready for users right now.  We even tested it out amongst our friends!<p>
  do_you_have_a_google_graph_analyzing_your_project_: No
  if_you_have_a_graph_analyzing_your_project_include_the_html_for_a_google_graph_here: 
  caption_for_your_google_graph_(project_analysis): 
  what_were_the_biggest_challenges_that_you_had_to_deal_with: Our biggest challenges were actually in group organization and planning.  We had trouble unifying the vision, and dividing tasks between group members.  We all had varied skill sets, and it took a while to figure out how to work together in the most productive way.
  where_there_major_changes_between_what_you_originally_proposed_and_your_final_product: No
  if_so_what_changed_between_your_original_plan_and_your_final_product: 
  what_are_some_limitations_of_your_product: In order to scale we would need to aggregate food runs in a more intelligent manner.  This will likely be done with GPS or other location information.  In addition, we would need to implement a revenue system to pay for the back end services that the system requires.  To effectively scale with this business model we would likely need to send notification to people on their mobile device.  There are many ways to do this, but we would have to choose and implement one of them.  The limitations of the project do exist, but they would take relatively small modifications to be able to scale out properly.<p>
  did_your_results_deviate_from_what_you_would_expect_from_previous_work_or_what_you_learned_in_the_class: No
  if_your_results_deviated_why_might_that_be: 
  caption_for_your_graph_(scaling_up): 
  caption_for_your_graph_(aggregation): 
  caption_for_your_graph_(quality): 
  caption_for_your_graph_(skills): 
  caption_for_your_graph_(incentives): 
  is_there_anything_else_youd_like_to_say_about_your_project: 
  did_you_train_a_machine_learning_component_from_what_the_crowd_gave_you: No
  if_you_trained_a_machine_learning_component_describe_what_you_did: 
  did_you_analyze_the_quality_of_the_machine_learning_component: No
  if_you_have_a_graph_analyzing_a_machine_learning_component_include_the_html_for_a_google_graph_here: 
  caption_for_your_graph_(incentives): 
  caption_for_your_graph_(machine_learning_component): 
  what_was_the_main_focus_of_your_teams_effort: Engineering a complex system
  provide_a_link_to_your_final_presentation_video: 
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_skills_: 
  what_incentivi: 
-
  how_many_teammates_are_in_your_group: 3
  name_1: Matt Labarre
  name_2: Jeremy Laskin
  name_3: Chris Holt
  name_4: 
  pennkey_username_1: mlabarre
  pennkey_username_2: jlaskin
  pennkey_username_3: holtc
  pennkey_username_4: 
  name_of_your_project: Crowdsourcing for Cash Flow
  give_a_one_sentence_description_of_your_project: Crowdsourcing for Cash Flow examines the correlation between sentiment expressed in tweets about a certain company and that company’s performance in the stock market during that time period.
  url_to_the_logo_for_your_project: https://github.com/holtc/nets/blob/master/docs/logo.pdf
  what_similar_projects_exist: There are some sentiment analysis tools available currently. These include StockFluence, The Stock Sonar, Sentdex, and SNTMNT, all of which perform sentiment analysis on Twitter or other news feeds. 
  what_type_of_project_is_it: Social science experiment with the crowd
  how_do_you_incentivize_the_crowd_to_participate: We used CrowdFlower’s payment system to incentivize the crowd to participate. Our job was very large – we had 3,969 units, and we wanted at least 3 judgments per unit. Since we wanted to minimize cost, we initially set out to pay 2 cents per job, and each job contained 10 tweets to evaluate as either negative, neutral, or positive sentiment. However, this initial payment scheme was not a great enough incentive for the crowd, and we had very low participation rates to start. After modifying our payment plan several times, we ended up paying the crowd 5 cents to evaluate 15 tweets. 
  what_does_the_crowd_provide_for_you: The crowd provided us very simple data&#58; whether the sentiment in the tweet towards Apple was positive, neutral, or negative. We wanted to keep this simple because we believed that adding options such as “very positive” and “very negative” would cause for too much subjectivity and variation amongst the crowd. We would have had to have had many more judgments per tweet in order to develop a consensus on the sentiment, and we did not believe that adding such options were necessary.
  how_do_you_ensure_the_quality_of_the_crowd_provides__: Quality was of significant concern. Since each individual task of analyzing sentiment in a tweet is quick and simple, there was concern that workers would hastily evaluate as many tweets as possible to receive more money. This concern was exacerbated by the fact that we did not pay workers much, and low paying, high volume jobs can lead to poor quality from the crowd. In order to ensure high quality results, we created 104 test questions, and deployed our job in quiz mode, where the workers had to answer a minimum of 5 quiz questions with at least 70&#37; accuracy. Additionally, each HIT had one test question in it.<p>	This quality control method proved successful. CrowdFlower’s Contributor analytics file reports a “trust_overall” score for each individual worker. The average trust was 0.85. However, this number is slightly skewed, because some workers who provided zero judgments were given trusts of 1. After filtering out these workers, we still received a high average trust of 0.79. Additionally, we calculated a weighted&#45;trust metric, where the trust_overall was multiplied by the number of judgments that the worker made, allowing us to calculate an average trust&#45;per&#45;judgment value. This value was 0.83. All of these metrics are very close in value, which points to a fairly consistent level of quality across workers. Thus, we can conclude that our quality control mechanism was successful, and maintained a high level of quality throughout the job. 
  how_do_you_aggregate_the_results_from_the_crowd: We conducted robust aggregation work on the results from the crowd. First, we read through the CrowdFlower results, and for each tweet, we assigned a negative sentiment a value of &#45;1, a neutral sentiment a value of 0, and a positive sentiment a value of +1. Then, we bucketed each tweet by the timestamp of when they were tweeted – the buckets were 30 minute intervals. For each bucket, we summed up the scores of the tweets in the bucket, assigning an overall score per bucket. Additionally, we only considered the tweets that were tweeted during stock market hours (9&#58;30am – 4pm, Mon&#45;Fri). Once the scores for each bucket were determined, we calculated the average bucket score, and the standard deviation of the bucket scores, allowing us to calculate z&#45;scores for each bucket. This normalized the data, accounting for the varying amount of tweets per bucket.
  how_does_your_project_work: Our project begins by writing a Twitter&#45;scraping script that retrieves tweets that contain #AAPL or @Apple. Once these tweets are received, we create a sentiment analysis crowdsourcing job on CrowdFlower. This job asks the crowd to evaluate the sentiment expressed in the tweet from the options of&#58; negative, neutral, positive, or “This tweet is not about the company Apple.” Once this is complete, we retrieve stock market data for AAPL over the desired time span, and perform advanced analytics to attempt to correlate the sentiment data with the market data. This involves bucketing the sentiment data bucketed by 30 minute time intervals, calculating the z&#45;scores of these buckets, and plotting these values and the similarly bucketed stock market positions, and comparing the data. We also calculated day&#45;by&#45;day correlation values to see how the sentiments compared to the movement of the stock price on a given day.
  would_your_project_benefit_if_you_could_get_contributions_from_thousands_of_people: No
  vimeo_link: https://vimeo.com/114584674
  pennkey_username_1: 
  may_we_have_your_permission_to_feature_your_project_on_the_class_website: Yes
  team_member_1__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_2__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_3__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_4__can_we_list_your_name_listed_alongside_your_project: 
  who_are_the_members_of_your_crowd: CrowdFlower participants
  how_many_unique_participants_did_you_have: 231
  for_your_final_project_did_you_simulate_the_crowd_or_run_a_real_experiment: Real crowd
  if_the_crowd_was_simulated_how_did_you_collect_this_set_of_data: 
  if_the_crowd_was_simulated_how_would_you_change_things_to_use_a_real_crowd: 
  how_do_you_ensure_the_quality_of_the_crowd_provides: 
  what_motivation_does_the_crowd_have_for_participating_in_your_project: Pay
  did_you_perform_any_analysis_comparing_different_incentives_: Yes
  if_you_compared_different_incentives_what_analysis_did_you_perform_: We did not perform a rigorous analysis, but we actively monitored how the crowd responded to different payment options. We had to vary our price per HIT and the number of tweets per HIT many times before we finally found an effective incentive plan.
  if_you_have_a_graph_analyzing_incentives_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_incentives_: 
  do_your_crowd_workers_need_specialized_skills: No
  what_sort_of_skills_do_they_need: We targeted our job towards members of CrowdFlower in the U.S. so that they were fluent in English and could understand the tweets well. This is the only skill necessary. 
  if_skills_vary_widely_what_factors_cause_one_person_to_be_better_than_another_: 
  did_you_analyze_the_skills_of_the_crowd: No
  if_you_analyzed_skills_what_analysis_did_you_perform: 
  do_you_have_a_google_graph_analyzing_skills_: 
  if_you_have_a_graph_analyzing_skills_include_the_html_for_a_google_graph_here: 
  is_the_quality_of_what_the_crowd_gives_you_a_concern: Yes
  did_you_create_a_user_interface_for_the_crowd_workers: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_crowdfacing_user_interface: https://github.com/holtc/nets/blob/master/docs/Screen&#37;20Shot&#37;202014&#45;12&#45;15&#37;20at&#37;201.09.46&#37;20AM.tiff<p>https://github.com/holtc/nets/blob/master/docs/Screen&#37;20Shot&#37;202014&#45;12&#45;15&#37;20at&#37;201.10.01&#37;20AM.tiff
  describe_your_crowdfacing_user_interface: We kept our interface simple to make it as easy as possible for the crowd. We hoped that simple and concise interfaces and instructions would heighten the accuracy of the crowd’s work. We also wrote robust definitions for what constitutes a positive, negative, and neutral tweets, again to heighten accuracy. 
  did_you_analyze_the_aggregated_results_: Yes
  what_analysis_did_you_perform_on_the_aggregated_results: 
  did_you_analyze_the_quality_of_what_you_got_back: Yes
  what_analysis_did_you_perform_on_the_aggregated_results: The analysis performed on the aggregated results is described above. 
  what_analysis_did_you_perform_on_quality: In addition to the analysis described above, wherein we calculated the average trust per judgement, and the average trust overall, we calculated a few other metrics to ensure that our quality was sufficient. First, we plotted a workers trust against his number of judgements, to visually observe the distribution of the quality of trust across workers. From this plot, we were able to see that workers who made a lot of judgments had higher trust scores. All trust scores below 0.6667 corresponded to workers who submitted at most 120 judgments. Additionally, the workers who submitted the most judgments (919 and 1015), had trust scores of 0.913 and 0.8356, respectively.<p>Finally, we analyzed the aggregated csv file generated by CrowdFlower to obtain further data on the quality of our results. This file contains a sentiment&#58;confidence column, where for each tweet and sentiment, it calculates a confidence parameter denoting how confident CrowdFlower is in the accuracy of the sentiment label. We found the average confidence for each sentiment (positive, neutral, and negative), and graphed them. The average confidences were all high.<p>From all of the analysis that we conducted on our data, it was clear that the quality of our results was strong.
  do_you_have_a_google_graph_analyzing_quality_: Yes
  if_you_have_a_graph_analyzing_quality_include_the_html_for_a_google_graph_here: f = open('worker_quality.html', 'w')<p><p>message = '''<html><p>  <head><p>    <script type=text/javascript src=https://www.google.com/jsapi></script><p>    <script type=text/javascript><p>      google.load(visualization, 1, {packages&#58;[corechart]});<p>      google.setOnLoadCallback(drawChart);<p>      function drawChart() {<p>        var data = google.visualization.arrayToDataTable([<p>          ['Number of Judgments', 'Trust Overall'],<p>          [50, 0.875],<p>[4, 0.75],<p>[5, 0.4],<p>[66, 1.0],<p>[20, 0.6667],<p>[10, 0.9],<p>[35, 0.8571],<p>[140, 0.9286],<p>[5, 0.6],<p>[445, 0.8462],<p>[126, 0.2222],<p>[20, 0.6667],<p>[5, 0.8],<p>[5, 0.6],<p>[140, 0.7857],<p>[485, 0.9189],<p>[5, 1.0],<p>[4, 0.6667],<p>[76, 0.7],<p>[110, 0.9167],<p>[12, 1.0],<p>[18, 1.0],<p>[485, 0.9459],<p>[12, 0.0],<p>[12, 1.0],<p>[12, 1.0],<p>[35, 1.0],<p>[5, 1.0],<p>[80, 0.7],<p>[140, 0.8571],<p>[290, 0.875],<p>[4, 0.75],<p>[5, 1.0],<p>[400, 0.7778],<p>[444, 0.7143],<p>[53, 0.8889],<p>[5, 1.0],<p>[20, 0.6667],<p>[27, 1.0],<p>[126, 0.3333],<p>[5, 1.0],<p>[40, 0.8333],<p>[72, 0.8333],<p>[485, 0.9189],<p>[20, 1.0],<p>[12, 1.0],<p>[20, 0.6667],<p>[290, 0.9167],<p>[18, 0.0],<p>[114, 0.8571],<p>[20, 0.8333],<p>[24, 1.0],<p>[50, 0.75],<p>[24, 1.0],<p>[5, 0.8],<p>[537, 0.9286],<p>[36, 0.5],<p>[485, 0.9459],<p>[140, 0.7857],<p>[5, 0.6],<p>[18, 1.0],<p>[140, 0.8571],<p>[25, 0.9091],<p>[35, 0.8571],<p>[42, 1.0],<p>[42, 0.6667],<p>[5, 1.0],<p>[80, 1.0],<p>[125, 1.0],<p>[5, 0.6],<p>[20, 0.6667],<p>[130, 0.7222],<p>[20, 1.0],<p>[19, 0.8],<p>[5, 0.0],<p>[10, 0.9],<p>[50, 0.875],<p>[35, 0.8571],<p>[72, 0.4],<p>[64, 1.0],<p>[50, 0.875],<p>[66, 1.0],<p>[15, 1.0],<p>[95, 1.0],<p>[5, 0.6],<p>[200, 0.8333],<p>[5, 0.6],<p>[222, 0.75],<p>[5, 1.0],<p>[50, 0.75],<p>[20, 0.8333],<p>[80, 0.9],<p>[120, 0.4286],<p>[42, 0.6667],<p>[5, 0.6],<p>[95, 0.9091],<p>[5, 0.4],<p>[12, 0.0],<p>[50, 0.75],<p>[16, 0.8],<p>[290, 0.875],<p>[20, 0.6667],<p>[330, 0.6957],<p>[5, 0.4],<p>[171, 0.75],<p>[5, 0.8],<p>[24, 0.0],<p>[5, 1.0],<p>[110, 0.9167],<p>[425, 0.7879],<p>[28, 0.6667],<p>[5, 0.6],<p>[12, 1.0],<p>[20, 0.8333],<p>[5, 1.0],<p>[35, 0.8571],<p>[5, 0.8],<p>[42, 1.0],<p>[485, 0.9459],<p>[50, 0.875],<p>[35, 0.8571],<p>[12, 1.0],<p>[140, 0.7857],<p>[36, 0.3333],<p>[12, 1.0],<p>[60, 0.8],<p>[558, 0.8095],<p>[5, 0.8],<p>[365, 0.7931],<p>[20, 0.6667],<p>[286, 0.9583],<p>[24, 1.0],<p>[66, 1.0],<p>[7, 1.0],<p>[20, 0.8333],<p>[65, 0.8889],<p>[16, 0.8],<p>[20, 0.8333],<p>[36, 1.0],<p>[48, 0.6667],<p>[84, 0.7143],<p>[170, 0.875],<p>[36, 0.6667],<p>[95, 0.8182],<p>[60, 0.6],<p>[1015, 0.8356],<p>[592, 0.8222],<p>[120, 0.2],<p>[25, 0.7273],<p>[12, 1.0],<p>[12, 1.0],<p>[20, 1.0],<p>[485, 0.8378],<p>[12, 1.0],<p>[5, 1.0],<p>[919, 0.913],<p>[24, 1.0],<p>[365, 0.7586],<p>[30, 1.0],<p>[490, 0.8333],<p>[484, 0.9286],<p>[42, 0.6667],<p>[100, 0.7778],<p>[35, 0.7143],<p>[170, 0.9375],<p>[36, 1.0],<p>[12, 1.0],<p>[24, 0.5],<p>[5, 0.8],<p>[25, 0.7273],<p>[5, 0.6],<p>[18, 0.0],<p>[12, 0.0],<p>[60, 1.0],<p>[120, 0.5556],<p>[392, 0.8788],<p>[30, 1.0],<p>[12, 1.0],<p>[24, 0.5],<p>[24, 1.0],<p>[63, 0.75],<p>[34, 0.8],<p>[65, 1.0],<p>[39, 0.6667]<p>        ]);<p>        var options = {<p>          title&#58; 'Worker Trust vs. Number of Judgments',<p>          hAxis&#58; {title&#58; 'Number of Judgments', minValue&#58; 0, maxValue&#58; 15},<p>          vAxis&#58; {title&#58; 'Trust Overall', minValue&#58; 0, maxValue&#58; 1},<p>          legend&#58; 'none'<p>        };<p>        var chart = new google.visualization.ScatterChart(document.getElementById('chart_div'));<p>        chart.draw(data, options);<p>      }<p>    </script><p>  </head><p>  <body><p>    <div id=chart_div style=width&#58; 900px; height&#58; 500px;></div><p>  </body><p></html>'''<p>message2 = '''<html><p>  <head><p>    <script type=text/javascript src=https://www.google.com/jsapi></script><p>    <script type=text/javascript><p>      google.load(visualization, 1, {packages&#58;[corechart]});<p>      google.setOnLoadCallback(drawChart);<p>      function drawChart() {<p>        var data = google.visualization.arrayToDataTable([<p>          ['Sentiment', 'Average Confidence'],<p>          ['Positive', 0.860],<p>          ['Negative', 0.789],<p>          ['Neutral', 0.826]          <p>        ]);<p>        var options = {<p>          title&#58; 'Average Confidence by Sentiment',<p>          vAxis&#58; {title&#58; 'Confidence', minValue&#58; 0, maxValue&#58; 1.0},<p>          hAxis&#58; {title&#58; 'Sentiment'}<p>        };<p>        var chart2 = new google.visualization.ColumnChart(document.getElementById('chart_div2'));<p>        chart2.draw(data, options);<p>      }<p>    </script><p>  </head><p>  <body><p>    <div id=chart_div2 style=width&#58; 900px; height&#58; 500px;></div><p>  </body><p></html><p>'''<p>f.write(message + \n + message2)<p>f.close()
  is_this_something_that_could_be_automated: No
  if_it_could_be_automated_say_how__if_it_is_difficult_or_impossible_to_automate_say_why: Yes, we could automate this process, but that would involve creating a sentiment analysis platform, which is very difficult for vocabulary like that seen in stock articles and tweets.  Additionally, we would still need to manually label large amounts of training data, which would have been impossible given the time limits.  We could use twitter queries to pre&#45;sort positive and negative tweets for the stocks, but then there would be no crowd aspect to this particular project.  
  do_the_skills_of_individual_workers_vary_widely: No
  do_you_have_a_google_graph_analyzing_the_aggregated_results_: No
  if_you_have_a_graph_analyzing_the_aggregated_results_include_the_html_for_a_google_graph_here: The graphs that analyzed our results were produced by the aggregation code that we wrote. 
  did_you_create_a_user_interface_for_the_end_users_to_see_the_aggregated_results: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_user_interface_for_the_end_user: https://github.com/holtc/nets/blob/master/docs/Screen&#37;20Shot&#37;202014&#45;12&#45;15&#37;20at&#37;2011.15.20&#37;20PM.tiff<p>https://github.com/holtc/nets/blob/master/docs/Screen&#37;20Shot&#37;202014&#45;12&#45;15&#37;20at&#37;2011.15.42&#37;20PM.tiff
  describe_what_your_end_user_sees_in_this_interface: These are two graphs, one of the z&#45;score plot and one of the AAPL stock market data. We did not plot the scores for any tweets that were published outside of stock market hours, which explains the gaps between 4&#58;30 PM and 9 AM and on the weekend of December 6 and seventh. It is clear from these graphs that the twitter sentiments are not heavily correlated with the stock market data, but there are general trends that can be observed that relate the two datasets. For example on December 8, the twitter sentiments were quite negative, and the stock market performance that day was negative as well. We can see similar correlations on december 4th and 5th
  what_is_the_scale_of_the_problem_that_you_are_trying_to_solve_: Ultimately, the problem that is being tackled is discovering the correlation between tweets about a company and its performance in the stock market. In this project, we simply looked at standard tweets from standard Twitter users; however, this concept can be scaled greatly to look at specific types of tweets, and who is tweeting them. For example, our project did not ask our crowd to read articles linked in tweets, and judge the sentiment of such articles. Additionally, we did not weight tweet sentiment for users that were news sources or prominent figures in the financial industry (Carl Icahn, Warren Buffet, etc.). We simply looked at standard Twitter users who were primarily tweeting about personal complaints (or positive experiences) with Apple, which, understandably, is not very tightly correlated to stock market movement. Scaling this project to include more companies and collecting a more specialized set of tweets can make for an interesting study on the correlation of tweets to stock market movement on a large scale. 
  if_it_would_benefit_from_a_huge_crowd_how_would_it_benefit_: 
  what_challenges_would_scaling_to_a_large_crowd_introduce: 
  did_you_perform_an_analysis_about_how_to_scale_up_your_project: Yes
  what_analysis_did_you_perform_on_the_scaling_up: Essentially, we just examined the possibility of looking at more companies (companies that are competitors), and the possibility of conducting this study over extended periods of time. For the idea of studying multiple companies, we simply multiplied our costs from this study of Apple by the number of companies to be studied. For conducting this study over an extended period of time, we considered the fact that for our current study, which was over 9 days, we collected 4000 tweets, which is an average of roughly 450 tweets per day. This can be multiplied by the number of days in the extended study, and then a cost analysis can be deduced using the cost of our current study.
  do_you_have_a_google_graph_analyzing_scaling_: 
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  url_to_the_flow_diagram_for_your_project: https://github.com/holtc/nets/blob/master/docs/Flow&#37;20Chart.pdf
  if_the_crowd_was_real_how_did_you_recruit_participants: We used CrowdFlower, which recruited participants for us
  did_your_project_work: Our project did work, even though the exact results did not necessarily point to tight correlation. We were able to utilize crowdsourcing to get sentiment analysis on nearly 4000 tweets about Apple, and then write programs to synthesize this data, and correlate it to Apple’s performance in the stock market. The success of this project was not to be measured by whether or not the correlation existed, but rather if a valid analysis could be conducted, specifically, via crowdsourcing. In our case, we conducted a very valid analysis – no aspect of our project contains skewed or inaccurate data or analytical methods. Therefore, even though our results did not point to significant correlations, we have a very good idea as to how we can repeat the study and obtain a more interesting outcome. 
  do_you_have_a_google_graph_analyzing_your_project_: No
  if_you_have_a_graph_analyzing_your_project_include_the_html_for_a_google_graph_here: 
  caption_for_your_google_graph_(project_analysis): 
  what_were_the_biggest_challenges_that_you_had_to_deal_with: One of the biggest challenges that we had to deal with was restructuring our project slightly when we ran into significant issues with CrowdFlower. Initially, we wanted to post three jobs on CrowdFlower to study three different companies (Apple, Microsoft, and Google), and, since these companies are competitors, conduct further analysis on how tweets about one company affected the stock market performance of another. Unfortunately, our CrowdFlower subscriptions expired after Apple’s job ran, and, after exploring the possibility of positing the other two jobs on MTurk, it was clear that this was not a viable option, and would taint the quality of our data. Therefore, we switched gears slightly, and decided to just conduct a more robust analysis on Apple’s tweets and market performance.  Additionally, we depended on the twitter search queries to get a representative section of tweets about the stocks, but our queries may not have been robust or varied enough to get enough tweets about the stocks.
  where_there_major_changes_between_what_you_originally_proposed_and_your_final_product: No
  if_so_what_changed_between_your_original_plan_and_your_final_product: The change that we made to our project was not major, we were just unfortunately unable to repeat the analysis on several companies, and had to stick to simply one. 
  what_are_some_limitations_of_your_product: We do not believe that our project consists of many sources of error. The only possible source of error would be inaccurate sentiment analysis from the crowd, but we implemented a strong quality control method that returned highly trusted results, according to CrowdFlower. The analytics that we performed on the CrowdFlower data was very standard, and did not introduce new sources of error. However, we would have liked to have either conducted this study on a longer time scale, or on multiple companies, to have obtained more data and thus validate our results further.  Additionally, we could improve the twitter search queries to get more relevant results, as well as collecting a much longer duration of tweets.  Furthermore, we could try correlating current stock prices to past twitter sentiment, with the idea that it takes some time for the sentiment to effect market prices due to trading delays.
  did_your_results_deviate_from_what_you_would_expect_from_previous_work_or_what_you_learned_in_the_class: No
  if_your_results_deviated_why_might_that_be: 
  caption_for_your_graph_(scaling_up): 
  caption_for_your_graph_(aggregation): 
  caption_for_your_graph_(quality): 
  caption_for_your_graph_(skills): 
  caption_for_your_graph_(incentives): 
  is_there_anything_else_youd_like_to_say_about_your_project: 
  did_you_train_a_machine_learning_component_from_what_the_crowd_gave_you: No
  if_you_trained_a_machine_learning_component_describe_what_you_did: 
  did_you_analyze_the_quality_of_the_machine_learning_component: 
  if_you_have_a_graph_analyzing_a_machine_learning_component_include_the_html_for_a_google_graph_here: 
  caption_for_your_graph_(incentives): 
  caption_for_your_graph_(machine_learning_component): 
  what_was_the_main_focus_of_your_teams_effort: Conducting an in depth analysis of data
  provide_a_link_to_your_final_presentation_video: 
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_skills_: 
  what_incentivi: 
-
  how_many_teammates_are_in_your_group: 4
  name_1: Rohan Bopardikar
  name_2: Varun Agarwal
  name_3: Shreshth Kilani
  name_4: Ethan Abramson 
  pennkey_username_1: rohanb
  pennkey_username_2: agarwalv
  pennkey_username_3: shreshth
  pennkey_username_4: etha
  name_of_your_project: Füd Füd
  give_a_one_sentence_description_of_your_project: Fud Fud leverages the crowd to form a food truck delivery service.
  url_to_the_logo_for_your_project: https://github.com/nflethana/fudfud/blob/master/node&#45;app/logo.png
  what_similar_projects_exist: FoodToEat is somewhat similar in that it helps food trucks process delivery orders. Post mates offers delivery for restaurants and services that do not offer it traditionally, but ours is the first crowd&#45;sourced based service targeting food trucks specifically.<p>
  what_type_of_project_is_it: A business idea that uses crowdsourcing
  how_do_you_incentivize_the_crowd_to_participate: Users will either receive monetary compensation in the case of runners or the satisfaction of delivered food from their favorite food truck in the case of eaters. In terms of financial compensation, the runner and the eater will work out an appropriate dollar amount to pay when they get in touch, in addition to the cost of the food. Generally, given the relatively cheap cost of food at food trucks, we expect the compensation to be between $2&#45;$5. <p>Eaters will definitely be incentivized to participate because for a small premium, they will not have to leave their current location to get the tasty food truck food &#45;&#45; something that will be especially valuable during the hot summer or upcoming cold winter months, and during lunch/dinner hours when the lines at these trucks are generally quite long. 
  what_does_the_crowd_provide_for_you: They provide delivery of food from local food trucks, and the demand for such delivery.
  how_do_you_ensure_the_quality_of_the_crowd_provides__: Eaters will review the runners who delivered their food after it is dropped off on a star rating scale from one to five.  If the rating is a two or below the runner will be ‘blacklisted’ for that eater, and the two will never be paired again. If a runner has a rating of 2 or below on average after multiple runs, he will automatically be blacklisted for all users on the site. <p>
  how_do_you_aggregate_the_results_from_the_crowd: We aggregated results from the crowd based on where the runner was delivering to, what trucks he was delivering from, and his estimated time of arrival. 
  how_does_your_project_work: As an eater&#58;  You go to the site and click on the location to which you would like food delivered.  From there you can select from the list which runner you would like to deliver your food.  You can then call them or select them on the website and place an order, paying with the venmo API. You are then able to review your experience with this delivery person, and rate them accordingly. <p>As a runner&#58;  You post a new Fud Run to the website and wait for users to contact you with their orders.  You then pick up their food, deliver it to their location, and accept the monetary compensation, while trying to increase your rating on the site.<p>The aggregation is done automatically, so eaters can view runners based on where they are delivering to, the trucks they are delivering from, and the time at which they intend to arrive with the food. An example can be seen in the PDF walkthrough.<p>
  would_your_project_benefit_if_you_could_get_contributions_from_thousands_of_people: Yes
  vimeo_link: http://vimeo.com/114519682
  pennkey_username_1: 
  may_we_have_your_permission_to_feature_your_project_on_the_class_website: Yes
  team_member_1__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_2__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_3__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_4__can_we_list_your_name_listed_alongside_your_project: Yes
  who_are_the_members_of_your_crowd: Penn students, faculty, and other local Philadelphia residents.
  how_many_unique_participants_did_you_have: 4
  for_your_final_project_did_you_simulate_the_crowd_or_run_a_real_experiment: Simulated crowd
  if_the_crowd_was_simulated_how_did_you_collect_this_set_of_data: We simulated the crowd by creating our own data and testing the integrity of site functions by simulating its use between the group members.<p>
  if_the_crowd_was_simulated_how_would_you_change_things_to_use_a_real_crowd: To change things to use a real crowd all we would have to do is get them to sign up.  The platform works as a stand alone application, and we wouldn’t need to do anything new from a technical perspective, as the application will scale The larger the real crowd, the more options eaters have to select delivery from &#45;&#45; and as such the quality of our service improves to the point where whenever an eater wants food at a specific time, there's always some runner delivering to the location at that time with the food in mind. 
  how_do_you_ensure_the_quality_of_the_crowd_provides: 
  what_motivation_does_the_crowd_have_for_participating_in_your_project: Pay
  did_you_perform_any_analysis_comparing_different_incentives_: Yes
  if_you_compared_different_incentives_what_analysis_did_you_perform_: We had to think carefully about how to add an additional delivery fee.  We could have added a flat rate (like our competitor postmates that charges a $10 flat fee).  In the end, we decided that because of the varying cost of meals at food trucks, it would be best to let our users determine the delivery fee as they should be able to reach the market clearing price over the phone or venmo. We found that generally a $2&#45;$5 delivery fee would be reasonable compensation for runners. In any case, if the runner feels the compensation is not adequate, the eater and him can get in contact via phone (the numbers are displayed on the site), or he can simply reject the transaction via venmo. In the future, we would like to build out a system where prices can be negotiated on more easily on the website rather than through venmo to ensure a fluid user experience. <p>
  if_you_have_a_graph_analyzing_incentives_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_incentives_: 
  do_your_crowd_workers_need_specialized_skills: No
  what_sort_of_skills_do_they_need: They just need to be able to use the application &#45;&#45; enter in the relevant information, and deliver the food accordingly if they are a runner. 
  if_skills_vary_widely_what_factors_cause_one_person_to_be_better_than_another_: 
  did_you_analyze_the_skills_of_the_crowd: Yes
  if_you_analyzed_skills_what_analysis_did_you_perform: We analyzed the skills of the crowd purely based on quality. After a run, the eater can rate the runner on a scale of 1&#45;5. If he rates the runner a 2 or below, he won't see that runner on the site again. If a runner's average rating falls to 2 or below after multiple runs, he will be blacklisted from the whole site. 
  do_you_have_a_google_graph_analyzing_skills_: 
  if_you_have_a_graph_analyzing_skills_include_the_html_for_a_google_graph_here: 
  is_the_quality_of_what_the_crowd_gives_you_a_concern: Yes
  did_you_create_a_user_interface_for_the_crowd_workers: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_crowdfacing_user_interface: https://github.com/nflethana/fudfud/blob/master/screen.png
  describe_your_crowdfacing_user_interface: The user interface is designed to be clean and easy to use. On the left the user can see all of the available delivery locations. He can choose a location and then a runner based on what trucks they're delivering from and their estimated time of arrival. The aggregation module allows them to view runners categorized as such. <p>On the right the user can choose to create a new food run (the screen displayed in the picture above). From there they simply enter the time at which they intend to arrive, the trucks they can deliver from, and the location they are heading to, so that all of that information can be aggregated for the eaters to view. The ease of use of our site was the main concern, and it drove the incorporation of menus, the venmo api, and part of bootstrap’s theme.<p>The PDF walkthrough includes more details on these screens. 
  did_you_analyze_the_aggregated_results_: No
  what_analysis_did_you_perform_on_the_aggregated_results: 
  did_you_analyze_the_quality_of_what_you_got_back: No
  what_analysis_did_you_perform_on_the_aggregated_results: We didn't need to analyze them; we just aggregated them for the eaters to be able to view on the website. 
  what_analysis_did_you_perform_on_quality: 
  do_you_have_a_google_graph_analyzing_quality_: No
  if_you_have_a_graph_analyzing_quality_include_the_html_for_a_google_graph_here: 
  is_this_something_that_could_be_automated: No
  if_it_could_be_automated_say_how__if_it_is_difficult_or_impossible_to_automate_say_why: There’s no way to profitably automate what delivery users do unless we use robots and self&#45;driving cards. In addition, an automated user base for a demand for orders would make no sense.
  do_the_skills_of_individual_workers_vary_widely: No
  do_you_have_a_google_graph_analyzing_the_aggregated_results_: No
  if_you_have_a_graph_analyzing_the_aggregated_results_include_the_html_for_a_google_graph_here: 
  did_you_create_a_user_interface_for_the_end_users_to_see_the_aggregated_results: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_user_interface_for_the_end_user: https://github.com/nflethana/fudfud/blob/master/aggregationScreenshot.png
  describe_what_your_end_user_sees_in_this_interface: For the end user, the interface is very clean, and aggregation is done by splitting food runs into the possible delivery location first. After clicking on the location you are currently in, users can then see the various eaters delivering to that location, based on the trucks they are delivering from and they time they intend to arrive. They can also see the runners overall rating. 
  what_is_the_scale_of_the_problem_that_you_are_trying_to_solve_: The problem we are trying to solve is specifically tailored to the Penn and City of Philadelphia community.  The solution, however, is more broadly applicable to any restaurant that does not currently offer a delivery service.  The scale, therefore, could be potentially massive if this were to catch on nation&#45;wide.  This scale, however, could be flawlessly managed.
  if_it_would_benefit_from_a_huge_crowd_how_would_it_benefit_: With more runners, theoretically, anytime an eater wanted food soon from a food truck, he could go online and have multiple people to choose from. Also, the user reviews would be more meaningful if there were enough people using it to provide us with good quality information. 
  what_challenges_would_scaling_to_a_large_crowd_introduce: Scaling would introduce location based aggregation issues where we would need to specify the exact delivery location instead of just building on Penn’s campus.  Most of the scaling challenges aren’t actually technical, but changes which would make the site user friendly.  The databases are distributed, scalable cloud databases that can be accessed and updated from anywhere around the world.  The web server is an Elastic Beanstalk scaling, auto&#45;balanced server, which will seamlessly scale up and down with demand.  All of which can be done from the comfort of my bed and pajamas on my cell phone.<p>
  did_you_perform_an_analysis_about_how_to_scale_up_your_project: No
  what_analysis_did_you_perform_on_the_scaling_up: We investigated the question of whether or not our application could handle the technical requirements of scaling up demand.  The analysis we performed was in the technical aspects of our project.  All of our databases are designed to scale flawlessly and conform to the standards of Amazon’s DynamoDB.  In addition, the web server is distributed and load&#45;balancing allowing it to adapt to different demand constraints around the world.<p>
  do_you_have_a_google_graph_analyzing_scaling_: No
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  url_to_the_flow_diagram_for_your_project: https://github.com/nflethana/fudfud/blob/master/Screen&#37;20Shot&#37;202014&#45;12&#45;14&#37;20at&#37;2010.50.17&#37;20PM.png
  if_the_crowd_was_real_how_did_you_recruit_participants: 
  did_your_project_work: It did work.  We were able to build a scalable working product that is ready for users right now.  We even tested it out amongst our friends!
  do_you_have_a_google_graph_analyzing_your_project_: No
  if_you_have_a_graph_analyzing_your_project_include_the_html_for_a_google_graph_here: 
  caption_for_your_google_graph_(project_analysis): 
  what_were_the_biggest_challenges_that_you_had_to_deal_with: Our biggest challenges were actually in group organization and planning.  We had trouble unifying the vision, and dividing tasks between group members.  We all had varied skill sets, and it took a while to figure out how to work together in the most productive way.<p>
  where_there_major_changes_between_what_you_originally_proposed_and_your_final_product: No
  if_so_what_changed_between_your_original_plan_and_your_final_product: 
  what_are_some_limitations_of_your_product: In order to scale we would need to aggregate food runs in a more intelligent manner.  This will likely be done with GPS or other location information.  In addition, we would need to implement a revenue system to pay for the back end services that the system requires.  To effectively scale with this business model we would likely need to send notification to people on their mobile device.  There are many ways to do this, but we would have to choose and implement one of them.  The limitations of the project do exist, but they would take relatively small modifications to be able to scale out properly.<p>
  did_your_results_deviate_from_what_you_would_expect_from_previous_work_or_what_you_learned_in_the_class: No
  if_your_results_deviated_why_might_that_be: 
  caption_for_your_graph_(scaling_up): 
  caption_for_your_graph_(aggregation): 
  caption_for_your_graph_(quality): 
  caption_for_your_graph_(skills): 
  caption_for_your_graph_(incentives): 
  is_there_anything_else_youd_like_to_say_about_your_project: 
  did_you_train_a_machine_learning_component_from_what_the_crowd_gave_you: No
  if_you_trained_a_machine_learning_component_describe_what_you_did: 
  did_you_analyze_the_quality_of_the_machine_learning_component: 
  if_you_have_a_graph_analyzing_a_machine_learning_component_include_the_html_for_a_google_graph_here: 
  caption_for_your_graph_(incentives): 
  caption_for_your_graph_(machine_learning_component): 
  what_was_the_main_focus_of_your_teams_effort: Engineering a complex system
  provide_a_link_to_your_final_presentation_video: 
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_skills_: 
  what_incentivi: 
-
  how_many_teammates_are_in_your_group: 2
  name_1: Jenny Hu
  name_2: Kelly Zhou
  name_3: 
  name_4: 
  pennkey_username_1: jenhu
  pennkey_username_2: zhouwe
  pennkey_username_3: 
  pennkey_username_4: 
  name_of_your_project: Quakr
  give_a_one_sentence_description_of_your_project: Quakr is an crowdsourced matchmaking tool for all of your single dating needs.
  url_to_the_logo_for_your_project: https://github.com/kelzhou/quakr/blob/master/site/quakr/images/logo.png
  what_similar_projects_exist: When we Googled crowdsourced matchmaking, one website did pop up called OnCrowdNine. It looks like user sign up and fill out a profile as well as some information and then “workers” actually recommend matches to them in exchange for a reward that the user themselves post for what they consider a good match. <p>Ours on the other hand is probably more similar to eHarmony or okCupid since we try to pair users up with other existing users in our system, but instead of a computer algorithm, we use the mechanical turkers from Crowdflower to curate matches. 
  what_type_of_project_is_it: nonbusiness&#45;application using crowdsourcing
  how_do_you_incentivize_the_crowd_to_participate: As stated before, we used crowdflower for our crowd and motivated the individual workers with a monetary incentive. For reviewing 5 pairings (one of which was a test question), we rewarded the worker 5 cents. If they completed multiple HITs and demonstrated a high trust rating, we rewarded them an additional 10 cent reward to try and encourage them to continue with our other HITs and match other pairs they hadn't seen before. 
  what_does_the_crowd_provide_for_you: The crowd provides us with 3 pieces of input for each pairing. The first is a ranking from 1 to 10, 1 being least compatible and 10 being most compatible. Next we asked them to tell us what was the main factor(s) they considered when they were ranking the pair (e.g. mutual interest, both described themselves as outgoing, etc.) Lastly, we asked the workers to tell us what other information they would've liked to see so that if we were to continue with Quakr, we would immediately have somewhere to start improvement.
  how_do_you_ensure_the_quality_of_the_crowd_provides__: Since we only had monetary incentives, other than the honor system and accuracy rating of a worker, there is always the fear that a worker or bot will try to randomly click through our task. We first minimized this, by asking workers to also give a text input at to what they considered in their rankings and information they would like to have to slow them down and prevent them from finishing within 5 seconds. The next thing we did was design test questions. We created a fake pairing that within one of the fake profiles said that something like this is a test question. Enter 8 for rank and test for factors and other information. That way we could create an actual gold standard for something that was otherwise an opinion. 
  how_do_you_aggregate_the_results_from_the_crowd: For each pair, we took the trusted rankings that we had for that pair and then averaged them over the total number of trusted rankings for that pair. We also took all of the reasons they listed to why they ranked the pair the way they did and tried our best to characterize them by interest, words that described them, type of relationship looking for, relationship experience, major, and other. 
  how_does_your_project_work: Users looking to be matched may sign up through our website. There they voluntarily submit a profile containing basic information, such as their major, their interests, pet peeves, etc. as information to be used in deciphering their matches, similar to okCupid or eHarmony. For each person, they are paired with the other users in our system. These pairing were uploaded by hand/aka by either Kelly or I to Crowdflower as a HIT.<p> <p>In addition to the regular profiles of actual users in our system, we also create fake users to act as test questions for our task. So if users workers actually read the profiles of each person, as they were suppose too, they would see that under one of the profiles was written that this was a test question and given a specific rank and text input to enter so that we knew they were reading carefully.<p>After aggregating multiple rankings of how compatible each pair was, we took the average and returned the matches that scored 7 or above to the users via their email.
  would_your_project_benefit_if_you_could_get_contributions_from_thousands_of_people: Yes
  vimeo_link: https://vimeo.com/114585311
  pennkey_username_1: 
  may_we_have_your_permission_to_feature_your_project_on_the_class_website: Yes
  team_member_1__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_2__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_3__can_we_list_your_name_listed_alongside_your_project: 
  team_member_4__can_we_list_your_name_listed_alongside_your_project: 
  who_are_the_members_of_your_crowd: Crowdflower workers
  how_many_unique_participants_did_you_have: 43
  for_your_final_project_did_you_simulate_the_crowd_or_run_a_real_experiment: Real crowd
  if_the_crowd_was_simulated_how_did_you_collect_this_set_of_data: The users, so people actually giving us their information to be matched were simulated. It was hard to generate a large amount of interest for our dating app in a short amount of time, so we polled/pressure a lot of our friends to sign up for our app.
  if_the_crowd_was_simulated_how_would_you_change_things_to_use_a_real_crowd: Since most of our friends who signed up weren't actually interested in being matched, they more so wanted their information to be kept private, so they did not actually want their full name to appear in another person's match. We respected that and displayed the match's information anonymously, but for a real crowd that would actually want to use it for matchmaking it would display their real name and an avenue for contact. 
  how_do_you_ensure_the_quality_of_the_crowd_provides: 
  what_motivation_does_the_crowd_have_for_participating_in_your_project: Pay
  did_you_perform_any_analysis_comparing_different_incentives_: No
  if_you_compared_different_incentives_what_analysis_did_you_perform_: 
  if_you_have_a_graph_analyzing_incentives_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_incentives_: 
  do_your_crowd_workers_need_specialized_skills: No
  what_sort_of_skills_do_they_need: All we really needed was a valid guess as whether two people will get along romantically or not. While we don't expect any professional matchmakers on Crowdflower, we do expect that our crowd is generally knowledgable in this are either from their own relationship experience, friends relationship experience or general people interaction.
  if_skills_vary_widely_what_factors_cause_one_person_to_be_better_than_another_: 
  did_you_analyze_the_skills_of_the_crowd: No
  if_you_analyzed_skills_what_analysis_did_you_perform: 
  do_you_have_a_google_graph_analyzing_skills_: 
  if_you_have_a_graph_analyzing_skills_include_the_html_for_a_google_graph_here: 
  is_the_quality_of_what_the_crowd_gives_you_a_concern: Yes
  did_you_create_a_user_interface_for_the_crowd_workers: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_crowdfacing_user_interface: https://github.com/kelzhou/quakr/blob/master/docs/screenshots/hit_screenshot1.png<p>https://github.com/kelzhou/quakr/blob/master/docs/screenshots/hit_screenshot2.png
  describe_your_crowdfacing_user_interface: We put the information of the two people next to each other, trying to align them as much as possible for workers to compare and give us a compatibility ranking and other short feedback.
  did_you_analyze_the_aggregated_results_: Yes
  what_analysis_did_you_perform_on_the_aggregated_results: 
  did_you_analyze_the_quality_of_what_you_got_back: Yes
  what_analysis_did_you_perform_on_the_aggregated_results: We wanted to see if the workers could return good matches and filter out bad matches. We believe that the workers certainly filtered out bad matches, since worker rankings tended to be conservative with handing out high rankings, hovering around 4&#45;6 and significantly dropped at 8 and were more hesitant to give 10s than they were 1s. The matches they did return &#45; typically one per person, sometimes 0 or 2 &#45; were ranked about 56&#37; good and 44&#37; bad by the actual users. However it was a very small sample size, so we do not think we have enough information to conclude whether the crowd truly succeeded. After looking at what workers listed for ranking factors, we broke the workers down by country and compared them against the overall population and noticed that some countries significantly consider some factors more and less than others probably from cultural differences. 
  what_analysis_did_you_perform_on_quality: After we designed our HIT, we launched about 50 HITs to see how quickly it would take to complete and get an estimate of how much it would cost. This first HIT, went pretty poorly and a very large proportion of people were failing. Many workers were failing our test questions, not because they were clicking randomly through our HIT, but because they were treating the fake profile like real people. Other people misunderstood our initial instructions as well. We lengthened our instructions and really emphasized that there were test question and not to enter their opinion for test questions. We also expanded what was considered correct since crowdflower is strict, down to the capitalization of a letter. This significantly decreased the number of units we were rejecting and increased the amount of time workers spent, we believe because the workers knew there were test questions and took more time to answer the questions. 
  do_you_have_a_google_graph_analyzing_quality_: No
  if_you_have_a_graph_analyzing_quality_include_the_html_for_a_google_graph_here: 
  is_this_something_that_could_be_automated: Yes
  if_it_could_be_automated_say_how__if_it_is_difficult_or_impossible_to_automate_say_why: We could create somekind of computer algorithm to take in people profile information and use that to generate somekind of ranking for a pair. This is after all, what matchmaking websites like okCupid and eHarmony do. This however would require making and implementing our own mathematical model for matchmaking to create these ratings. There are some papers that exist on mathematical model interest matchmaking, but it would definitely be out of the scope of the semester and our coding experience.
  do_the_skills_of_individual_workers_vary_widely: No
  do_you_have_a_google_graph_analyzing_the_aggregated_results_: No
  if_you_have_a_graph_analyzing_the_aggregated_results_include_the_html_for_a_google_graph_here: https://github.com/kelzhou/quakr/blob/master/site/india.html<p>https://github.com/kelzhou/quakr/blob/master/site/mexico.html<p>https://github.com/kelzhou/quakr/blob/master/site/unitedstates.html<p>https://github.com/kelzhou/quakr/blob/master/site/reasons.html
  did_you_create_a_user_interface_for_the_end_users_to_see_the_aggregated_results: No
  if_yes_please_give_the_url_to_a_screenshot_of_the_user_interface_for_the_end_user: 
  describe_what_your_end_user_sees_in_this_interface: Since most people we recruited to our project were our friends, we just casually messaged them or emailed them their match without the name and asked them what they thought.
  what_is_the_scale_of_the_problem_that_you_are_trying_to_solve_: The scale of problem of our problem was not too large at the moment. We had about 50 users. However, it does grow quickly, since for one new user, thats about 25 new pairs, each requiring a significant number of rankings from different turkers. 
  if_it_would_benefit_from_a_huge_crowd_how_would_it_benefit_: A huge crowd leads to a very large user base which leads to many more potential matches. The chances of being matched up with someone for each user would significantly increase. A huge crowd will also attract more crowds to sign up as the user base has a larger variety and the chances of being matched with someone is very high. 
  what_challenges_would_scaling_to_a_large_crowd_introduce: The cost of crowdsourcing will increase significantly. We would have to reconsider if it is still reason to have workers judge every potential match and or if we would still want to have 5 judgement per unit. 
  did_you_perform_an_analysis_about_how_to_scale_up_your_project: Yes
  what_analysis_did_you_perform_on_the_scaling_up: We performed a cost analysis on scaling up. We investigated how much we would have to spend on crowdsourcing as the user base size increases. We also investigated if we were to make this a paid service, how much each user would need to pay based on the number of users in the user base in order to break even. Both values grow linearly as the users increase. Each user would have more as the user base grows but the chances of being matched with someone also grows as the user base grows. 
  do_you_have_a_google_graph_analyzing_scaling_: Yes
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: https://github.com/kelzhou/quakr/blob/master/docs/scaling_up.html
  url_to_the_flow_diagram_for_your_project: https://github.com/kelzhou/quakr/blob/master/docs/flow_diagram.jpg
  if_the_crowd_was_real_how_did_you_recruit_participants: We used Crowdflower to recruit workers to rank our pairings. We provided a monetary incentive for workers to complete our task. Additionally, we tried to make our task as easy to use and understand as possible, running trial HITS and responding to the feedback that we received from workers as soon as possible. We also tried to respond to contentious as quickly as possible. To encourage some of the workers that performed well/got the highest trust value, we provided a small 5&#45;10 cent bonus. The work of actually recruited a large number of workers is largely done by Crowdflower already though.  
  did_your_project_work: The code of our project worked. We do not believe we have a large enough data set to actually conclude if the matches produced were actually good matches; however, we do believe that since workers are more likely to be conservative in their ratings, meaning more likely to rate lower than higher, it seems it would at least filter out bad matches, but possibly also filter out good matches, since a lot of our friends who were good friends with one another did not achieve a high enough rating to be considered compatible. Also are inclined that it is possible to use the aggregation of crowd rankings for matchmaking sense our project did allow us to analyze the crowd and all the conditions for a successful crowd is there&#58; independence, generally knowledgable, diversity, and decentralized.  
  do_you_have_a_google_graph_analyzing_your_project_: No
  if_you_have_a_graph_analyzing_your_project_include_the_html_for_a_google_graph_here: 
  caption_for_your_google_graph_(project_analysis): 
  what_were_the_biggest_challenges_that_you_had_to_deal_with: The largest problem we had was the HIT design, because our initial test question design was giving us a lot of rejections and bringing us plenty of complaints from workers. 
  where_there_major_changes_between_what_you_originally_proposed_and_your_final_product: Yes
  if_so_what_changed_between_your_original_plan_and_your_final_product: We ended up getting rid of the end display matches. One because we didn't actually produce a lot of matches and because a lot of our users were our friends, they did not actually want their name displayed in another person's match. So we ended up messaging our friends with their the match's information anonymously and asking their thoughts.<p>
  what_are_some_limitations_of_your_product: We were only able to generate rankings for about half of all possible pairing due to cost constraints. We also really struggled with crowdflower and felt really limited by the interface of their HIT design. For example, for a pairing we need to display the information of two people in the data that we uploaded, but Crowdflower really wants you to just choose columns and populate that with ONE random person from your dataset. So we ended up hardcoding one person into the HIT and letting crowdflower populate the other person and having one job for each male. This might've biased our results a bit since the left half was always the same profile, and thus you might be ranking matches relative to the other possible partners instead of a good or bad match independently of external factors.
  did_your_results_deviate_from_what_you_would_expect_from_previous_work_or_what_you_learned_in_the_class: No
  if_your_results_deviated_why_might_that_be: 
  caption_for_your_graph_(scaling_up): Scaling Up
  caption_for_your_graph_(aggregation): Ranking Factors Considered by workers by region
  caption_for_your_graph_(quality): 
  caption_for_your_graph_(skills): 
  caption_for_your_graph_(incentives): 
  is_there_anything_else_youd_like_to_say_about_your_project: 
  did_you_train_a_machine_learning_component_from_what_the_crowd_gave_you: No
  if_you_trained_a_machine_learning_component_describe_what_you_did: 
  did_you_analyze_the_quality_of_the_machine_learning_component: 
  if_you_have_a_graph_analyzing_a_machine_learning_component_include_the_html_for_a_google_graph_here: 
  caption_for_your_graph_(incentives): 
  caption_for_your_graph_(machine_learning_component): 
  what_was_the_main_focus_of_your_teams_effort: Something in between
  provide_a_link_to_your_final_presentation_video: 
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_skills_: 
  what_incentivi: 
-
  how_many_teammates_are_in_your_group: 3
  name_1: Ross Mechanic
  name_2: Fahim Abouelfadl
  name_3: Francesco Melpignano
  name_4: 
  pennkey_username_1: mechanic
  pennkey_username_2: fahim
  pennkey_username_3: fmelp
  pennkey_username_4: 
  name_of_your_project: PictureThis
  give_a_one_sentence_description_of_your_project: PictureThis uses crowdsourcing to have the crowd write new version of picture books. 
  url_to_the_logo_for_your_project: https://github.com/rossmechanic/PictureThis/blob/master/Screen&#37;20Shot&#37;202014&#45;12&#45;16&#37;20at&#37;206.43.10&#37;20AM.png
  what_similar_projects_exist: None. 
  what_type_of_project_is_it: Social science experiment with the crowd, Creativity tool
  how_do_you_incentivize_the_crowd_to_participate: We used monetary incentive, and over the course of our entire project, which produced 18 new versions of books, we spent about $30. But it was also important to us to make the HIT look fun and interest, and most of our contributors gave our HITs high ratings across the board.
  what_does_the_crowd_provide_for_you: The crowd writes our captions for the picture books, and the crowd also provides quality control by rating the different captions. 
  how_do_you_ensure_the_quality_of_the_crowd_provides__: Occasionally (although far more rarely than we expected), the crowd would give irrelevant answers to our questions on Crowdflower, (such as when a worker wrote his three parts of the story as children playing, children playing, and children playing). However, using the crowd to rate the captions effectively weeded out the poor quality. 
  how_do_you_aggregate_the_results_from_the_crowd: We aggregated the results manually. We did so through excel manipulation, where we would average the ratings that each worker got on their captions for each book, and then select the captions of the two highest rated workers and manually moved their captions into the excel sheet to uploaded for the next round. Realistically we could have done this using the Python API, and I spent some time learning it, but with different lengths of books and the fact that the python API returns data as a list of dictionaries rather than a CSV file, it was simply less time&#45;consuming to do it manually for only 9 books. 
  how_does_your_project_work: First, we took books from the International Children's Digital Library and separated the text from the pictures, uploading the pictures so that they each had their own unique URL to use for the Crowdflower HITs. We then posted HITs on Crowdflower that included all of the pictures, in order, from each book, and asked the workers to write captions for the first 3 pictures, given the rest of the pictures for reference (of where the story might be going). We took 6 new judgements for every book that we had. Next, for quality control, we posted another round of HITs that showed all of the judgements that had been made in the previous round, and asked the workers to rate them on a 1&#45;5 scale. We then averaged this ratings for each worker on each book, and the two workers with the highest average caption rating for a given book had their work advanced to the next round. This continued until 2 new versions of each of the 9 books we had were complete. Then, we had the crowd vote between the original version of each book, and the crowdsourced version of each book. 
  would_your_project_benefit_if_you_could_get_contributions_from_thousands_of_people: No
  vimeo_link: https://vimeo.com/115816106
  pennkey_username_1: 
  may_we_have_your_permission_to_feature_your_project_on_the_class_website: Yes
  team_member_1__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_2__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_3__can_we_list_your_name_listed_alongside_your_project: Yes
  team_member_4__can_we_list_your_name_listed_alongside_your_project: 
  who_are_the_members_of_your_crowd: Crowdflower workers
  how_many_unique_participants_did_you_have: 100
  for_your_final_project_did_you_simulate_the_crowd_or_run_a_real_experiment: Real crowd
  if_the_crowd_was_simulated_how_did_you_collect_this_set_of_data: 
  if_the_crowd_was_simulated_how_would_you_change_things_to_use_a_real_crowd: 
  how_do_you_ensure_the_quality_of_the_crowd_provides: 
  what_motivation_does_the_crowd_have_for_participating_in_your_project: Pay, Enjoyment
  did_you_perform_any_analysis_comparing_different_incentives_: No
  if_you_compared_different_incentives_what_analysis_did_you_perform_: 
  if_you_have_a_graph_analyzing_incentives_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_incentives_: 
  do_your_crowd_workers_need_specialized_skills: No
  what_sort_of_skills_do_they_need: They only need to be English speaking.
  if_skills_vary_widely_what_factors_cause_one_person_to_be_better_than_another_: 
  did_you_analyze_the_skills_of_the_crowd: No
  if_you_analyzed_skills_what_analysis_did_you_perform: 
  do_you_have_a_google_graph_analyzing_skills_: No
  if_you_have_a_graph_analyzing_skills_include_the_html_for_a_google_graph_here: 
  is_the_quality_of_what_the_crowd_gives_you_a_concern: Yes
  did_you_create_a_user_interface_for_the_crowd_workers: Yes
  if_yes_please_give_the_url_to_a_screenshot_of_the_crowdfacing_user_interface: https://github.com/rossmechanic/PictureThis/blob/master/Mockups/screenshots_from_first_HIT/Screen&#37;20Shot&#37;202014&#45;11&#45;13&#37;20at&#37;203.52.19&#37;20PM.png
  describe_your_crowdfacing_user_interface: Our crowd&#45;facing interface showed all of the pictures from a given pictures as well as an area under 3 of the pictures for the crowd workers to write captions. 
  did_you_analyze_the_aggregated_results_: No
  what_analysis_did_you_perform_on_the_aggregated_results: 
  did_you_analyze_the_quality_of_what_you_got_back: Yes
  what_analysis_did_you_perform_on_the_aggregated_results: We tested whether the crowd preferred the original books to the new crowdsourced versions. 
  what_analysis_did_you_perform_on_quality: Well we compared our final results to the original versions of the book, asking the crowd which was better, and the crowd thought the crowdsourced versions were better. 76.67&#37; of the time. 
  do_you_have_a_google_graph_analyzing_quality_: No
  if_you_have_a_graph_analyzing_quality_include_the_html_for_a_google_graph_here: 
  is_this_something_that_could_be_automated: No
  if_it_could_be_automated_say_how__if_it_is_difficult_or_impossible_to_automate_say_why: Can not be automated. Although the aggregation parts could be. 
  do_the_skills_of_individual_workers_vary_widely: No
  do_you_have_a_google_graph_analyzing_the_aggregated_results_: 
  if_you_have_a_graph_analyzing_the_aggregated_results_include_the_html_for_a_google_graph_here: 
  did_you_create_a_user_interface_for_the_end_users_to_see_the_aggregated_results: No
  if_yes_please_give_the_url_to_a_screenshot_of_the_user_interface_for_the_end_user: 
  describe_what_your_end_user_sees_in_this_interface: 
  what_is_the_scale_of_the_problem_that_you_are_trying_to_solve_: Well our project attempted to see if we could produce thousands of picture books for a relatively low cost. 
  if_it_would_benefit_from_a_huge_crowd_how_would_it_benefit_: I don't think the size of the crowd matters too much, anything in the hundreds would work. But the larger the crowd, the more creativity we would get. 
  what_challenges_would_scaling_to_a_large_crowd_introduce: I think with a larger crowd, we would want to create more versions of each story, because it would increase the probability that the final product is great. 
  did_you_perform_an_analysis_about_how_to_scale_up_your_project: No
  what_analysis_did_you_perform_on_the_scaling_up: 
  do_you_have_a_google_graph_analyzing_scaling_: 
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  url_to_the_flow_diagram_for_your_project: https://github.com/rossmechanic/PictureThis/blob/master/picturethisflowdiagram.jpg
  if_the_crowd_was_real_how_did_you_recruit_participants: We used Crowdflower workers, most of whom gave our HIT high ratings.
  did_your_project_work: Yes, we know that it worked because we created 18 new versions of picture books from the crowd, and the crowd preferred the crowdsourced versions of the book 76.67&#37; of the time, and thought they were equal 13.3&#37; of the time. 
  do_you_have_a_google_graph_analyzing_your_project_: 
  if_you_have_a_graph_analyzing_your_project_include_the_html_for_a_google_graph_here: 
  caption_for_your_google_graph_(project_analysis): 
  what_were_the_biggest_challenges_that_you_had_to_deal_with: Well we spent a lot of time trying to figure out how we could have the whole process automated using the the Python API, but we had difficulty figuring out how we would edit the CML files to cater to the different lengths of the books and also weren't entirely sure how we would transfer the important data from HIT to HIT. In the end, it made more sense to do the aggregation manually, but if we were to scale this project up, we would have to use the API. 
  where_there_major_changes_between_what_you_originally_proposed_and_your_final_product: Yes
  if_so_what_changed_between_your_original_plan_and_your_final_product: In our original plan we wanted to compare how closely related the text would be between the original versions and the crowdsourced versions of the picture books, but we ended up seeing little purpose in that. Instead we decided we wanted to see if the crowd could make picture books that were as good or even better than the original version, and it seems that the crowd did just that. 
  what_are_some_limitations_of_your_product: There is certainly the possibility that workers voted on their own material, skewing what may have been passed through to later rounds. Moreover, voters may have been voting between stories that were partially written by themselves and the original versions when they voted on which was better, which would skew results. 
  did_your_results_deviate_from_what_you_would_expect_from_previous_work_or_what_you_learned_in_the_class: Yes
  if_your_results_deviated_why_might_that_be: I expected very few of the crowdsourced stories to be voted better than the originals, with only small portion equal, but the results turned out to be far more promising than that. The main thing that I worried about going in, was that the style would change between every three pictures, but since workers could see what was written before them, they tended to adapt the style to fit what was already written. 
  caption_for_your_graph_(scaling_up): 
  caption_for_your_graph_(aggregation): 
  caption_for_your_graph_(quality): 
  caption_for_your_graph_(skills): 
  caption_for_your_graph_(incentives): 
  is_there_anything_else_youd_like_to_say_about_your_project: 
  did_you_train_a_machine_learning_component_from_what_the_crowd_gave_you: No
  if_you_trained_a_machine_learning_component_describe_what_you_did: 
  did_you_analyze_the_quality_of_the_machine_learning_component: 
  if_you_have_a_graph_analyzing_a_machine_learning_component_include_the_html_for_a_google_graph_here: 
  caption_for_your_graph_(incentives): 
  caption_for_your_graph_(machine_learning_component): 
  what_was_the_main_focus_of_your_teams_effort: Something in between
  provide_a_link_to_your_final_presentation_video: 
  if_you_have_a_graph_analyzing_scaling_include_the_html_for_a_google_graph_here: 
  do_you_have_a_google_graph_analyzing_skills_: 
  what_incentivi: 
